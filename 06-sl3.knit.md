---
output:
  pdf_document: default
  html_document: default
---
# Super (Machine) Learning {#sl3}

_Rachael Phillips_

Based on the [`sl3` `R` package](https://github.com/tlverse/sl3) by _Jeremy
Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, and Oleg Sofrygin_.

Updated: 2021-03-07

## Learning Objectives {-}

By the end of this chapter you will be able to:

1. Select a loss function that is appropriate for the functional parameter to be
   estimated.
2. Assemble an ensemble of learners based on the properties that identify what
   features they support.
3. Customize learner hyperparameters to incorporate a diversity of different
   settings.
4. Select a subset of available covariates and pass only those variables to the
   modeling algorithm.
5. Fit an ensemble with nested cross-validation to obtain an estimate of the
   performance of the ensemble itself.
6. Obtain `sl3` variable importance metrics.
7. Interpret the discrete and continuous Super Learner fits.
8. Rationalize the need to remove bias from the Super Learner to make an optimal
   bias–variance tradeoff for the parameter of interest.

## Motivation {-}

- A common task in data analysis is prediction --- using the observed data to 
  learn a function, which can be used to map new input variables into a 
  predicted outcome. <!--  Oftentimes, the scientific question of interest translates 
  to a statistical question that requires (causal) effect estimation. Even in 
  these scenarios, where prediction is not in the forefront, there are Often
  prediction steps embedded in the procedure. --->
- For some data, algorithms that can model a complex function are necessary to  
  adequately model the data. For other data, a main terms regression model might 
  fit the data quite well.  
- The Super Learner, an ensemble learner, solves this issue, by allowing a
  combination of algorithms from the simplest (intercept-only) to most complex
  (neural nets, random forests, SVM, etc).
- It works by using cross-validation in a manner which guarantees that the
  resulting fit will be as good as possible, given the learners provided.

## Introduction {-}

In [Chapter 1](#intro), we introduced the [_Roadmap for Targeted
Learning_](#roadmap) as a general template to translate real-world data
applications into formal statistical estimation problems. The first steps of
this roadmap define the *statistical estimation problem*, which establish

1. Data as a random variable, or equivalently, a realization of a
   particular experiment/study. We assume the observations in the data are 
   independent and identically distributed.
2. A statistical model as the set of possible probability distributions that 
   could have given rise to the observed data. 
3. A translation of the scientific question, which is often causal, into a
   target estimand.

Note that if the estimand is causal, step 3 also requires establishing 
identifiability of the estimand from the observed data, under possible 
non-testable assumptions that may not necessarily be reasonable. Still, the 
target quantity does have a valid statistical interpretation. See [causal target 
parameters](#causal) for more detail on causal models and identifiability.

Now that we have defined the statistical estimation problem, we are ready to
construct the TMLE; an asymptotically linear and efficient substitution
estimator of this estimand. The first step in this estimation procedure
is an initial estimate of the data-generating distribution, or the relevant part
of this distribution that is needed to evaluate the target parameter. For this
initial estimation, we use the Super Learner [@vdl2007super].

The Super Learner provides an important step in creating a robust estimator. It
is a loss-function-based tool that uses cross-validation to obtain the best
prediction of our target parameter, based on a weighted average of a library of
machine learning algorithms. The library of machine learning algorithms 
consists of functions ("learners" in the `sl3` nomenclature) that we think might
be consistent with the true data-generating distribution. By "consistent with 
the true data-generating distribution", we mean that the algorithms selected 
should not violate subject-matter knowledge about the experiment that 
generated the data. Also, the library should contain a diversity of algorithms 
that range from parametric regression models to multi-step algorithms involving 
screening covariates, penalizations, optimizing tuning parameters, etc.

The ensembling of the collection of algorithms with weights ("metalearning" in
the `sl3` nomenclature) has been shown to be adaptive and robust, even in small
samples [@polley2010super]. The Super Learner is proven to be asymptotically as
accurate as the best possible prediction algorithm in the library
[@vdl2003unified; @vaart2006oracle].

### Background {-}

We use a *loss function* $L$ to assign a measure of performance to each 
candidate learner $\psi$ when applied to the data $O$, and subsequently compare 
performance across the learners. More generally, $L$ maps every 
$\psi \in \mathbb{R}$ to $L(\psi) : (O) \mapsto L(\psi)(O)$. It is important 
to recall that $\psi$ is an estimator of $\psi_0$, the unknown and true 
parameter value under $P_0$. A valid loss function will have expectation (risk) 
that is minimized at the true value of the parameter $\psi_0$. Minimizing the 
expected loss will bring the chosen candidate closer to the true $\psi_0$. For 
example, say we observe a learning data set $O_i=(Y_i,X_i)$, of $i=1, ..., n$
independent and identically distributed observations, where $Y_i$ is a 
continuous outcome of interest, $X_i$ is a set of covariates, and our objective 
is to estimate the function $\psi_0: X \mapsto \psi_0(X) = E_0(Y|X)$. This 
function can be expressed as the minimizer of the expected squared error loss: 
$\psi_0 = \text{argmin}_{\psi} E[L(O,\psi(X))]$, where 
$L(O,\psi(X)) = (Y − \psi(X))^2$. We can estimate the loss by substituting the 
empirical distribution of the data $P_n$ for the true and unknown distribution 
of the observed data $P_0$. Also, we can use the cross-validated risk to 
empirically determine the relative performance of the candidate learners. Once 
we have tested different algorithms on actual data and looked at the performance 
(e.g., MSE of predictions across all learners), we can see which algorithm has 
the lowest risk, and thus is closest to the true $\psi_0$. 

The *cross-validated empirical risk* of an algorithm is defined as the empirical
mean over a validation sample of the loss of the algorithm fitted on the
training sample, averaged across the splits of the data.

The *discrete Super Learner*, or *cross-validation selector*, is the algorithm
in the library that minimizes the cross-validated empirical risk.

The *continuous/ensemble Super Learner*, often referred to as *Super Learner*
is a weighted average of the library of algorithms, where the weights are chosen
to minimize the cross-validated empirical risk of the library. Restricting the
weights to be positive and sum to one (i.e., a convex combination) has been
shown to improve upon the discrete Super Learner [@polley2010super;
@vdl2007super]. This notion of weighted combinations was introduced in
@wolpert1992stacked for neural networks and adapted for regressions in
@breiman1996stacked.

Cross-validation is proven to be optimal for selection among estimators. This
result was established through the oracle inequality for the cross-validation
selector among a collection of candidate estimators [@vdl2003unified;
@vaart2006oracle]. The only condition is that loss function is uniformly
bounded, which is guaranteed in `sl3`.

<!--
The *oracle results* prove that, if the number of algorithms in the library are
polynomial in sample size, then the cross-validation selector (i.e., discrete
Super Learner) (1) is equivalent with the oracle selector asymptotically (based
on sample of size of training samples), or (2) achieves the parametric rate
(log $n/n$) for convergence with respect to the loss-based dissimilarity (risk)
between a candidate estimate $\psi$ and the true parameter value $\psi_0$.


### Super Learner for Prediction {-}

Below, we show the results of
such a study, comparing the fits of several different learners, including the SL
algorithms.

r cv_fig3, results="asis", echo = FALSE
knitr::include_graphics("img/misc/ericSL.pdf")


For more detail on Super Learner we refer the reader to @vdl2007super and
@polley2010super. The optimality results for the cross-validation selector
among a family of algorithms were established in @vdl2003unified and extended
in @vaart2006oracle.
-->
## `sl3` "Microwave Dinner" Implementation {-}

We begin by illustrating the core functionality of the super learner algorithm
as implemented in `sl3`. 

The `sl3` implementation consists of the following steps:

0. Load the necessary libraries and data
1. Define the machine learning task
2. Make a super learner by creating library of base learners and a metalearner
3. Train the super learner on the machine learning task
4. Obtain predicted values

### WASH Benefits Study Example {-}

Using the WASH Benefits Bangladesh data, we are interested in predicting 
weight-for-height z-score `whz` using the available covariate data. More 
information on this dataset, and all other data that we will work with in this 
handbook, is contained in [Chapter 3]{#data}. Let's begin!

### 0. Load the necessary libraries and data {-}

First, we will load the relevant `R` packages, set a seed, and load the data.

<!--
If you would like to use newer `sl3` functionality that is available in the
devel branch of the `sl3` GitHub repository, you need to install that version
of the package (i.e., `usethis::install_github(tlverse/sl3@devel)`), re-start
your `R` session, and then re-load the `sl3` package.
-->


```r
library(data.table)
library(dplyr)
library(readr)
library(ggplot2)
library(SuperLearner)
library(origami)
library(sl3)
library(knitr)
library(kableExtra)

# load data set and take a peek
washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)
head(washb_data) %>%
  kable() %>%
  kableExtra:::kable_styling(fixed_thead = T) %>%
  scroll_box(width = "100%", height = "300px")
```

<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; "><table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> whz </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> tr </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> fracode </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> month </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> aged </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> sex </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> momage </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> momedu </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> momheight </th>
   <th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> hfiacat </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> Nlt18 </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> Ncomp </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> watmin </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> elec </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> floor </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> walls </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> roof </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_wardrobe </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_table </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_chair </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_khat </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_chouki </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_tv </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_refrig </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_bike </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_moto </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_sewmach </th>
   <th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;"> asset_mobile </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 0.00 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N05265 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 268 </td>
   <td style="text-align:left;"> male </td>
   <td style="text-align:right;"> 30 </td>
   <td style="text-align:left;"> Primary (1-5y) </td>
   <td style="text-align:right;"> 146.40 </td>
   <td style="text-align:left;"> Food Secure </td>
   <td style="text-align:right;"> 3 </td>
   <td style="text-align:right;"> 11 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> -1.16 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N05265 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 286 </td>
   <td style="text-align:left;"> male </td>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:left;"> Primary (1-5y) </td>
   <td style="text-align:right;"> 148.75 </td>
   <td style="text-align:left;"> Moderately Food Insecure </td>
   <td style="text-align:right;"> 2 </td>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> -1.05 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N08002 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 264 </td>
   <td style="text-align:left;"> male </td>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:left;"> Primary (1-5y) </td>
   <td style="text-align:right;"> 152.15 </td>
   <td style="text-align:left;"> Food Secure </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> -1.26 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N08002 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 252 </td>
   <td style="text-align:left;"> female </td>
   <td style="text-align:right;"> 28 </td>
   <td style="text-align:left;"> Primary (1-5y) </td>
   <td style="text-align:right;"> 140.25 </td>
   <td style="text-align:left;"> Food Secure </td>
   <td style="text-align:right;"> 3 </td>
   <td style="text-align:right;"> 5 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> -0.59 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N06531 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 336 </td>
   <td style="text-align:left;"> female </td>
   <td style="text-align:right;"> 19 </td>
   <td style="text-align:left;"> Secondary (&gt;5y) </td>
   <td style="text-align:right;"> 150.95 </td>
   <td style="text-align:left;"> Food Secure </td>
   <td style="text-align:right;"> 2 </td>
   <td style="text-align:right;"> 7 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> -0.51 </td>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> N06531 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 304 </td>
   <td style="text-align:left;"> male </td>
   <td style="text-align:right;"> 20 </td>
   <td style="text-align:left;"> Secondary (&gt;5y) </td>
   <td style="text-align:right;"> 154.20 </td>
   <td style="text-align:left;"> Severely Food Insecure </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 3 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 1 </td>
  </tr>
</tbody>
</table></div>

### 1. Define the machine learning task {-}

To define the machine learning **"task"** (predict weight-for-height Z-score
`whz` using the available covariate data), we need to create an `sl3_Task`
object.

The `sl3_Task` keeps track of the roles the variables play in the machine
learning problem, the data, and any metadata (e.g., observational-level weights,
IDs, offset).

Also, if we had missing outcomes, we would need to set `drop_missing_outcome =
TRUE` when we create the task. In the next analysis, with the [IST stroke trial
data](#ist), we do have a missing outcome. In the following chapter, we need to
estimate this "missingness mechanism"; which is the conditional probably that 
the outcome is observed, given the history (i.e., variables that were measured 
before the missingness). Estimating the missingness mechanism requires learning 
a prediction function that outputs the predicted probability that a unit 
is missing, given their history. 


```r
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]

# create the sl3 task
washb_task <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome
)
#> Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
#> Missing covariate data detected: imputing covariates.
```
*This warning is important.* The task just imputed missing covariates for us.
Specifically, for each covariate column with missing values, `sl3` uses the
median to impute missing continuous covariates, and the mode to impute binary
and categorical covariates.

Also, for each covariate column with missing values, `sl3` adds an additional
column indicating whether or not the value was imputed, which is particularly
handy when the missingness in the data might be informative.

Also, notice that we did not specify the number of folds, or the loss function
in the task. The default cross-validation scheme is V-fold, with $V=10$ number 
of folds.

Let's visualize our `washb_task`:


```r
washb_task
#> A sl3 Task with 4695 obs and these nodes:
#> $covariates
#>  [1] "tr"              "fracode"         "month"           "aged"           
#>  [5] "sex"             "momage"          "momedu"          "momheight"      
#>  [9] "hfiacat"         "Nlt18"           "Ncomp"           "watmin"         
#> [13] "elec"            "floor"           "walls"           "roof"           
#> [17] "asset_wardrobe"  "asset_table"     "asset_chair"     "asset_khat"     
#> [21] "asset_chouki"    "asset_tv"        "asset_refrig"    "asset_bike"     
#> [25] "asset_moto"      "asset_sewmach"   "asset_mobile"    "delta_momage"   
#> [29] "delta_momheight"
#> 
#> $outcome
#> [1] "whz"
#> 
#> $id
#> NULL
#> 
#> $weights
#> NULL
#> 
#> $offset
#> NULL
#> 
#> $time
#> NULL
```

We can't see when we print the task, but the default cross-validation fold 
structure ($V$-fold cross-validation with $V$=10 folds) was created when we 
defined the task.


```r
length(washb_task$folds) # how many folds?
#> [1] 10

head(washb_task$folds[[1]]$training_set) # row indexes for fold 1 training
#> [1] 1 2 3 4 5 6
head(washb_task$folds[[1]]$validation_set) # row indexes for fold 1 validation
#> [1] 12 21 29 41 43 53

any(washb_task$folds[[1]]$training_set %in% washb_task$folds[[1]]$validation_set)
#> [1] FALSE
```

`R6` Tip: If you type `washb_task$` and then press the "tab"  button (you will 
need to press "tab" twice if you're not in RStudio), you can view all of the 
active and public fields and methods that can be accessed from the `washb_task` 
object. 

### 2. Make a Super Learner {-}

Now that we have defined our machine learning problem with the `sl3_Task`, we 
are ready to **"make"** the Super Learner. This requires specification of

* A set of candidate machine learning algorithms, also commonly referred to as 
  a "library" of "learners". The set should include a diversity of algorithms 
  that are believed to be consistent with the true data-generating distribution.
* A metalearner, to ensemble the base learners.

We might also incorporate

* Feature selection, to pass only a subset of the predictors to the algorithm.
* Hyperparameter specification, to tune base learners.

Learners have properties that indicate what features they support. We may use
`sl3_list_properties()` to get a list of all properties supported by at least
one learner.


```r
sl3_list_properties()
#>  [1] "binary"        "binomial"      "categorical"   "continuous"   
#>  [5] "cv"            "density"       "ids"           "importance"   
#>  [9] "offset"        "preprocessing" "sampling"      "screener"     
#> [13] "timeseries"    "weights"       "wrapper"
```
Since we have a continuous outcome, we may identify the learners that support
this outcome type with `sl3_list_learners()`.


```r
sl3_list_learners("continuous")
#>  [1] "Lrnr_arima"                     "Lrnr_bartMachine"              
#>  [3] "Lrnr_bilstm"                    "Lrnr_bound"                    
#>  [5] "Lrnr_caret"                     "Lrnr_cv_selector"              
#>  [7] "Lrnr_dbarts"                    "Lrnr_earth"                    
#>  [9] "Lrnr_expSmooth"                 "Lrnr_gam"                      
#> [11] "Lrnr_gbm"                       "Lrnr_glm"                      
#> [13] "Lrnr_glm_fast"                  "Lrnr_glmnet"                   
#> [15] "Lrnr_grf"                       "Lrnr_gru_keras"                
#> [17] "Lrnr_gts"                       "Lrnr_h2o_glm"                  
#> [19] "Lrnr_h2o_grid"                  "Lrnr_hal9001"                  
#> [21] "Lrnr_HarmonicReg"               "Lrnr_hts"                      
#> [23] "Lrnr_lstm"                      "Lrnr_lstm_keras"               
#> [25] "Lrnr_mean"                      "Lrnr_multiple_ts"              
#> [27] "Lrnr_nnet"                      "Lrnr_nnls"                     
#> [29] "Lrnr_optim"                     "Lrnr_pkg_SuperLearner"         
#> [31] "Lrnr_pkg_SuperLearner_method"   "Lrnr_pkg_SuperLearner_screener"
#> [33] "Lrnr_polspline"                 "Lrnr_randomForest"             
#> [35] "Lrnr_ranger"                    "Lrnr_rpart"                    
#> [37] "Lrnr_rugarch"                   "Lrnr_screener_correlation"     
#> [39] "Lrnr_solnp"                     "Lrnr_stratified"               
#> [41] "Lrnr_svm"                       "Lrnr_tsDyn"                    
#> [43] "Lrnr_xgboost"
```

Now that we have an idea of some learners, we can construct them using the
`make_learner` function or the `new` method.


```r
# choose base learners
lrn_glm <- make_learner(Lrnr_glm)
lrn_mean <- Lrnr_mean$new()
```
We can customize learner hyperparameters to incorporate a diversity of different
settings. Documentation for the learners and their hyperparameters can be found
in the [`sl3` Learners
Reference](https://tlverse.org/sl3/reference/index.html#section-sl-learners).


```r
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
lrn_ridge <- Lrnr_glmnet$new(alpha = 0)
lrn_enet.5 <- make_learner(Lrnr_glmnet, alpha = 0.5)

lrn_polspline <- Lrnr_polspline$new()

lrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)

lrn_hal_faster <- Lrnr_hal9001$new(max_degree = 2, reduce_basis = 0.05)

xgb_fast <- Lrnr_xgboost$new() # default with nrounds = 20 is pretty fast
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)
```
We can use `Lrnr_define_interactions` to define interaction terms among 
covariates. The interactions should be supplied as list of character vectors, 
where each vector specifies an interaction. For example, we specify interactions 
below between (1) `tr` (whether or not the subject received the WASH 
intervention) and `elec` (whether or not the subject had electricity); and 
between (2) `tr` and `hfiacat` (the subject's level of food security). 


```r
interactions <- list(c("elec", "tr"), c("tr", "hfiacat"))
# main terms as well as the interactions above will be included
lrn_interaction <- make_learner(Lrnr_define_interactions, interactions)
```
What we just defined above is incomplete. In order to fit learners with these 
interactions, we need to create a `Pipeline`. A `Pipeline` is a set of learners 
to be fit sequentially, where the fit from one learner is used to define the 
task for the next learner. We need to create a `Pipeline` with the interaction 
defining learner and another learner that incorporate these terms when fitting
a model. Let's create a learner pipeline that will fit a linear model with the 
combination of main terms and interactions terms, as specified in 
`lrn_interaction_main`. 


```r
# we already instantiated a linear model learner above, no need to do that again
lrn_glm_interaction <- make_learner(Pipeline, lrn_interaction, lrn_glm)
lrn_glm_interaction
#> [1] "Lrnr_define_interactions_TRUE"
#> [1] "Lrnr_glm_TRUE"
```

We can also include learners from the `SuperLearner` `R` package.


```r
lrn_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
```

Here is a fun trick to create customized learners over a grid of parameters.


```r
# I like to crock pot my super learners
grid_params <- list(
  cost = c(0.01, 0.1, 1, 10, 100, 1000),
  gamma = c(0.001, 0.01, 0.1, 1),
  kernel = c("polynomial", "radial", "sigmoid"),
  degree = c(1, 2, 3)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
svm_learners <- apply(grid, MARGIN = 1, function(tuning_params) {
  do.call(Lrnr_svm$new, as.list(tuning_params))
})
```

```r
grid_params <- list(
  max_depth = c(2, 4, 6),
  eta = c(0.001, 0.1, 0.3),
  nrounds = 100
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
grid
#>   max_depth   eta nrounds
#> 1         2 0.001     100
#> 2         4 0.001     100
#> 3         6 0.001     100
#> 4         2 0.100     100
#> 5         4 0.100     100
#> 6         6 0.100     100
#> 7         2 0.300     100
#> 8         4 0.300     100
#> 9         6 0.300     100

xgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {
  do.call(Lrnr_xgboost$new, as.list(tuning_params))
})
xgb_learners
#> [[1]]
#> [1] "Lrnr_xgboost_100_1_2_0.001"
#> 
#> [[2]]
#> [1] "Lrnr_xgboost_100_1_4_0.001"
#> 
#> [[3]]
#> [1] "Lrnr_xgboost_100_1_6_0.001"
#> 
#> [[4]]
#> [1] "Lrnr_xgboost_100_1_2_0.1"
#> 
#> [[5]]
#> [1] "Lrnr_xgboost_100_1_4_0.1"
#> 
#> [[6]]
#> [1] "Lrnr_xgboost_100_1_6_0.1"
#> 
#> [[7]]
#> [1] "Lrnr_xgboost_100_1_2_0.3"
#> 
#> [[8]]
#> [1] "Lrnr_xgboost_100_1_4_0.3"
#> 
#> [[9]]
#> [1] "Lrnr_xgboost_100_1_6_0.3"
```

Did you see `Lrnr_caret` when we called `sl3_list_learners(c("binomial"))`?
All we need to specify to use this popular algorithm as a candidate in our 
Super Learner is the `algorithm` we want to tune, which is passed as 
`method` to `caret::train()`. The default method for parameter selection 
criterion with is set to "CV" instead of the `caret::train()` default `boot`. 
The summary metric used to select the optimal model is `RMSE` for continuous 
outcomes and `Accuracy` for categorical and binomial outcomes.


```r
# Unlike xgboost, I have no idea how to tune a neural net or BART machine, so
# I let caret take the reins
lrnr_caret_nnet <- make_learner(Lrnr_caret, algorithm = "nnet")
lrnr_caret_bartMachine <- make_learner(Lrnr_caret,
  algorithm = "bartMachine",
  method = "boot", metric = "Accuracy",
  tuneLength = 10
)
```

In order to assemble the library of learners, we need to **"stack"** them
together.

A `Stack` is a special learner and it has the same interface as all other
learners. What makes a stack special is that it combines multiple learners by
training them simultaneously, so that their predictions can be either combined
or compared.


```r
stack <- make_learner(
  Stack, lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50
)
stack
#> [1] "Lrnr_glm_TRUE"                            
#> [2] "Lrnr_polspline_5"                         
#> [3] "Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE"
#> [4] "Lrnr_glmnet_NULL_deviance_10_0_100_TRUE"  
#> [5] "Lrnr_glmnet_NULL_deviance_10_1_100_TRUE"  
#> [6] "Lrnr_xgboost_50_1"
```

We can also stack the learners by first creating a vector, and then 
instantiating the stack. I prefer this method, since it easily allows us to 
modify the names of the learners.


```r
# named vector of learners first
learners <- c(lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50)
names(learners) <- c("glm", "polspline", "enet.5", "ridge", "lasso", "xgboost50")
# next make the stack
stack <- make_learner(Stack, learners)
# now the names are pretty
stack
#> [1] "glm"       "polspline" "enet.5"    "ridge"     "lasso"     "xgboost50"
```

We're jumping ahead a bit, but let's check something out quickly. It's 
straightforward, and just one more step, to set up this stack such that 
all of the learners will train in a cross-validated manner.


```r
cv_stack <- Lrnr_cv$new(stack)
cv_stack
#> [1] "Lrnr_cv"
#> [1] "glm"       "polspline" "enet.5"    "ridge"     "lasso"     "xgboost50"
```

#### Screening Algorithms for Feature Selection {-}

We can optionally select a subset of available covariates and pass only
those variables to the modeling algorithm. The current set of learners that 
can be used for prescreening covariates is included below. 

- `Lrnr_screener_importance` selects `num_screen` (default = 5) covariates 
  based on the variable importance ranking provided by the `learner`. Any 
  learner with an "importance" method can be used in `Lrnr_screener_importance`; 
  and this currently includes `Lrnr_ranger`, `Lrnr_randomForest`, and 
  `Lrnr_xgboost`. 
- `Lrnr_screener_coefs`, which provides screening of covariates based on the
  magnitude of their estimated coefficients in a (possibly regularized) GLM.
  The `threshold` (default = 1e-3) defines the minimum absolute size of the 
  coefficients, and thus covariates, to be kept. Also, a `max_retain` argument 
  can be optionally provided to restrict the number of selected covariates to be 
  no more than `max_retain`.
- `Lrnr_screener_correlation` provides covariate screening procedures by 
  running a test of correlation (Pearson default), and then selecting the (1) 
  top ranked variables (default), or (2) the variables with a pvalue lower than 
  some pre-specified threshold.
- `Lrnr_screener_augment` augments a set of screened covariates with additional    
  covariates that should be included by default, even if the screener did not 
  select them. An example of how to use this screener is included below. 
   
Let's consider screening covariates based on their `randomForest` variable
importance ranking (ordered by mean decrease in accuracy). To select the top
5 most important covariates according to this ranking, we can combine
`Lrnr_screener_importance` with `Lrnr_ranger` (limiting the number of trees by 
setting `ntree = 20`). 

Hang on! Before you think it -- I will confess: Bob Ross and I both know that 20
trees makes for a lonely forest, and I shouldn't consider it, but these are the 
sacrifices I have to make for this chapter to build in time! 


```r
miniforest <- Lrnr_ranger$new(
  num.trees = 20, write.forest = FALSE,
  importance = "impurity_corrected"
)

# learner must already be instantiated, we did this when we created miniforest
screen_rf <- Lrnr_screener_importance$new(learner = miniforest, num_screen = 5)
screen_rf
#> [1] "Lrnr_screener_importance_5"

# which covariates are selected on the full data?
screen_rf$train(washb_task)
#> [1] "Lrnr_screener_importance_5"
#> $selected
#> [1] "aged"      "month"     "momheight" "Ncomp"     "tr"
```

An example of how to format `Lrnr_screener_augment` is included below for 
clarity.

```r
keepme <- c("aged", "momage")
# screener must already be instantiated, we did this when we created screen_rf
screen_augment_rf <- Lrnr_screener_augment$new(
  screener = screen_rf, default_covariates = keepme
)
screen_augment_rf
#> [1] "Lrnr_screener_augment_c(\"aged\", \"momage\")"
```

Selecting covariates with non-zero lasso coefficients is quite common. Let's 
construct `Lrnr_screener_coefs` screener that does just that, and test it 
out. 


```r
# we already instantiated a lasso learner above, no need to do it again
screen_lasso <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)
screen_lasso
#> [1] "Lrnr_screener_coefs_0_NULL"
```

To **"pipe"** only the selected covariates to the modeling algorithm, we need to
make a `Pipeline`, similar to the one we built for the regression model with 
interaction terms. 


```r
screen_rf_pipe <- make_learner(Pipeline, screen_rf, stack)
screen_lasso_pipe <- make_learner(Pipeline, screen_lasso, stack)
```
Now, these learners with no internal screening will be preceded by a screening 
step.

We also consider the original `stack`, to compare how the feature selection
methods perform in comparison to the methods without feature selection.

Analogous to what we have seen before, we have to stack the pipeline and
original `stack` together, so we may use them as base learners in our super
learner.


```r
# pretty names again
learners2 <- c(learners, screen_rf_pipe, screen_lasso_pipe)
names(learners2) <- c(names(learners), "randomforest_screen", "lasso_screen")

fancy_stack <- make_learner(Stack, learners2)
fancy_stack
#> [1] "glm"                 "polspline"           "enet.5"             
#> [4] "ridge"               "lasso"               "xgboost50"          
#> [7] "randomforest_screen" "lasso_screen"
```

We will use the [default
metalearner](https://github.com/tlverse/sl3/blob/master/R/default_metalearner.R),
which uses
[`Lrnr_solnp()`](https://github.com/tlverse/sl3/blob/master/R/Lrnr_solnp.R) to
provide fitting procedures for a pairing of [loss
function](https://github.com/tlverse/sl3/blob/master/R/loss_functions.R) and
[metalearner
function](https://github.com/tlverse/sl3/blob/master/R/metalearners.R). This
default metalearner selects a loss and metalearner pairing based on the outcome
type. Note that any learner can be used as a metalearner.

Now that we have made a diverse stack of base learners, we are ready to make the 
super learner. The super learner algorithm fits a metalearner on the 
validation-set predictions.


```r
sl <- make_learner(Lrnr_sl, learners = fancy_stack)
```
We can also use `Lrnr_cv` to build a super learner, cross-validate a stack of
learners to compare performance of the learners in the stack, or cross-validate
any single learner (see "Cross-validation" section of this [`sl3`
introductory tutorial](https://tlverse.org/sl3/articles/intro_sl3.html)).

Furthermore, we can [Define New `sl3`
Learners](https://tlverse.org/sl3/articles/custom_lrnrs.html) which can be used
in all the places you could otherwise use any other `sl3` learners, including
`Pipelines`, `Stacks`, and the Super Learner.

Recall that the discrete Super Learner, or cross-validated selector, is a 
metalearner that assigns a weight of 1 to the learner with the lowest 
cross-validated empirical risk, and weight of 0 to all other learners. This 
metalearner specification can be invoked with `Lrnr_cv_selector`. 


```r
discrete_sl_metalrn <- Lrnr_cv_selector$new()
discrete_sl <- Lrnr_sl$new(
  learners = fancy_stack,
  metalearner = discrete_sl_metalrn
)
```
<!--
In the plot below, we visualize the steps for executing the Super Learner in the
`tlverse/delayed` framework. For those like myself who are not particularly
keen on understanding the intricacies of `delayed`, let's focus on the main
point of this figure: we can see there are 10 realizations of the stack which
represent the 10 cross-validation folds and there is a separate hold-out
(top branch of the figure) that will not be used to fit the Super Learner.


```r
dt_sl <- delayed_learner_train(sl, washb_task)
plot(dt_sl, color = FALSE, height = "400px", width = "90%")
```

<!--html_preserve--><div id="htmlwidget-e31bdf0964cb79602267" style="width:90%;height:400px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-e31bdf0964cb79602267">{"x":{"nodes":{"id":["bbab7dae-7faa-11eb-86c1-acde48001122","bbab7052-7faa-11eb-86c1-acde48001122","bbab3a42-7faa-11eb-86c1-acde48001122","bbab2d2c-7faa-11eb-86c1-acde48001122","bb7517fa-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb7272a2-7faa-11eb-86c1-acde48001122","bb728288-7faa-11eb-86c1-acde48001122","bb729192-7faa-11eb-86c1-acde48001122","bb72a056-7faa-11eb-86c1-acde48001122","bb72af10-7faa-11eb-86c1-acde48001122","bb72bdf2-7faa-11eb-86c1-acde48001122","bb73d76e-7faa-11eb-86c1-acde48001122","bb73ca3a-7faa-11eb-86c1-acde48001122","bb72ced2-7faa-11eb-86c1-acde48001122","bb73b130-7faa-11eb-86c1-acde48001122","bb72dda0-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb730618-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb73215c-7faa-11eb-86c1-acde48001122","bb733d54-7faa-11eb-86c1-acde48001122","bb73588e-7faa-11eb-86c1-acde48001122","bb7373aa-7faa-11eb-86c1-acde48001122","bb738ebc-7faa-11eb-86c1-acde48001122","bb74f7de-7faa-11eb-86c1-acde48001122","bb74ea3c-7faa-11eb-86c1-acde48001122","bb73eb1e-7faa-11eb-86c1-acde48001122","bb74cfb6-7faa-11eb-86c1-acde48001122","bb73f6f4-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb741fbc-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb743b28-7faa-11eb-86c1-acde48001122","bb745676-7faa-11eb-86c1-acde48001122","bb7475a2-7faa-11eb-86c1-acde48001122","bb749370-7faa-11eb-86c1-acde48001122","bb74afd6-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb7d7530-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7a17e6-7faa-11eb-86c1-acde48001122","bb7a2830-7faa-11eb-86c1-acde48001122","bb7a3762-7faa-11eb-86c1-acde48001122","bb7a46d0-7faa-11eb-86c1-acde48001122","bb7a5648-7faa-11eb-86c1-acde48001122","bb7a65de-7faa-11eb-86c1-acde48001122","bb7c4372-7faa-11eb-86c1-acde48001122","bb7c362a-7faa-11eb-86c1-acde48001122","bb7b427e-7faa-11eb-86c1-acde48001122","bb7c1c8a-7faa-11eb-86c1-acde48001122","bb7b4f62-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7b78a2-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7b940e-7faa-11eb-86c1-acde48001122","bb7baeda-7faa-11eb-86c1-acde48001122","bb7bc9ec-7faa-11eb-86c1-acde48001122","bb7be512-7faa-11eb-86c1-acde48001122","bb7c0056-7faa-11eb-86c1-acde48001122","bb7d5690-7faa-11eb-86c1-acde48001122","bb7d49a2-7faa-11eb-86c1-acde48001122","bb7c57cc-7faa-11eb-86c1-acde48001122","bb7d30fc-7faa-11eb-86c1-acde48001122","bb7c637a-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7c8bc0-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7ca740-7faa-11eb-86c1-acde48001122","bb7cc284-7faa-11eb-86c1-acde48001122","bb7cde18-7faa-11eb-86c1-acde48001122","bb7cfa24-7faa-11eb-86c1-acde48001122","bb7d1568-7faa-11eb-86c1-acde48001122","bb8edbf4-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8c43b2-7faa-11eb-86c1-acde48001122","bb8c541a-7faa-11eb-86c1-acde48001122","bb8c6310-7faa-11eb-86c1-acde48001122","bb8c71fc-7faa-11eb-86c1-acde48001122","bb8c817e-7faa-11eb-86c1-acde48001122","bb8c9074-7faa-11eb-86c1-acde48001122","bb8d9e9c-7faa-11eb-86c1-acde48001122","bb8d919a-7faa-11eb-86c1-acde48001122","bb8ca140-7faa-11eb-86c1-acde48001122","bb8d78c2-7faa-11eb-86c1-acde48001122","bb8cad52-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8cd5e8-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8cf104-7faa-11eb-86c1-acde48001122","bb8d0c3e-7faa-11eb-86c1-acde48001122","bb8d2778-7faa-11eb-86c1-acde48001122","bb8d428a-7faa-11eb-86c1-acde48001122","bb8d5d7e-7faa-11eb-86c1-acde48001122","bb8eba16-7faa-11eb-86c1-acde48001122","bb8eaa1c-7faa-11eb-86c1-acde48001122","bb8db18e-7faa-11eb-86c1-acde48001122","bb8e8bd6-7faa-11eb-86c1-acde48001122","bb8dbd32-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8de546-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e0062-7faa-11eb-86c1-acde48001122","bb8e1c0a-7faa-11eb-86c1-acde48001122","bb8e3780-7faa-11eb-86c1-acde48001122","bb8e52b0-7faa-11eb-86c1-acde48001122","bb8e6ef8-7faa-11eb-86c1-acde48001122","bb93a030-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb8f88ba-7faa-11eb-86c1-acde48001122","bb8f9864-7faa-11eb-86c1-acde48001122","bb8fa796-7faa-11eb-86c1-acde48001122","bb8fb7f4-7faa-11eb-86c1-acde48001122","bb90b672-7faa-11eb-86c1-acde48001122","bb90d332-7faa-11eb-86c1-acde48001122","bb923b64-7faa-11eb-86c1-acde48001122","bb922bf6-7faa-11eb-86c1-acde48001122","bb90f060-7faa-11eb-86c1-acde48001122","bb920e50-7faa-11eb-86c1-acde48001122","bb91023a-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb913f2a-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb916306-7faa-11eb-86c1-acde48001122","bb9188a4-7faa-11eb-86c1-acde48001122","bb91aa96-7faa-11eb-86c1-acde48001122","bb91ccce-7faa-11eb-86c1-acde48001122","bb91edb2-7faa-11eb-86c1-acde48001122","bb937e5c-7faa-11eb-86c1-acde48001122","bb936f34-7faa-11eb-86c1-acde48001122","bb9252fc-7faa-11eb-86c1-acde48001122","bb9353b4-7faa-11eb-86c1-acde48001122","bb9260d0-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb9291fe-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb92b22e-7faa-11eb-86c1-acde48001122","bb92d5c4-7faa-11eb-86c1-acde48001122","bb92f540-7faa-11eb-86c1-acde48001122","bb9314c6-7faa-11eb-86c1-acde48001122","bb93344c-7faa-11eb-86c1-acde48001122","bb96e8bc-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb945624-7faa-11eb-86c1-acde48001122","bb9466fa-7faa-11eb-86c1-acde48001122","bb947654-7faa-11eb-86c1-acde48001122","bb9485b8-7faa-11eb-86c1-acde48001122","bb9494ea-7faa-11eb-86c1-acde48001122","bb94a43a-7faa-11eb-86c1-acde48001122","bb95b62c-7faa-11eb-86c1-acde48001122","bb95a8da-7faa-11eb-86c1-acde48001122","bb94b5b0-7faa-11eb-86c1-acde48001122","bb958f80-7faa-11eb-86c1-acde48001122","bb94c17c-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb94e9ae-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb95048e-7faa-11eb-86c1-acde48001122","bb951f78-7faa-11eb-86c1-acde48001122","bb953a62-7faa-11eb-86c1-acde48001122","bb955722-7faa-11eb-86c1-acde48001122","bb957374-7faa-11eb-86c1-acde48001122","bb96c918-7faa-11eb-86c1-acde48001122","bb96bc02-7faa-11eb-86c1-acde48001122","bb95c9be-7faa-11eb-86c1-acde48001122","bb96a316-7faa-11eb-86c1-acde48001122","bb95d5c6-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb95fdf8-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb961978-7faa-11eb-86c1-acde48001122","bb9634e4-7faa-11eb-86c1-acde48001122","bb965050-7faa-11eb-86c1-acde48001122","bb966bb2-7faa-11eb-86c1-acde48001122","bb968728-7faa-11eb-86c1-acde48001122","bb9aa006-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9790c8-7faa-11eb-86c1-acde48001122","bb97a018-7faa-11eb-86c1-acde48001122","bb97afa4-7faa-11eb-86c1-acde48001122","bb97bf76-7faa-11eb-86c1-acde48001122","bb97ce94-7faa-11eb-86c1-acde48001122","bb97ddee-7faa-11eb-86c1-acde48001122","bb994332-7faa-11eb-86c1-acde48001122","bb992ed8-7faa-11eb-86c1-acde48001122","bb97eeb0-7faa-11eb-86c1-acde48001122","bb990322-7faa-11eb-86c1-acde48001122","bb97fad6-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb982524-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb984392-7faa-11eb-86c1-acde48001122","bb9863a4-7faa-11eb-86c1-acde48001122","bb9884a6-7faa-11eb-86c1-acde48001122","bb98aa26-7faa-11eb-86c1-acde48001122","bb98d41a-7faa-11eb-86c1-acde48001122","bb9a7fae-7faa-11eb-86c1-acde48001122","bb9a7216-7faa-11eb-86c1-acde48001122","bb995f0c-7faa-11eb-86c1-acde48001122","bb9a56d2-7faa-11eb-86c1-acde48001122","bb996fe2-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb99a5b6-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99c3f2-7faa-11eb-86c1-acde48001122","bb99e152-7faa-11eb-86c1-acde48001122","bb99fe76-7faa-11eb-86c1-acde48001122","bb9a1bc2-7faa-11eb-86c1-acde48001122","bb9a3940-7faa-11eb-86c1-acde48001122","bb9de400-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9b4ae2-7faa-11eb-86c1-acde48001122","bb9b5a78-7faa-11eb-86c1-acde48001122","bb9b69d2-7faa-11eb-86c1-acde48001122","bb9b794a-7faa-11eb-86c1-acde48001122","bb9b8872-7faa-11eb-86c1-acde48001122","bb9b97ae-7faa-11eb-86c1-acde48001122","bb9cb062-7faa-11eb-86c1-acde48001122","bb9ca31a-7faa-11eb-86c1-acde48001122","bb9ba956-7faa-11eb-86c1-acde48001122","bb9c89ca-7faa-11eb-86c1-acde48001122","bb9bb5e0-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9be0a6-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9bfd5c-7faa-11eb-86c1-acde48001122","bb9c19c2-7faa-11eb-86c1-acde48001122","bb9c35ec-7faa-11eb-86c1-acde48001122","bb9c51a8-7faa-11eb-86c1-acde48001122","bb9c6d8c-7faa-11eb-86c1-acde48001122","bb9dc4ac-7faa-11eb-86c1-acde48001122","bb9db746-7faa-11eb-86c1-acde48001122","bb9cc462-7faa-11eb-86c1-acde48001122","bb9d9e00-7faa-11eb-86c1-acde48001122","bb9cd056-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9cf950-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d14ee-7faa-11eb-86c1-acde48001122","bb9d305a-7faa-11eb-86c1-acde48001122","bb9d4bc6-7faa-11eb-86c1-acde48001122","bb9d670a-7faa-11eb-86c1-acde48001122","bb9d8262-7faa-11eb-86c1-acde48001122","bba12480-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bb9e9bc0-7faa-11eb-86c1-acde48001122","bb9eabec-7faa-11eb-86c1-acde48001122","bb9ebb00-7faa-11eb-86c1-acde48001122","bb9eca00-7faa-11eb-86c1-acde48001122","bb9ed8d8-7faa-11eb-86c1-acde48001122","bb9ee79c-7faa-11eb-86c1-acde48001122","bb9ff8bc-7faa-11eb-86c1-acde48001122","bb9febb0-7faa-11eb-86c1-acde48001122","bb9ef840-7faa-11eb-86c1-acde48001122","bb9fd2ce-7faa-11eb-86c1-acde48001122","bb9f0420-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9f2f0e-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f4b56-7faa-11eb-86c1-acde48001122","bb9f6668-7faa-11eb-86c1-acde48001122","bb9f81a2-7faa-11eb-86c1-acde48001122","bb9f9c82-7faa-11eb-86c1-acde48001122","bb9fb76c-7faa-11eb-86c1-acde48001122","bba105a4-7faa-11eb-86c1-acde48001122","bba0f8ac-7faa-11eb-86c1-acde48001122","bba00bc2-7faa-11eb-86c1-acde48001122","bba0dfc0-7faa-11eb-86c1-acde48001122","bba01752-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba03f02-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba05a5a-7faa-11eb-86c1-acde48001122","bba074ea-7faa-11eb-86c1-acde48001122","bba08f84-7faa-11eb-86c1-acde48001122","bba0aa0a-7faa-11eb-86c1-acde48001122","bba0c4a4-7faa-11eb-86c1-acde48001122","bba49336-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba1d2ae-7faa-11eb-86c1-acde48001122","bba1e280-7faa-11eb-86c1-acde48001122","bba1f20c-7faa-11eb-86c1-acde48001122","bba20134-7faa-11eb-86c1-acde48001122","bba2146c-7faa-11eb-86c1-acde48001122","bba22376-7faa-11eb-86c1-acde48001122","bba34f30-7faa-11eb-86c1-acde48001122","bba33f04-7faa-11eb-86c1-acde48001122","bba23456-7faa-11eb-86c1-acde48001122","bba322bc-7faa-11eb-86c1-acde48001122","bba24072-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba26a20-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba287b2-7faa-11eb-86c1-acde48001122","bba2a558-7faa-11eb-86c1-acde48001122","bba2c2fe-7faa-11eb-86c1-acde48001122","bba2e298-7faa-11eb-86c1-acde48001122","bba3032c-7faa-11eb-86c1-acde48001122","bba4741e-7faa-11eb-86c1-acde48001122","bba466e0-7faa-11eb-86c1-acde48001122","bba367b8-7faa-11eb-86c1-acde48001122","bba44d86-7faa-11eb-86c1-acde48001122","bba3765e-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba3a5a2-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3c406-7faa-11eb-86c1-acde48001122","bba3e058-7faa-11eb-86c1-acde48001122","bba3fbce-7faa-11eb-86c1-acde48001122","bba41730-7faa-11eb-86c1-acde48001122","bba43210-7faa-11eb-86c1-acde48001122","bba7d884-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba541fa-7faa-11eb-86c1-acde48001122","bba55320-7faa-11eb-86c1-acde48001122","bba56234-7faa-11eb-86c1-acde48001122","bba57134-7faa-11eb-86c1-acde48001122","bba5800c-7faa-11eb-86c1-acde48001122","bba58ec6-7faa-11eb-86c1-acde48001122","bba6a784-7faa-11eb-86c1-acde48001122","bba69a28-7faa-11eb-86c1-acde48001122","bba59f9c-7faa-11eb-86c1-acde48001122","bba67f8e-7faa-11eb-86c1-acde48001122","bba5ab9a-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba5d412-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba5efb0-7faa-11eb-86c1-acde48001122","bba60c48-7faa-11eb-86c1-acde48001122","bba628a4-7faa-11eb-86c1-acde48001122","bba64546-7faa-11eb-86c1-acde48001122","bba66256-7faa-11eb-86c1-acde48001122","bba7b9a8-7faa-11eb-86c1-acde48001122","bba7acb0-7faa-11eb-86c1-acde48001122","bba6bbc0-7faa-11eb-86c1-acde48001122","bba793c4-7faa-11eb-86c1-acde48001122","bba6c796-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba6f018-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba70ba2-7faa-11eb-86c1-acde48001122","bba726aa-7faa-11eb-86c1-acde48001122","bba741d0-7faa-11eb-86c1-acde48001122","bba75d1e-7faa-11eb-86c1-acde48001122","bba7784e-7faa-11eb-86c1-acde48001122","bbab124c-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bba88e96-7faa-11eb-86c1-acde48001122","bba89de6-7faa-11eb-86c1-acde48001122","bba8acd2-7faa-11eb-86c1-acde48001122","bba8bba0-7faa-11eb-86c1-acde48001122","bba8ca5a-7faa-11eb-86c1-acde48001122","bba8d91e-7faa-11eb-86c1-acde48001122","bba9e4c6-7faa-11eb-86c1-acde48001122","bba9d7b0-7faa-11eb-86c1-acde48001122","bba8e9b8-7faa-11eb-86c1-acde48001122","bba9bef6-7faa-11eb-86c1-acde48001122","bba8f58e-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba91da2-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba9388c-7faa-11eb-86c1-acde48001122","bba95376-7faa-11eb-86c1-acde48001122","bba96e38-7faa-11eb-86c1-acde48001122","bba988fa-7faa-11eb-86c1-acde48001122","bba9a3a8-7faa-11eb-86c1-acde48001122","bbaaf3ca-7faa-11eb-86c1-acde48001122","bbaae6d2-7faa-11eb-86c1-acde48001122","bba9f808-7faa-11eb-86c1-acde48001122","bbaace36-7faa-11eb-86c1-acde48001122","bbaa03d4-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaa2bc0-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa4722-7faa-11eb-86c1-acde48001122","bbaa627a-7faa-11eb-86c1-acde48001122","bbaa7d46-7faa-11eb-86c1-acde48001122","bbaa981c-7faa-11eb-86c1-acde48001122","bbaab2fc-7faa-11eb-86c1-acde48001122","bbab4780-7faa-11eb-86c1-acde48001122","bbab61b6-7faa-11eb-86c1-acde48001122"],"label":["CV_","bundle","CV_Stack","bundle","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","bundle","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_importance_5->Stack)","bundle","Lrnr_screener_importance_5","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","Pipeline(Lrnr_screener_coefs_0_NULL->Stack)","bundle","Lrnr_screener_coefs_0_NULL","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_polspline_5","Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_50_1","chain","Lrnr_solnp_TRUE_TRUE_FALSE_1e-05"],"level":[1,2,5,6,7,8,9,9,9,9,9,9,9,10,16,11,15,12,13,14,13,13,13,13,13,9,10,16,11,15,12,13,14,13,13,13,13,13,7,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,8,9,10,10,10,10,10,10,10,11,17,12,16,13,14,15,14,14,14,14,14,10,11,17,12,16,13,14,15,14,14,14,14,14,4,3],"sequential":[true,true,true,true,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,true,false,true,true,true,false,false,false,false,false,false,false,true,false],"state":["waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","ready","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting"],"group":["none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none"]},"edges":{"from":["bb7272a2-7faa-11eb-86c1-acde48001122","bb728288-7faa-11eb-86c1-acde48001122","bb729192-7faa-11eb-86c1-acde48001122","bb72a056-7faa-11eb-86c1-acde48001122","bb72af10-7faa-11eb-86c1-acde48001122","bb72bdf2-7faa-11eb-86c1-acde48001122","bb72ced2-7faa-11eb-86c1-acde48001122","bb72ced2-7faa-11eb-86c1-acde48001122","bb72dda0-7faa-11eb-86c1-acde48001122","bb72dda0-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb730618-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb73215c-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb733d54-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb73588e-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb7373aa-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb738ebc-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb73b130-7faa-11eb-86c1-acde48001122","bb73ca3a-7faa-11eb-86c1-acde48001122","bb73d76e-7faa-11eb-86c1-acde48001122","bb73eb1e-7faa-11eb-86c1-acde48001122","bb73eb1e-7faa-11eb-86c1-acde48001122","bb73f6f4-7faa-11eb-86c1-acde48001122","bb73f6f4-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb741fbc-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb743b28-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb745676-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb7475a2-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb749370-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb74afd6-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb74cfb6-7faa-11eb-86c1-acde48001122","bb74ea3c-7faa-11eb-86c1-acde48001122","bb74f7de-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb7517fa-7faa-11eb-86c1-acde48001122","bb7a17e6-7faa-11eb-86c1-acde48001122","bb7a2830-7faa-11eb-86c1-acde48001122","bb7a3762-7faa-11eb-86c1-acde48001122","bb7a46d0-7faa-11eb-86c1-acde48001122","bb7a5648-7faa-11eb-86c1-acde48001122","bb7a65de-7faa-11eb-86c1-acde48001122","bb7b427e-7faa-11eb-86c1-acde48001122","bb7b427e-7faa-11eb-86c1-acde48001122","bb7b4f62-7faa-11eb-86c1-acde48001122","bb7b4f62-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7b78a2-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7b940e-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7baeda-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7bc9ec-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7be512-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7c0056-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7c1c8a-7faa-11eb-86c1-acde48001122","bb7c362a-7faa-11eb-86c1-acde48001122","bb7c4372-7faa-11eb-86c1-acde48001122","bb7c57cc-7faa-11eb-86c1-acde48001122","bb7c57cc-7faa-11eb-86c1-acde48001122","bb7c637a-7faa-11eb-86c1-acde48001122","bb7c637a-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7c8bc0-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7ca740-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7cc284-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7cde18-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7cfa24-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7d1568-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7d30fc-7faa-11eb-86c1-acde48001122","bb7d49a2-7faa-11eb-86c1-acde48001122","bb7d5690-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d7530-7faa-11eb-86c1-acde48001122","bb8c43b2-7faa-11eb-86c1-acde48001122","bb8c541a-7faa-11eb-86c1-acde48001122","bb8c6310-7faa-11eb-86c1-acde48001122","bb8c71fc-7faa-11eb-86c1-acde48001122","bb8c817e-7faa-11eb-86c1-acde48001122","bb8c9074-7faa-11eb-86c1-acde48001122","bb8ca140-7faa-11eb-86c1-acde48001122","bb8ca140-7faa-11eb-86c1-acde48001122","bb8cad52-7faa-11eb-86c1-acde48001122","bb8cad52-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8cd5e8-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8cf104-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8d0c3e-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8d2778-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8d428a-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8d5d7e-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d78c2-7faa-11eb-86c1-acde48001122","bb8d919a-7faa-11eb-86c1-acde48001122","bb8d9e9c-7faa-11eb-86c1-acde48001122","bb8db18e-7faa-11eb-86c1-acde48001122","bb8db18e-7faa-11eb-86c1-acde48001122","bb8dbd32-7faa-11eb-86c1-acde48001122","bb8dbd32-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8de546-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e0062-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e1c0a-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e3780-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e52b0-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8e6ef8-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e8bd6-7faa-11eb-86c1-acde48001122","bb8eaa1c-7faa-11eb-86c1-acde48001122","bb8eba16-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8edbf4-7faa-11eb-86c1-acde48001122","bb8f88ba-7faa-11eb-86c1-acde48001122","bb8f9864-7faa-11eb-86c1-acde48001122","bb8fa796-7faa-11eb-86c1-acde48001122","bb8fb7f4-7faa-11eb-86c1-acde48001122","bb90b672-7faa-11eb-86c1-acde48001122","bb90d332-7faa-11eb-86c1-acde48001122","bb90f060-7faa-11eb-86c1-acde48001122","bb90f060-7faa-11eb-86c1-acde48001122","bb91023a-7faa-11eb-86c1-acde48001122","bb91023a-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb913f2a-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb916306-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb9188a4-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb91aa96-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb91ccce-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb91edb2-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb920e50-7faa-11eb-86c1-acde48001122","bb922bf6-7faa-11eb-86c1-acde48001122","bb923b64-7faa-11eb-86c1-acde48001122","bb9252fc-7faa-11eb-86c1-acde48001122","bb9252fc-7faa-11eb-86c1-acde48001122","bb9260d0-7faa-11eb-86c1-acde48001122","bb9260d0-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb9291fe-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb92b22e-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb92d5c4-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb92f540-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb9314c6-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb93344c-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb9353b4-7faa-11eb-86c1-acde48001122","bb936f34-7faa-11eb-86c1-acde48001122","bb937e5c-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb93a030-7faa-11eb-86c1-acde48001122","bb945624-7faa-11eb-86c1-acde48001122","bb9466fa-7faa-11eb-86c1-acde48001122","bb947654-7faa-11eb-86c1-acde48001122","bb9485b8-7faa-11eb-86c1-acde48001122","bb9494ea-7faa-11eb-86c1-acde48001122","bb94a43a-7faa-11eb-86c1-acde48001122","bb94b5b0-7faa-11eb-86c1-acde48001122","bb94b5b0-7faa-11eb-86c1-acde48001122","bb94c17c-7faa-11eb-86c1-acde48001122","bb94c17c-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb94e9ae-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb95048e-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb951f78-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb953a62-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb955722-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb957374-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb958f80-7faa-11eb-86c1-acde48001122","bb95a8da-7faa-11eb-86c1-acde48001122","bb95b62c-7faa-11eb-86c1-acde48001122","bb95c9be-7faa-11eb-86c1-acde48001122","bb95c9be-7faa-11eb-86c1-acde48001122","bb95d5c6-7faa-11eb-86c1-acde48001122","bb95d5c6-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb95fdf8-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb961978-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb9634e4-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb965050-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb966bb2-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb968728-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb96a316-7faa-11eb-86c1-acde48001122","bb96bc02-7faa-11eb-86c1-acde48001122","bb96c918-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96e8bc-7faa-11eb-86c1-acde48001122","bb9790c8-7faa-11eb-86c1-acde48001122","bb97a018-7faa-11eb-86c1-acde48001122","bb97afa4-7faa-11eb-86c1-acde48001122","bb97bf76-7faa-11eb-86c1-acde48001122","bb97ce94-7faa-11eb-86c1-acde48001122","bb97ddee-7faa-11eb-86c1-acde48001122","bb97eeb0-7faa-11eb-86c1-acde48001122","bb97eeb0-7faa-11eb-86c1-acde48001122","bb97fad6-7faa-11eb-86c1-acde48001122","bb97fad6-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb982524-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb984392-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb9863a4-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb9884a6-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb98aa26-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb98d41a-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb990322-7faa-11eb-86c1-acde48001122","bb992ed8-7faa-11eb-86c1-acde48001122","bb994332-7faa-11eb-86c1-acde48001122","bb995f0c-7faa-11eb-86c1-acde48001122","bb995f0c-7faa-11eb-86c1-acde48001122","bb996fe2-7faa-11eb-86c1-acde48001122","bb996fe2-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99a5b6-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99c3f2-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99e152-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99fe76-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb9a1bc2-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb9a3940-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb9a56d2-7faa-11eb-86c1-acde48001122","bb9a7216-7faa-11eb-86c1-acde48001122","bb9a7fae-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9aa006-7faa-11eb-86c1-acde48001122","bb9b4ae2-7faa-11eb-86c1-acde48001122","bb9b5a78-7faa-11eb-86c1-acde48001122","bb9b69d2-7faa-11eb-86c1-acde48001122","bb9b794a-7faa-11eb-86c1-acde48001122","bb9b8872-7faa-11eb-86c1-acde48001122","bb9b97ae-7faa-11eb-86c1-acde48001122","bb9ba956-7faa-11eb-86c1-acde48001122","bb9ba956-7faa-11eb-86c1-acde48001122","bb9bb5e0-7faa-11eb-86c1-acde48001122","bb9bb5e0-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9be0a6-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9bfd5c-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9c19c2-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9c35ec-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9c51a8-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9c6d8c-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c89ca-7faa-11eb-86c1-acde48001122","bb9ca31a-7faa-11eb-86c1-acde48001122","bb9cb062-7faa-11eb-86c1-acde48001122","bb9cc462-7faa-11eb-86c1-acde48001122","bb9cc462-7faa-11eb-86c1-acde48001122","bb9cd056-7faa-11eb-86c1-acde48001122","bb9cd056-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9cf950-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d14ee-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d305a-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d4bc6-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d670a-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9d8262-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d9e00-7faa-11eb-86c1-acde48001122","bb9db746-7faa-11eb-86c1-acde48001122","bb9dc4ac-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9de400-7faa-11eb-86c1-acde48001122","bb9e9bc0-7faa-11eb-86c1-acde48001122","bb9eabec-7faa-11eb-86c1-acde48001122","bb9ebb00-7faa-11eb-86c1-acde48001122","bb9eca00-7faa-11eb-86c1-acde48001122","bb9ed8d8-7faa-11eb-86c1-acde48001122","bb9ee79c-7faa-11eb-86c1-acde48001122","bb9ef840-7faa-11eb-86c1-acde48001122","bb9ef840-7faa-11eb-86c1-acde48001122","bb9f0420-7faa-11eb-86c1-acde48001122","bb9f0420-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f2f0e-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f4b56-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f6668-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f81a2-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f9c82-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9fb76c-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9fd2ce-7faa-11eb-86c1-acde48001122","bb9febb0-7faa-11eb-86c1-acde48001122","bb9ff8bc-7faa-11eb-86c1-acde48001122","bba00bc2-7faa-11eb-86c1-acde48001122","bba00bc2-7faa-11eb-86c1-acde48001122","bba01752-7faa-11eb-86c1-acde48001122","bba01752-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba03f02-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba05a5a-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba074ea-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba08f84-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba0aa0a-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba0c4a4-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba0dfc0-7faa-11eb-86c1-acde48001122","bba0f8ac-7faa-11eb-86c1-acde48001122","bba105a4-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba12480-7faa-11eb-86c1-acde48001122","bba1d2ae-7faa-11eb-86c1-acde48001122","bba1e280-7faa-11eb-86c1-acde48001122","bba1f20c-7faa-11eb-86c1-acde48001122","bba20134-7faa-11eb-86c1-acde48001122","bba2146c-7faa-11eb-86c1-acde48001122","bba22376-7faa-11eb-86c1-acde48001122","bba23456-7faa-11eb-86c1-acde48001122","bba23456-7faa-11eb-86c1-acde48001122","bba24072-7faa-11eb-86c1-acde48001122","bba24072-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba26a20-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba287b2-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba2a558-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba2c2fe-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba2e298-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba3032c-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba322bc-7faa-11eb-86c1-acde48001122","bba33f04-7faa-11eb-86c1-acde48001122","bba34f30-7faa-11eb-86c1-acde48001122","bba367b8-7faa-11eb-86c1-acde48001122","bba367b8-7faa-11eb-86c1-acde48001122","bba3765e-7faa-11eb-86c1-acde48001122","bba3765e-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3a5a2-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3c406-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3e058-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3fbce-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba41730-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba43210-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba44d86-7faa-11eb-86c1-acde48001122","bba466e0-7faa-11eb-86c1-acde48001122","bba4741e-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba49336-7faa-11eb-86c1-acde48001122","bba541fa-7faa-11eb-86c1-acde48001122","bba55320-7faa-11eb-86c1-acde48001122","bba56234-7faa-11eb-86c1-acde48001122","bba57134-7faa-11eb-86c1-acde48001122","bba5800c-7faa-11eb-86c1-acde48001122","bba58ec6-7faa-11eb-86c1-acde48001122","bba59f9c-7faa-11eb-86c1-acde48001122","bba59f9c-7faa-11eb-86c1-acde48001122","bba5ab9a-7faa-11eb-86c1-acde48001122","bba5ab9a-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba5d412-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba5efb0-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba60c48-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba628a4-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba64546-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba66256-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba67f8e-7faa-11eb-86c1-acde48001122","bba69a28-7faa-11eb-86c1-acde48001122","bba6a784-7faa-11eb-86c1-acde48001122","bba6bbc0-7faa-11eb-86c1-acde48001122","bba6bbc0-7faa-11eb-86c1-acde48001122","bba6c796-7faa-11eb-86c1-acde48001122","bba6c796-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba6f018-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba70ba2-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba726aa-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba741d0-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba75d1e-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba7784e-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba793c4-7faa-11eb-86c1-acde48001122","bba7acb0-7faa-11eb-86c1-acde48001122","bba7b9a8-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7d884-7faa-11eb-86c1-acde48001122","bba88e96-7faa-11eb-86c1-acde48001122","bba89de6-7faa-11eb-86c1-acde48001122","bba8acd2-7faa-11eb-86c1-acde48001122","bba8bba0-7faa-11eb-86c1-acde48001122","bba8ca5a-7faa-11eb-86c1-acde48001122","bba8d91e-7faa-11eb-86c1-acde48001122","bba8e9b8-7faa-11eb-86c1-acde48001122","bba8e9b8-7faa-11eb-86c1-acde48001122","bba8f58e-7faa-11eb-86c1-acde48001122","bba8f58e-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba91da2-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba9388c-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba95376-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba96e38-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba988fa-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba9a3a8-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba9bef6-7faa-11eb-86c1-acde48001122","bba9d7b0-7faa-11eb-86c1-acde48001122","bba9e4c6-7faa-11eb-86c1-acde48001122","bba9f808-7faa-11eb-86c1-acde48001122","bba9f808-7faa-11eb-86c1-acde48001122","bbaa03d4-7faa-11eb-86c1-acde48001122","bbaa03d4-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa2bc0-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa4722-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa627a-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa7d46-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa981c-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaab2fc-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaace36-7faa-11eb-86c1-acde48001122","bbaae6d2-7faa-11eb-86c1-acde48001122","bbaaf3ca-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab124c-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bbab2d2c-7faa-11eb-86c1-acde48001122","bbab3a42-7faa-11eb-86c1-acde48001122","bbab3a42-7faa-11eb-86c1-acde48001122","bbab4780-7faa-11eb-86c1-acde48001122","bbab4780-7faa-11eb-86c1-acde48001122","bbab61b6-7faa-11eb-86c1-acde48001122","bbab7052-7faa-11eb-86c1-acde48001122"],"to":["bb750940-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb73ca3a-7faa-11eb-86c1-acde48001122","bb72dda0-7faa-11eb-86c1-acde48001122","bb73b130-7faa-11eb-86c1-acde48001122","bb72eb2e-7faa-11eb-86c1-acde48001122","bb730618-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb73215c-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb733d54-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb73588e-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb7373aa-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb738ebc-7faa-11eb-86c1-acde48001122","bb73a320-7faa-11eb-86c1-acde48001122","bb73b130-7faa-11eb-86c1-acde48001122","bb73ca3a-7faa-11eb-86c1-acde48001122","bb73d76e-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb74ea3c-7faa-11eb-86c1-acde48001122","bb73f6f4-7faa-11eb-86c1-acde48001122","bb74cfb6-7faa-11eb-86c1-acde48001122","bb740478-7faa-11eb-86c1-acde48001122","bb741fbc-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb743b28-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb745676-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb7475a2-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb749370-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb74afd6-7faa-11eb-86c1-acde48001122","bb74bf30-7faa-11eb-86c1-acde48001122","bb74cfb6-7faa-11eb-86c1-acde48001122","bb74ea3c-7faa-11eb-86c1-acde48001122","bb74f7de-7faa-11eb-86c1-acde48001122","bb750940-7faa-11eb-86c1-acde48001122","bb7517fa-7faa-11eb-86c1-acde48001122","bbab2d2c-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7c362a-7faa-11eb-86c1-acde48001122","bb7b4f62-7faa-11eb-86c1-acde48001122","bb7c1c8a-7faa-11eb-86c1-acde48001122","bb7b5d4a-7faa-11eb-86c1-acde48001122","bb7b78a2-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7b940e-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7baeda-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7bc9ec-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7be512-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7c0056-7faa-11eb-86c1-acde48001122","bb7c0df8-7faa-11eb-86c1-acde48001122","bb7c1c8a-7faa-11eb-86c1-acde48001122","bb7c362a-7faa-11eb-86c1-acde48001122","bb7c4372-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d49a2-7faa-11eb-86c1-acde48001122","bb7c637a-7faa-11eb-86c1-acde48001122","bb7d30fc-7faa-11eb-86c1-acde48001122","bb7c70ae-7faa-11eb-86c1-acde48001122","bb7c8bc0-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7ca740-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7cc284-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7cde18-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7cfa24-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7d1568-7faa-11eb-86c1-acde48001122","bb7d2328-7faa-11eb-86c1-acde48001122","bb7d30fc-7faa-11eb-86c1-acde48001122","bb7d49a2-7faa-11eb-86c1-acde48001122","bb7d5690-7faa-11eb-86c1-acde48001122","bb7d66da-7faa-11eb-86c1-acde48001122","bb7d7530-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8d919a-7faa-11eb-86c1-acde48001122","bb8cad52-7faa-11eb-86c1-acde48001122","bb8d78c2-7faa-11eb-86c1-acde48001122","bb8cbab8-7faa-11eb-86c1-acde48001122","bb8cd5e8-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8cf104-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d0c3e-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d2778-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d428a-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d5d7e-7faa-11eb-86c1-acde48001122","bb8d6b0c-7faa-11eb-86c1-acde48001122","bb8d78c2-7faa-11eb-86c1-acde48001122","bb8d919a-7faa-11eb-86c1-acde48001122","bb8d9e9c-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8eaa1c-7faa-11eb-86c1-acde48001122","bb8dbd32-7faa-11eb-86c1-acde48001122","bb8e8bd6-7faa-11eb-86c1-acde48001122","bb8dca7a-7faa-11eb-86c1-acde48001122","bb8de546-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e0062-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e1c0a-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e3780-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e52b0-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e6ef8-7faa-11eb-86c1-acde48001122","bb8e7d08-7faa-11eb-86c1-acde48001122","bb8e8bd6-7faa-11eb-86c1-acde48001122","bb8eaa1c-7faa-11eb-86c1-acde48001122","bb8eba16-7faa-11eb-86c1-acde48001122","bb8ecd08-7faa-11eb-86c1-acde48001122","bb8edbf4-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb922bf6-7faa-11eb-86c1-acde48001122","bb91023a-7faa-11eb-86c1-acde48001122","bb920e50-7faa-11eb-86c1-acde48001122","bb9117ac-7faa-11eb-86c1-acde48001122","bb913f2a-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb916306-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb9188a4-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb91aa96-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb91ccce-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb91edb2-7faa-11eb-86c1-acde48001122","bb91fdca-7faa-11eb-86c1-acde48001122","bb920e50-7faa-11eb-86c1-acde48001122","bb922bf6-7faa-11eb-86c1-acde48001122","bb923b64-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb936f34-7faa-11eb-86c1-acde48001122","bb9260d0-7faa-11eb-86c1-acde48001122","bb9353b4-7faa-11eb-86c1-acde48001122","bb927192-7faa-11eb-86c1-acde48001122","bb9291fe-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb92b22e-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb92d5c4-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb92f540-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb9314c6-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb93344c-7faa-11eb-86c1-acde48001122","bb934392-7faa-11eb-86c1-acde48001122","bb9353b4-7faa-11eb-86c1-acde48001122","bb936f34-7faa-11eb-86c1-acde48001122","bb937e5c-7faa-11eb-86c1-acde48001122","bb938ff0-7faa-11eb-86c1-acde48001122","bb93a030-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb95a8da-7faa-11eb-86c1-acde48001122","bb94c17c-7faa-11eb-86c1-acde48001122","bb958f80-7faa-11eb-86c1-acde48001122","bb94cef6-7faa-11eb-86c1-acde48001122","bb94e9ae-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb95048e-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb951f78-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb953a62-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb955722-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb957374-7faa-11eb-86c1-acde48001122","bb958170-7faa-11eb-86c1-acde48001122","bb958f80-7faa-11eb-86c1-acde48001122","bb95a8da-7faa-11eb-86c1-acde48001122","bb95b62c-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96bc02-7faa-11eb-86c1-acde48001122","bb95d5c6-7faa-11eb-86c1-acde48001122","bb96a316-7faa-11eb-86c1-acde48001122","bb95e32c-7faa-11eb-86c1-acde48001122","bb95fdf8-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb961978-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb9634e4-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb965050-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb966bb2-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb968728-7faa-11eb-86c1-acde48001122","bb9694fc-7faa-11eb-86c1-acde48001122","bb96a316-7faa-11eb-86c1-acde48001122","bb96bc02-7faa-11eb-86c1-acde48001122","bb96c918-7faa-11eb-86c1-acde48001122","bb96d9ee-7faa-11eb-86c1-acde48001122","bb96e8bc-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb992ed8-7faa-11eb-86c1-acde48001122","bb97fad6-7faa-11eb-86c1-acde48001122","bb990322-7faa-11eb-86c1-acde48001122","bb9808b4-7faa-11eb-86c1-acde48001122","bb982524-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb984392-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb9863a4-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb9884a6-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb98aa26-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb98d41a-7faa-11eb-86c1-acde48001122","bb98e86a-7faa-11eb-86c1-acde48001122","bb990322-7faa-11eb-86c1-acde48001122","bb992ed8-7faa-11eb-86c1-acde48001122","bb994332-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9a7216-7faa-11eb-86c1-acde48001122","bb996fe2-7faa-11eb-86c1-acde48001122","bb9a56d2-7faa-11eb-86c1-acde48001122","bb99852c-7faa-11eb-86c1-acde48001122","bb99a5b6-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb99c3f2-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb99e152-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb99fe76-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb9a1bc2-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb9a3940-7faa-11eb-86c1-acde48001122","bb9a47f0-7faa-11eb-86c1-acde48001122","bb9a56d2-7faa-11eb-86c1-acde48001122","bb9a7216-7faa-11eb-86c1-acde48001122","bb9a7fae-7faa-11eb-86c1-acde48001122","bb9a90d4-7faa-11eb-86c1-acde48001122","bb9aa006-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9ca31a-7faa-11eb-86c1-acde48001122","bb9bb5e0-7faa-11eb-86c1-acde48001122","bb9c89ca-7faa-11eb-86c1-acde48001122","bb9bc45e-7faa-11eb-86c1-acde48001122","bb9be0a6-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9bfd5c-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c19c2-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c35ec-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c51a8-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c6d8c-7faa-11eb-86c1-acde48001122","bb9c7b88-7faa-11eb-86c1-acde48001122","bb9c89ca-7faa-11eb-86c1-acde48001122","bb9ca31a-7faa-11eb-86c1-acde48001122","bb9cb062-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9db746-7faa-11eb-86c1-acde48001122","bb9cd056-7faa-11eb-86c1-acde48001122","bb9d9e00-7faa-11eb-86c1-acde48001122","bb9cde02-7faa-11eb-86c1-acde48001122","bb9cf950-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d14ee-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d305a-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d4bc6-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d670a-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d8262-7faa-11eb-86c1-acde48001122","bb9d9004-7faa-11eb-86c1-acde48001122","bb9d9e00-7faa-11eb-86c1-acde48001122","bb9db746-7faa-11eb-86c1-acde48001122","bb9dc4ac-7faa-11eb-86c1-acde48001122","bb9dd532-7faa-11eb-86c1-acde48001122","bb9de400-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bb9febb0-7faa-11eb-86c1-acde48001122","bb9f0420-7faa-11eb-86c1-acde48001122","bb9fd2ce-7faa-11eb-86c1-acde48001122","bb9f11ae-7faa-11eb-86c1-acde48001122","bb9f2f0e-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9f4b56-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9f6668-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9f81a2-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9f9c82-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9fb76c-7faa-11eb-86c1-acde48001122","bb9fc50e-7faa-11eb-86c1-acde48001122","bb9fd2ce-7faa-11eb-86c1-acde48001122","bb9febb0-7faa-11eb-86c1-acde48001122","bb9ff8bc-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba0f8ac-7faa-11eb-86c1-acde48001122","bba01752-7faa-11eb-86c1-acde48001122","bba0dfc0-7faa-11eb-86c1-acde48001122","bba02468-7faa-11eb-86c1-acde48001122","bba03f02-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba05a5a-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba074ea-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba08f84-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba0aa0a-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba0c4a4-7faa-11eb-86c1-acde48001122","bba0d1ec-7faa-11eb-86c1-acde48001122","bba0dfc0-7faa-11eb-86c1-acde48001122","bba0f8ac-7faa-11eb-86c1-acde48001122","bba105a4-7faa-11eb-86c1-acde48001122","bba11602-7faa-11eb-86c1-acde48001122","bba12480-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba33f04-7faa-11eb-86c1-acde48001122","bba24072-7faa-11eb-86c1-acde48001122","bba322bc-7faa-11eb-86c1-acde48001122","bba24e3c-7faa-11eb-86c1-acde48001122","bba26a20-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba287b2-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba2a558-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba2c2fe-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba2e298-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba3032c-7faa-11eb-86c1-acde48001122","bba312a4-7faa-11eb-86c1-acde48001122","bba322bc-7faa-11eb-86c1-acde48001122","bba33f04-7faa-11eb-86c1-acde48001122","bba34f30-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba466e0-7faa-11eb-86c1-acde48001122","bba3765e-7faa-11eb-86c1-acde48001122","bba44d86-7faa-11eb-86c1-acde48001122","bba386c6-7faa-11eb-86c1-acde48001122","bba3a5a2-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba3c406-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba3e058-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba3fbce-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba41730-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba43210-7faa-11eb-86c1-acde48001122","bba43f9e-7faa-11eb-86c1-acde48001122","bba44d86-7faa-11eb-86c1-acde48001122","bba466e0-7faa-11eb-86c1-acde48001122","bba4741e-7faa-11eb-86c1-acde48001122","bba484a4-7faa-11eb-86c1-acde48001122","bba49336-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba69a28-7faa-11eb-86c1-acde48001122","bba5ab9a-7faa-11eb-86c1-acde48001122","bba67f8e-7faa-11eb-86c1-acde48001122","bba5b946-7faa-11eb-86c1-acde48001122","bba5d412-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba5efb0-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba60c48-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba628a4-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba64546-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba66256-7faa-11eb-86c1-acde48001122","bba67070-7faa-11eb-86c1-acde48001122","bba67f8e-7faa-11eb-86c1-acde48001122","bba69a28-7faa-11eb-86c1-acde48001122","bba6a784-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7acb0-7faa-11eb-86c1-acde48001122","bba6c796-7faa-11eb-86c1-acde48001122","bba793c4-7faa-11eb-86c1-acde48001122","bba6d524-7faa-11eb-86c1-acde48001122","bba6f018-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba70ba2-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba726aa-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba741d0-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba75d1e-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba7784e-7faa-11eb-86c1-acde48001122","bba785f0-7faa-11eb-86c1-acde48001122","bba793c4-7faa-11eb-86c1-acde48001122","bba7acb0-7faa-11eb-86c1-acde48001122","bba7b9a8-7faa-11eb-86c1-acde48001122","bba7ca10-7faa-11eb-86c1-acde48001122","bba7d884-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bba9d7b0-7faa-11eb-86c1-acde48001122","bba8f58e-7faa-11eb-86c1-acde48001122","bba9bef6-7faa-11eb-86c1-acde48001122","bba902f4-7faa-11eb-86c1-acde48001122","bba91da2-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba9388c-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba95376-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba96e38-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba988fa-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba9a3a8-7faa-11eb-86c1-acde48001122","bba9b118-7faa-11eb-86c1-acde48001122","bba9bef6-7faa-11eb-86c1-acde48001122","bba9d7b0-7faa-11eb-86c1-acde48001122","bba9e4c6-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbaae6d2-7faa-11eb-86c1-acde48001122","bbaa03d4-7faa-11eb-86c1-acde48001122","bbaace36-7faa-11eb-86c1-acde48001122","bbaa1112-7faa-11eb-86c1-acde48001122","bbaa2bc0-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaa4722-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaa627a-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaa7d46-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaa981c-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaab2fc-7faa-11eb-86c1-acde48001122","bbaac062-7faa-11eb-86c1-acde48001122","bbaace36-7faa-11eb-86c1-acde48001122","bbaae6d2-7faa-11eb-86c1-acde48001122","bbaaf3ca-7faa-11eb-86c1-acde48001122","bbab03f6-7faa-11eb-86c1-acde48001122","bbab124c-7faa-11eb-86c1-acde48001122","bbab1f76-7faa-11eb-86c1-acde48001122","bbab2d2c-7faa-11eb-86c1-acde48001122","bbab3a42-7faa-11eb-86c1-acde48001122","bbab7052-7faa-11eb-86c1-acde48001122","bbab4780-7faa-11eb-86c1-acde48001122","bbab7052-7faa-11eb-86c1-acde48001122","bbab61b6-7faa-11eb-86c1-acde48001122","bbab7052-7faa-11eb-86c1-acde48001122","bbab7dae-7faa-11eb-86c1-acde48001122"],"label":["","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot"},"manipulation":{"enabled":false},"edges":{"arrows":"to"},"layout":{"hierarchical":{"enabled":true,"levelSeparation":500,"nodeSpacing":200,"direction":"RL"}},"groups":{"useDefaultGroups":true,"none":{"color":{"border":"black","background":"white"}}}},"groups":["none"],"width":"90%","height":"400px","idselection":{"enabled":false},"byselection":{"enabled":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)"},"evals":[],"jsHooks":[]}</script><!--/html_preserve-->
--->

### 3. Train the Super Learner on the machine learning task {-}

The Super Learner algorithm fits a metalearner on the validation-set
predictions in a cross-validated manner, thereby avoiding overfitting.

Now we are ready to **"train"** our Super Learner on our `sl3_task` object,
`washb_task`.


```r
set.seed(4197)
sl_fit <- sl$train(washb_task)
```

### 4. Obtain predicted values {-}

Now that we have fit the super learner, we are ready to calculate the predicted
outcome for each subject.


```r
# we did it! now we have super learner predictions
sl_preds <- sl_fit$predict()
head(sl_preds)
#> [1] -0.66689 -0.81956 -0.70954 -0.71227 -0.65638 -0.66748
```
<!--
Below we visualize the observed versus predicted values.

For fun, we will also include the cross-validated predictions from most popular 
learner on the block, main terms linear regression.



```r

# df_plot <- data.frame(Observed = washb_data[["whz"]], Predicted = sl_preds,
#                        count = seq(1:nrow(washb_data))

# df_plot_melted <- melt(df_plot, id.vars = "count",
#                         measure.vars = c("Observed", "Predicted"))

# ggplot(df_plot_melted, aes(value, count, color = variable)) + geom_point()
```
-->

We can also obtain a summary of the results.


```r
sl_fit_summary <- sl_fit$print()
#> [1] "SuperLearner:"
#> List of 8
#>  $ glm                : chr "Lrnr_glm_TRUE"
#>  $ polspline          : chr "Lrnr_polspline_5"
#>  $ enet.5             : chr "Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE"
#>  $ ridge              : chr "Lrnr_glmnet_NULL_deviance_10_0_100_TRUE"
#>  $ lasso              : chr "Lrnr_glmnet_NULL_deviance_10_1_100_TRUE"
#>  $ xgboost50          : chr "Lrnr_xgboost_50_1"
#>  $ randomforest_screen: chr "Pipeline(Lrnr_screener_importance_5->Stack)"
#>  $ lasso_screen       : chr "Pipeline(Lrnr_screener_coefs_0_NULL->Stack)"
#> [1] "Lrnr_solnp_TRUE_TRUE_FALSE_1e-05"
#> $pars
#>  [1] 0.055571 0.055557 0.055564 0.055569 0.055564 0.055592 0.055543 0.055562
#>  [9] 0.055543 0.055543 0.055543 0.055522 0.055561 0.055559 0.055561 0.055560
#> [17] 0.055561 0.055526
#> 
#> $convergence
#> [1] 0
#> 
#> $values
#> [1] 1.0137 1.0137
#> 
#> $lagrange
#>            [,1]
#> [1,] -0.0015316
#> 
#> $hessian
#>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
#>  [1,]    1    0    0    0    0    0    0    0    0     0     0     0     0
#>  [2,]    0    1    0    0    0    0    0    0    0     0     0     0     0
#>  [3,]    0    0    1    0    0    0    0    0    0     0     0     0     0
#>  [4,]    0    0    0    1    0    0    0    0    0     0     0     0     0
#>  [5,]    0    0    0    0    1    0    0    0    0     0     0     0     0
#>  [6,]    0    0    0    0    0    1    0    0    0     0     0     0     0
#>  [7,]    0    0    0    0    0    0    1    0    0     0     0     0     0
#>  [8,]    0    0    0    0    0    0    0    1    0     0     0     0     0
#>  [9,]    0    0    0    0    0    0    0    0    1     0     0     0     0
#> [10,]    0    0    0    0    0    0    0    0    0     1     0     0     0
#> [11,]    0    0    0    0    0    0    0    0    0     0     1     0     0
#> [12,]    0    0    0    0    0    0    0    0    0     0     0     1     0
#> [13,]    0    0    0    0    0    0    0    0    0     0     0     0     1
#> [14,]    0    0    0    0    0    0    0    0    0     0     0     0     0
#> [15,]    0    0    0    0    0    0    0    0    0     0     0     0     0
#> [16,]    0    0    0    0    0    0    0    0    0     0     0     0     0
#> [17,]    0    0    0    0    0    0    0    0    0     0     0     0     0
#> [18,]    0    0    0    0    0    0    0    0    0     0     0     0     0
#>       [,14] [,15] [,16] [,17] [,18]
#>  [1,]     0     0     0     0     0
#>  [2,]     0     0     0     0     0
#>  [3,]     0     0     0     0     0
#>  [4,]     0     0     0     0     0
#>  [5,]     0     0     0     0     0
#>  [6,]     0     0     0     0     0
#>  [7,]     0     0     0     0     0
#>  [8,]     0     0     0     0     0
#>  [9,]     0     0     0     0     0
#> [10,]     0     0     0     0     0
#> [11,]     0     0     0     0     0
#> [12,]     0     0     0     0     0
#> [13,]     0     0     0     0     0
#> [14,]     1     0     0     0     0
#> [15,]     0     1     0     0     0
#> [16,]     0     0     1     0     0
#> [17,]     0     0     0     1     0
#> [18,]     0     0     0     0     1
#> 
#> $ineqx0
#> NULL
#> 
#> $nfuneval
#> [1] 23
#> 
#> $outer.iter
#> [1] 1
#> 
#> $elapsed
#> Time difference of 0.0083261 secs
#> 
#> $vscale
#>  [1] 1.01367 0.00001 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000
#> [10] 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000
#> [19] 1.00000 1.00000
#> 
#> $coefficients
#>                           glm                     polspline 
#>                      0.055571                      0.055557 
#>                        enet.5                         ridge 
#>                      0.055564                      0.055569 
#>                         lasso                     xgboost50 
#>                      0.055564                      0.055592 
#>       randomforest_screen_glm randomforest_screen_polspline 
#>                      0.055543                      0.055562 
#>    randomforest_screen_enet.5     randomforest_screen_ridge 
#>                      0.055543                      0.055543 
#>     randomforest_screen_lasso randomforest_screen_xgboost50 
#>                      0.055543                      0.055522 
#>              lasso_screen_glm        lasso_screen_polspline 
#>                      0.055561                      0.055559 
#>           lasso_screen_enet.5            lasso_screen_ridge 
#>                      0.055561                      0.055560 
#>            lasso_screen_lasso        lasso_screen_xgboost50 
#>                      0.055561                      0.055526 
#> 
#> $training_offset
#> [1] FALSE
#> 
#> $name
#> [1] "solnp"
#> 
#> [1] "Cross-validated risk (MSE, squared error loss):"
#>                           learner coefficients   risk       se  fold_sd
#>  1:                           glm     0.055571 1.0202 0.023955 0.067500
#>  2:                     polspline     0.055557 1.0208 0.023577 0.067921
#>  3:                        enet.5     0.055564 1.0132 0.023613 0.066333
#>  4:                         ridge     0.055569 1.0157 0.023743 0.065744
#>  5:                         lasso     0.055564 1.0133 0.023612 0.066098
#>  6:                     xgboost50     0.055592 1.1136 0.025262 0.077580
#>  7:       randomforest_screen_glm     0.055543 1.0296 0.024250 0.069857
#>  8: randomforest_screen_polspline     0.055562 1.0231 0.024113 0.071171
#>  9:    randomforest_screen_enet.5     0.055543 1.0294 0.024255 0.070076
#> 10:     randomforest_screen_ridge     0.055543 1.0296 0.024261 0.069907
#> 11:     randomforest_screen_lasso     0.055543 1.0294 0.024252 0.070063
#> 12: randomforest_screen_xgboost50     0.055522 1.1397 0.026152 0.092286
#> 13:              lasso_screen_glm     0.055561 1.0156 0.023529 0.064486
#> 14:        lasso_screen_polspline     0.055559 1.0182 0.023523 0.065457
#> 15:           lasso_screen_enet.5     0.055561 1.0158 0.023545 0.064384
#> 16:            lasso_screen_ridge     0.055560 1.0158 0.023548 0.064221
#> 17:            lasso_screen_lasso     0.055561 1.0157 0.023538 0.064450
#> 18:        lasso_screen_xgboost50     0.055526 1.1282 0.025915 0.087340
#> 19:                  SuperLearner           NA 1.0137 0.023645 0.068682
#>     fold_min_risk fold_max_risk
#>  1:       0.89442        1.1200
#>  2:       0.89892        1.1255
#>  3:       0.88839        1.1094
#>  4:       0.88603        1.1098
#>  5:       0.88930        1.1090
#>  6:       0.96019        1.2337
#>  7:       0.89942        1.1275
#>  8:       0.90131        1.1365
#>  9:       0.89760        1.1275
#> 10:       0.89818        1.1268
#> 11:       0.89773        1.1275
#> 12:       0.96391        1.2488
#> 13:       0.90339        1.1114
#> 14:       0.89742        1.1162
#> 15:       0.90318        1.1112
#> 16:       0.90282        1.1104
#> 17:       0.90318        1.1112
#> 18:       0.96040        1.2622
#> 19:       0.88550        1.1079
```
From the table of the printed Super Learner fit, we note that the Super Learner
had a mean risk of `sl_fit_summary$mean_risk[length(sl_fit_summary$learner)]`
and that this ensemble weighted the `ranger` and `glmnet` learners highest while
not weighting the `mean` learner highly.

We can also see that the `glmnet` learner had the lowest cross-validated mean
risk, thus making it the cross-validated selector (or the _discrete_ Super
Learner). The mean risk of the Super Learner is calculated using all of the 
data, and not a separate hold-out, so the Super Learner's mean risk that 
is reported here is an underestimation.

## Cross-validated Super Learner {-}

We can cross-validate the Super Learner to see how well the Super Learner
performs on unseen data, and obtain an estimate of the cross-validated risk of
the Super Learner.

This estimation procedure requires an "external" layer of cross-validation,
also called nested cross-validation, which involves setting aside a separate
holdout sample that we don’t use to fit the Super Learner. This
external cross validation procedure may also incorporate 10 folds, which is the
default in `sl3`. However, we will incorporate 2 outer/external folds of
cross-validation for computational efficiency.

We also need to specify a loss function to evaluate Super Learner.
Documentation for the available loss functions can be found in the [`sl3` Loss
Function Reference](https://tlverse.org/sl3/reference/loss_functions.html).

















