# Cross-validation {#origami}

_Ivana Malenica_

Based on the [`origami` `R` package](https://github.com/tlverse/origami)
by _Jeremy Coyle, Nima Hejazi, Ivana Malenica and Rachael Phillips_.

`r if (knitr::is_latex_output()) '\\begin{VT1}\n\\VH{Learning Objectives}'`

`r if (knitr::is_html_output()) '## Learning Objectives{-}'`

By the end of this chapter you will be able to:

1. Differentiate between training, validation and test sets.

2. Understand the concept of a loss function, risk and cross-validation.

3. Select a loss function that is appropriate for the functional parameter to be
   estimated.

4. Understand and contrast different cross-validation schemes for i.i.d. data.

5. Understand and contrast different cross-validation schemes for time dependent
   data.

6. Setup the proper fold structure, build custom fold-based function, and
   cross-validate the proposed function using the `origami` `R` package.

7. Setup the proper cross-validation structure for the use by the Super Learner
   using the the `origami` `R` package.

`r if (knitr::is_latex_output()) '\\end{VT1}'`

## Introduction

In this chapter, we elaborate on the estimation step outlined in the chapter on
the [_Roadmap for Targeted Learning_](#roadmap). In order to generate an initial
estimate of the target parameter, which is the focus of the following [chapter
on Super Learning](#sl3), we first need to translate, and incorporate, our
knowledge about the data-generating process into the estimation procedure, and
decide how to evaluate the quality of our estimation procedure's performance.

The performance, or error, of any algorithm used in the estimation procedure
directly relates to its generalizability to independent datasets arising from
the same data-generating distribution. The proper assessment of the performance
of proposed algorithms is extremely important -- it guides the choice of the
finally selected learning algorithm, and it provides a quantitative assessment
of how well the chosen algorithm performs. In order to assess the performance of
a learning algorithm, we introduce the concept of a **loss function**, which
defines the **risk** or the **expected prediction error**.  

`r if (knitr::is_latex_output()) '\\begin{shortbox}\n\\Boxhead{Constructing a library that is consistent with the data-generating distribution}'`
Our goal, as further specified in the next chapter, will be to estimate the true
risk of the proposed statistical learning method. Our goal(s) consist of:

1. Estimating the performance of different learning algorithms in order to
   choose the best one for the problem at hand.
2. Having chosen a winning algorithm, estimate the true risk of the proposed
   statistical learning method.
`r if (knitr::is_latex_output()) '\\end{shortbox}'`

In the following, we propose a method to do so using the observed data and
**cross-validation** procedures implemented in the `origami` package
[@coyle2018origami; @coyle-cran-origami].

## Background

Ideally, in a data-rich scenario (i.e., one with many observations), we would
split our dataset into three parts:

1. the training set,
2. the validation set, and
3. the test (or holdout) set.

The training set is used to fit algorithm(s) of interest; we evaluate the
performance of the fit(s) on a validation set, which can be used to estimate
prediction error (e.g., for tuning and model selection). The final error of the
chosen algorithm(s) is obtained by using the test (or holdout) set, which is
kept entirely separate such that the learning algorithms never encounter these
observations until the final model evaluation step.  One might wonder, with
training data readily available, why not use the training error to evaluate the
proposed algorithm's performance? Unfortunately, the training error is a poor
estimate (being optimistic) of the true risk; it consistently decreases with
model complexity, resulting in a possible overfitting to the training data and,
accordingly, low generalizability to similar datasets.

Since data are often scarce, separating a dataset into training, validation and
test sets can prove too limiting, on account of decreasing the available data
for use in training by too much. In the absence of a large dataset and a
designated test set, we must resort to methods that estimate the true risk by
efficient sample re-use. Re-sampling methods -- like the bootstrap -- involve
repeatedly sampling from the training set and fitting proposed learning
algorithms to these artificially created datasets. While often computationally
intensive, re-sampling methods are particularly useful for model selection and
estimation of the true risk. In addition, they might provide more insight on
variability and robustness of the fitted learning algorithm than is possible to
obtain by fitting a learning algorithm only once on all the training data.

### Introducing: cross-validation

In this chapter, we focus on **cross-validation** -- an essential tool for
evaluating how any given learning algorithm extends from a sample to the target
population from which the sample arises. Cross-validation has seen widespread
application in all facets of modern statistics, and perhaps most notably in
statistical machine learning. The cross-validation procedure can be used for
model selection, as well as for estimation of the true risk associated with any
statistical learning method in order to evaluate its performance. In particular,
cross-validation directly estimates the true risk when the estimate is applied
to an independent sample from the joint distribution of the predictors and
outcome. When used for model selection, cross-validation has powerful optimality
properties. The asymptotic optimality results state that the cross-validated
selector performs (in terms of risk) asymptotically as well as an optimal oracle
selector, a hypothetical procedure with free access to the true, unknown
data-generating distribution. For further details on the theoretical results, we
suggest consulting @vdl2003unified, @vdl2004asymptotic, @dudoit2005asymptotics
and @vaart2006oracle.

Cross-validation works by partitioning a sample into complementary subsets: a
training (sub)set, to which a particular learning algorithm is applied, and a
complementary validation (sub)set, used to evaluate the given algorithm's
learning performance. By repeating this general procedure across multiple
partitions of the dataset, the average risk (over the partitions of the data)
can be computed without allowing data to leak between training and validation
subsets. A variety of different partitioning schemes exist, each tailored to the
salient details of the problem of interest, including data size, prevalence of
the outcome, and dependence structure (between units). The `origami` package
provides a suite of tools that generalize the application of cross-validation to
arbitrary data analytic procedures. In the following, we describe different
types of cross-validation schemes readily available in `origami`, introduce the
general structure of the `origami` package, and demonstrate the use of these
procedures in applied settings.

---

## Estimation Roadmap: How does it all fit together?

Similarly to how we defined the [_Roadmap for Targeted Learning_](#roadmap), we
can define the **Estimation Roadmap** as a guide for the estimation process. In
particular, the unified loss-based estimation framework [@vdl2003unified;
@vdl2004asymptotic; @dudoit2005asymptotics; @vaart2006oracle; @vdl2007super],
which relies upon cross-validation methodology for estimator construction,
selection, and performance assessment, follows three main steps:

1. **The loss function**:
Define the target parameter as the minimizer of the expected loss (risk) for a
complete data loss function chosen to represent the desired performance measure.
Map the complete data loss function into an observed data loss function, having
the same expected value and leading to an efficient estimator of risk.

2. **The learning algorithms**:
Construct a finite collection of candidate estimators for the parameter of
interest.

3. **The cross-validation scheme**:
Apply appropriate cross-validation to select an optimal estimator among the
candidates and assess the overall performance of the resulting estimator.

Step 1 of the [Estimation Roadmap](#roadmap) allows us to unify a broad range of
problems that are traditionally treated separately in the statistical
literature, including both density estimation and the prediction of
polychotomous and/or continuous outcomes. For example, when we are interested in
estimating the full joint conditional density, we may use the negative
log-likelihood loss to select an asymptotically optimal density estimation
algorithm. If instead we are interested in the conditional mean of a continuous
outcome, we may use the squared error loss to select an asymptotically optimal
algorithm. On the other hand, had the outcome been binary, we would instead
resort to using the indicator (0-1) loss. The unified loss-based estimation
framework also reconciles censored and complete data estimation methods, by
generalizing any loss-based learning algorithm for complete data structures into
loss-based learning for corresponding censored data structures.

## Example: Cross-validation and Prediction

Having introduced the [Estimation Roadmap](#roadmap), we can more precisely
define our objective (requiring more mathematical notation), using prediction as
an example. Let the observed data be defined as $O = (W, Y)$, where a unit
specific data structure can be written as $O_i = (W_i, Y_i)$, for $i = 1,
\ldots, n$. For each of the $n$ sampled units, we denote $Y_i$ as the outcome of
interest (polychotomous or continuous), and $W_i$ as a $p$-dimensional set of
covariates. Let $\psi_0(W)$ denote the target parameter of interest (what we
wish to estimate); for this example, we are interested in estimating the
conditional expectation of the outcome given the covariates, $\psi_0(W) = \E(Y
\mid W)$. Following the [Estimation Roadmap](#roadmap), we choose the
appropriate loss function, $L$, such that $\psi_0(W) = \text{argmin}_{\psi}
\E_0[L(O, \psi(W))]$. But, how do we know how well each of the candidate
estimators of $\psi$ is doing? In order to pick the optimal estimator among the
candidates, and assess the overall performance of the resultant estimator, we
use cross-validation -- dividing the available data into the training set and
validation set.  Observations in the training set are used to fit (or train) the
estimator, while those in validation set are used to assess the risk of (or
validate) it.

Next, we introduce notation flexible enough to represent any cross-validation
scheme. Define a **split vector**, $B_n = (B_n(i): i = 1, \ldots, n) \in
\{0,1\}^n$ and Note that such a split vector is independent of the empirical
distribution $P_n$. A realization of $B_n$ defines a random split of the data
into training and validation subsets such that if

$$B_n(i) = 0, \ \ \text{i sample is in the training set}$$
$$B_n(i) = 1, \ \ \text{i sample is in the validation set.}$$
We can further define $P_{n, B_n}^0$ and $P_{n, B_n}^1$ as the empirical
distributions of the training and validation subsets, respectively. Then, $n_0 =
\sum_i (1 - B_n(i))$ and $n_1 = \sum_i B_n(i)$ denote the number of samples in
the training and validation subsets, respectively. The particular distribution
of the split vector $B_n$ defines the type of cross-validation scheme, tailored
to the problem and dataset at hand.

## Cross-validation schemes in `origami`

Recall that the particular distribution of the split vector $B_n$ defines the
choice of cross-validation scheme. We next describe the different
cross-validation schemes available in the `origami` package, and we go on to
demonstrate their use in practical data analysis examples.

### WASH Benefits Study Example {-}

In order to illustrate different cross-validation schemes, we will be using the
WASH study data. Detailed information on the WASH Benefits example dataset can
be found in [Chapter 3](#wash). In particular, we are interested in predicting
weight-for-height Z-score (`whz`) using the available covariate data. For this
illustration, we will start by treating the data as independent and identically
distributed (i.i.d.) random draws from an unknown distribution $P_0 \in \M$. To
see what each cross-validation scheme is doing, we will subset the data to only
$n=30$. Note that each row represents an i.i.d. sampled unit, indexed by the row
number.

```{r setup}
library(data.table)
library(origami)
library(knitr)
library(kableExtra)

# load data set and take a peek
washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)
```

```{r origami_washb_example_table1, echo=FALSE}
n_samp <- 30
washb_data <- washb_data[seq_len(n_samp), ]
if (knitr::is_latex_output()) {
  head(washb_data) %>%
    kable(format = "latex")
} else if (knitr::is_html_output()) {
  head(washb_data) %>%
    kable() %>%
    kable_styling(fixed_thead = TRUE) %>%
    scroll_box(width = "100%", height = "300px")
}
```

Above is a look at the first `r n_samp` of the data.

### Cross-validation for i.i.d. data

#### Re-substitution

The re-substitution method is perhaps the simplest strategy for estimating the
risk associated with fitting a proposed algorithm on a set of observations. With
this cross-validation scheme, all observed data units are used in both the
training and validation set.

We illustrate the usage of the re-substitution method with `origami` below,
using the function `folds_resubstitution(n)`. In order to set up
`folds_resubstitution(n)`, we need only to specify the total number of sampled
units that we want to allocate to the training and validation sets; remember
that each row of the dataset is a unique i.i.d. sampled unit. Also, notice the
structure of the `origami` output:

1. **v:** the cross-validation fold
2. **training_set:** the indices of the samples in the training set
2. **validation_set:** the indices of the samples in the training set.

The structure of the `origami` output, a `list` of fold(s), holds across all of
the cross-validation schemes presented in this chapter. Below, we show the fold
generated by the re-substitution method:

```{r resubstitution}
folds_resubstitution(nrow(washb_data))
```

#### Holdout

The holdout method, or the validation set approach, consists of randomly
dividing the available data into training and validation (holdout) sets.  The
model is then fitted (i.e., "trained") using the observations in the training
set and subsequently evaluated (i.e., "validated") using the observations in the
validation set. Typically, the dataset is split into $60:40$, $70:30$, $80:20$
or even $90:10$ training-to-validation splits.

The holdout method is intuitive and computationally inexpensive; however, it
does carry a disadvantage: If we were to repeat the process of randomly
splitting the data into training and validation sets, we could get very
different cross-validated estimates of the empirical risk. In particular, the
empirical mean of the loss function (i.e., the empirical risk) evaluated over
the validation set(s) could be highly variable, depending on which samples were
included in the training and validation splits. Overall, the cross-validated
empirical risk for the holdout method is more variable, since in includes
variability of the random split as well -- this is not desirable. For
classification problems (with a binary or categorical outcome variable), there
is an additional disadvantage: It is possible for the training and validation
sets to end up with uneven distributions of the two (or more) outcome classes,
leading to better training and poor validation, or vice-versa, though this may
be corrected by incorporating stratification into the cross-validation process.
Finally, note that we are not using all of the data in training or in evaluating
the performance of the proposed algorithm, which could itself introduce bias.

#### Leave-one-out

The leave-one-out cross-validation scheme is closely related to the holdout
method, as it also involves splitting the dataset into training and validation
sets; however, instead of partitioning the dataset into sets of similar size, a
single observation is used as the validation set. In doing so, the vast majority
of the sampled units are employed for fitting (or training) the candidate
learning algorithm. Since only a single sampled unit (for example $O_1 = (W_1,
Y_1)$) is left out of the fitting process, leave-one-out cross-validation can
result in a (possibly) less biased estimate of the risk. Typically, the
leave-one-out approach will not overestimate the risk as much as the holdout
method does. On the other hand, since the estimate of risk is based on a single
sampled unit, it is usually a highly variable estimate.

We can repeat the process of spiting the dataset into training and validation
sets until all of the sampled units have had a chance to act as the validation
set. Continuing the example above, a subsequent iteration of the leave-one-out
cross-validation scheme may use $O_2 = (W_2, Y_2)$ as the validation set (where,
before, $O_1 = (W_1, Y_1)$ played that role) while the remaining $n-1$ sampled
units are included in the training set.  Repeating this approach $n$ times
results in $n$ risk estimates, for example, $MSE_1, MSE_2, \ldots, MSE_n$ (note
that these are the mean squared error (MSE) estimates when unit $i$ is
validation set). The estimate of the true risk is then the average over the $n$
leave-one-out risk estimates. While the leave-one-out cross-validation scheme
results in a less biased (albeit, more variable) estimate of risk than the
holdout method, it can be computationally very expensive to implement when $n$
is large.

We illustrate the usage of the leave-one-out cross-validation scheme with
`origami` below, using the `folds_loo(n)` function. In order to set up
`folds_loo(n)`, similarly to the case of the re-substitution method, we need
only the total number of sampled units over which the cross-validation procedure
is to operate. We show the first two folds generated by leave-one-out
cross-validation below.

```{r loo}
folds <- folds_loo(nrow(washb_data))
folds[[1]]
folds[[2]]
```

#### $V$-fold

An alternative to the leave-one-out scheme is $V$-fold cross-validation. This
cross-validation scheme randomly divides the dataset into $v$ sets (folds) of
equal (or approximately equal) size, where, for each fold, the number of sampled
units in the corresponding validation set are the same.  For $V$-fold
cross-validation, a single fold is treated as the validation set while the
candidate learning algorithm is trained by combining the sampled units across
the remaining $v-1$ folds (i.e., the training set). The risk, for example the
MSE, is computed using only the sampled units in the given validation set. With
the candidate learning algorithm trained and its performance evaluated in a
single fold, the process is repeated $v$ times, and, each time, a different fold
is taken to be the validation set. This is repeated until each of the $v$ folds
has had a chance to act as the validation set; this corresponds with each of the
sampled units having a chance to be included in one of the validation sets.  Of
course, this means that with $V$-fold cross-validation, all of the sampled units
are used in the training and validation stages, preventing the candidate
learning algorithm from overfitting to only a subset of the data (e.g., a given
training set). Once completed, $V$-fold cross-validation results in $v$
estimates of the validation error (or risk). The "final" estimate of the risk is
the average over these $v$ risk estimates

For a dataset with $n$ sampled units, $V$-fold cross-validation with $v=n$
merely reduces to leave-one-out; similarly, if we set $n=1$, we can get the
holdout method's estimate of the candidate learning algorithm's performance.
Beyond its computational advantages, $V$-fold cross-validation often yields more
accurate estimates of the true, underlying risk. This is rooted in the differing
bias-variance trade-offs associated with these two cross-validation schemes:
While the leave-one-out scheme may be less biased, it has much greater variance
(since only a single unit is included in the validation set). This difference
becomes more obvious as $n$ becomes much greater than $v$. With the $V$-fold
cross-validation scheme, we end up averaging risk estimates across the $v$
validation folds, which are typically less correlated than the risk estimates
from the leave-one-out fits. Owing to the fact that the mean of many highly
correlated quantities has higher variance, leave-one-out estimates of the risk
will have higher variance than the corresponding estimates based on $V$-fold
cross-validation.

Now, let's see $V$-fold cross-validation with `origami` in action! In the next
chapter, we will turn to studying the Super Learner algorithm --- an algorithm
capable of selecting a "best" algorithm from among a large library of candidate
learning algorithms -- which we'd like to fit _and_ evaluate the performance of.
The Super Learner algorithm relies on $V$-fold cross-validation as its default
cross-validation scheme. In order to set up $V$-fold cross-validation, we need
to call `origami`'s `folds_vfold(n, V)` function. The two required arguments for
`folds_vfold(n, V)` are the total number of sample units to be cross-validated
and the number of folds we wish to have.

For example, at $V=2$, we will get two folds, each with approximately $n/2$
sampled units in the training and validation sets.

```{r cv}
folds <- folds_vfold(nrow(washb_data), V = 2)
folds[[1]]
folds[[2]]
```

#### Monte Carlo

In the Monte Carlo cross-validation scheme, we randomly select some fraction of
the data, _without replacement_, to form the training set, assigning the
remainder of the sampled units to the validation set.  In this way, the dataset
is randomly divided into two independent splits: A training set of $n_0 = n
\cdot (1 - p)$ observations and a validation set of $n_1 = n \cdot p$
observations. By repeating this procedure many times, the Monte Carlo
cross-validation scheme generates -- at random -- many training and validation
partitions of the dataset.

Since the partitions are independent across folds, the same observational unit
can appear in the validation set multiple times; note that this is a stark
difference between the Monte Carlo and $V$-fold cross-validation schemes. For a
given sampling fraction $p$, the Monte Carlo cross-validation scheme would be
optimal if repeated infinitely many times -- of course, this is not
computationally feasible. With Monte Carlo cross-validation, it is possible to
explore many more partitions of the dataset than with $V$-fold cross-validation,
resulting in (possibly) less variable estimates of the risk (across partitions),
though this comes at the cost of an increase in bias (because the splits are
correlated).  Because Monte Carlo cross-validation generates many splits with
overlaps in the sampled units, more splits (and thus more computational time)
will be necessary to achieve the level of performance (in terms of unbiasedness)
that the $V$-fold cross-validation scheme achieves with only $V$ splits.

We illustrate the usage of the Monte Carlo cross-validation scheme with
`origami` below, using the `folds_montecarlo(n, V, pvalidation)` function. In
order to set up `folds_montecarlo(n, V, pvalidation)`, we need the following,

1. the total number of observations we wish to cross-validate;
2. the number of folds; and
3. the proportion of observations to be placed in the validation set.

For example, setting $V=2$ and $pvalidation = 0.2$, we obtain two folds, each
with approximately $6$ sampled units in the validation set for each fold.

```{r montecarlo}
folds <- folds_montecarlo(nrow(washb_data), V = 2, pvalidation = 0.2)
folds[[1]]
folds[[2]]
```

#### Bootstrap

Like the Monte Carlo cross-validation scheme, the bootstrap cross-validation
scheme also consists of randomly selecting sampled units, _with replacement_,
for the training set; the rest of the sampled units are allocated to the
validation set. This process is then repeated multiple times, generating (at
random) new training and validation partitions of the dataset each time. In
contract to the Monte Carlo cross-validation scheme, the total number of sampled
units in training and validation sets (i.e., the sizes of the two partitions)
across folds is not held constant Also, as the name suggests, sampling is
performed with replacement (as in the bootstrap [@davison1997bootstrap]), hence
the exact same observational units may be included in multiple training sets.
The proportion of observational units in the validation sets is a random
variable, with expectation $\sim 0.368$.
<!--
nh: I don't follow the last bit about the proportion coming out of nowhere
-->

We illustrate the usage of the bootstrap cross-validation scheme with `origami`
below, using the `folds_bootstrap(n, V)` function. In order to set up
`folds_bootstrap(n, V)`, we need to specify the following arguments:

1. the total number of observations we wish to cross-validate;
2. the number of folds.

For example, setting $V=2$, we obtain two folds, each with different numbers of
sampled units in the validation sets across the folds.

```{r bootstrap}
folds <- folds_bootstrap(nrow(washb_data), V = 2)
folds[[1]]
folds[[2]]
```

### Cross-validation for Dependent Data

The `origami` package also supports numerous cross-validation schemes for
dependent (e.g., time-series) data, for both single and multiple time-series
with arbitrary time and network dependence.

### `AirPassenger` Data Example {-}

In order to illustrate different cross-validation schemes for time-series, we
will be using the _AirPassenger_ data; this is a widely used, freely available
dataset. The _AirPassenger_ dataset, included in `R`, provides monthly totals of
international airline passengers between the years 1949 and 1960. This dataset
is already of a time series class; therefore, no further class or date
manipulation is required -- how convenient!

`r if (knitr::is_latex_output()) '\\begin{shortbox}\n\\Boxhead{Constructing a library that is consistent with the data-generating distribution}'`
**Goal:** we want to forecast the number of airline passengers at time $h$
horizon using the historical data from 1949 to 1960.
`r if (knitr::is_latex_output()) '\\end{shortbox}'`

```{r plot_airpass}
library(ggfortify)

data(AirPassengers)
AP <- AirPassengers

autoplot(AP) +
  labs(
    x = "Date",
    y = "Passenger numbers (1000's)",
    title = "Air Passengers from 1949 to 1961"
  )

t <- length(AP)
```

#### Rolling origin

The rolling origin cross-validation scheme lends itself to "online" learning
algorithms, in which large streams of data have to be fit continually
(respecting time), where the fit of the learning algorithm is (constantly)
updated as more data accrues. In general, the rolling origin scheme defines an
initial training set, and, with each iteration, the size of the training set
grows by $m$ observations, until time $t$ is reached for a particular fold. The
time points included in the training set always lag behind behind those in the
validation set. In addition, there might be a gap between training and
validation times of size $h$ (a lag window).

To further illustrate rolling origin cross-validation, we show below an example
with 3 folds. Here, the first window size is 15 time points, on which we first
train the proposed algorithm. We then evaluate its performance on 10 time
points, with a gap ($h$) of size 5 between the training and validation time
points.

For the following fold, we train the algorithm on a longer stream of data, 25
time points, including the original 15 we started with. We then evaluate its
performance on 10 time points in the future.

```{r, fig.cap="Rolling origin CV", results="asis", echo=FALSE}
knitr::include_graphics(path = "img/png/rolling_origin.png")
```

We illustrate the usage of the rolling origin cross-validation with `origami`
package below using the function `folds_rolling_origin(n, first_window,
validation_size, gap, batch)`. In order to setup `folds_rolling_origin(n,
first_window, validation_size, gap, batch)`, we need:

1. the total number of time points we want to cross-validate;
2. the size of the first training set;
3. the size of the validation set;
4. the gap between training and validation set;
5. the size of the update on the training set per each iteration of CV.

Our time-series has $t=144$ time points. Setting the `first_window` to $50$,
`validation_size` to 10, `gap` to 5 and `batch` to 20, we get 4 time-series
folds; we show the first two below.

```{r rolling_origin}
folds <- folds_rolling_origin(
  t,
  first_window = 50, validation_size = 10, gap = 5, batch = 20
)
folds[[1]]
folds[[2]]
```

#### Rolling window

Instead of adding more time points to the training set per each iteration, the
rolling window cross-validation scheme "rolls" the training sample forward by
$m$ time units. The rolling window scheme might be considered in parametric
settings when one wishes to guard against moment or parameter drift that is
difficult to model explicitly; it is also more efficient for computationally
demanding settings such as streaming data, in which large amounts of training
data cannot be stored. In contrast to rolling origin CV, the training sample for
each iteration of the rolling window scheme is always the same.

To illustrate the rolling window cross-validation with 3 time-series folds
below. The first window size is 15 time points, on which we first train the
proposed algorithm. As in the previous illustration, we evaluate its performance
on 10 time points, with a gap of size 5 between the training and validation time
points. However, for the next fold, we train the algorithm on time points
further away from the origin (here, 10 time points). Note that the size of the
training set in the new fold is the same as in the first fold (15 time points).
This setup keeps the training sets comparable over time (and fold) as compared
to the rolling origin CV. We then evaluate the performance of the proposed
algorithm on 10 time points in the future.

```{r, fig.cap="Rolling window CV", results="asis", echo=FALSE}
knitr::include_graphics(path = "img/png/rolling_window.png")
```

We illustrate the usage of the rolling window cross-validation with `origami`
package below using the function `folds_rolling_window(n, window_size,
validation_size, gap, batch)`. In order to setup `folds_rolling_window(n,
window_size, validation_size, gap, batch)`, we need:

1. the total number of time points we want to cross-validate;
2. the size of the training sets;
3. the size of the validation set;
4. the gap between training and validation set;
5. the size of the update on the training set per each iteration of CV.

Setting the `window_size` to $50$, `validation_size` to 10, `gap` to 5 and
`batch` to 20, we also get 4 time-series folds; we show the first two below.

```{r rolling_window}
folds <- folds_rolling_window(
  t,
  window_size = 50, validation_size = 10, gap = 5, batch = 20
)
folds[[1]]
folds[[2]]
```

#### Rolling origin with V-fold

A variant of rolling origin scheme which accounts for sample dependence is the
rolling-origin-$V$-fold cross-validation. In contrast to the canonical rolling
origin CV, samples in the training and validation set are not the same, as the
variant encompasses $V$-fold CV in addition to the time-series setup. The
predictions are evaluated on the future times of time-series units not seen
during the training step, allowing for dependence in both samples and time. One
can use the rolling-origin-$v$-fold cross-validation with `origami` package
using the function `folds_vfold_rolling_origin_pooled(n, t, id, time, V,
first_window, validation_size, gap, batch)`. In the figure below, we show $V=2$
$V$-folds, and 2 time-series CV folds.

```{r, fig.cap="Rolling origin V-fold CV", results="asis", echo=FALSE}
knitr::include_graphics(path = "img/png/rolling_origin_v_fold.png")
```

#### Rolling window with v-fold

Analogous to the previous section, we can extend rolling window CV to support
multiple time-series with arbitrary sample dependence. One can use the
rolling-window-$V$-fold cross-validation with `origami` package using the
function `folds_vfold_rolling_window_pooled(n, t, id, time, V, window_size,
validation_size, gap, batch)`. In the figure below, we show $V=2$ $V$-folds, and
2 time-series CV folds.

```{r, fig.cap="Rolling window V-fold CV", results="asis", echo=FALSE}
knitr::include_graphics(path = "img/png/rolling_window_v_fold.png")
```

## General workflow of `origami`

Before we dive into more details, let's take a moment to review some of the
basic functionality in `origami` R package. The main function in the `origami`
is `cross_validate`. To start off, the user must define the fold structure and a
function that operates on each fold. Once these are passed to `cross_validate`,
`cross_validate` will apply the same specified function to each fold, and
combine the fold-specific results in a meaningful way. We will see this in
action in later sections; for now, we provide specific details on each each step
of this process below.

### (1) Define folds

The `folds` object passed to `cross_validate` is a list of folds; such lists can
be generated using the `make_folds` function. Each fold consists of a list with
a `training` index vector, a `validation` index vector, and a `fold_index` (its
order in the list of folds). This function supports a variety of
cross-validation schemes we describe in the following section. The `make_folds`
can balance across levels of a variable (`strata_ids`), and it can also keep
all observations from the same independent unit together (`cluster`).

### (2) Define fold function

The `cv_fun` argument to `cross_validate` is a function that will perform some
operation on each fold. The first argument to this function must be `fold`,
which will receive an individual fold object to operate on. Additional arguments
can be passed to `cv_fun` using the `...` argument to `cross_validate`. Within
this function, the convenience functions `training`, `validation` and
`fold_index` can return the various components of a fold object. If `training`
or `validation` is passed an object, it will index it in a sensible way.
For instance, if it is a vector, it will index the vector directly; if it is a
`data.frame` or `matrix`, it will index rows. This allows the user to easily
partition data into training and validation sets. The fold function must return
a named list of results containing whatever fold-specific outputs are generated.

### (3) Apply `cross_validate`

After defining folds, `cross_validate` can be used to map the `cv_fun` across
the `folds` using `future_lapply`. This means that it can be easily parallelized
by specifying a parallelization scheme (i.e., a `plan` from the [future
parallelization framework for `R`](https://Cran.R-project.org/package=future)
[@bengtsson2021unifying]). The application of `cross_validate` generates a list
of results. As described above, each call to `cv_fun` itself returns a list of
results, with different elements for each type of result we care about. The main
loop generates a list of these individual lists of results (a sort of
"meta-list"). This "meta-list" is then inverted such that there is one element
per result type (this too is a list of the results for each fold). By default,
`combine_results` is used to combine these results type lists in a sensible
manner. How results are combined is determined automatically by examining the
data types of the results from the first fold. This can be modified by
specifying a list of arguments to `.combine_control`.

## Cross-validation in action

Let's see `origami` in action! In the following chapter we will learn how to use
cross-validation with the Super Learner, and how we can utilize the power of
cross-validation to build optimal ensembles of algorithms, not just its use on a
single statistical learning method.

### Cross-validation with linear regression

First, we will load the relevant `R` packages, set a seed, and load the full
WASH data once again. In order to illustrate cross-validation with `origami` and
linear regression, we will focus on predicting the weight-for-height Z-score
`whz` using all of the available covariate data. As stated previously, we will
assume the data is independent and identically distributed, ignoring the cluster
structure imposed by the clinical trial design. For the sake of illustration, we
will work with a subset of data, and remove all samples with missing data from
the dataset; we will learn in the next chapter how to deal with missingness.

```{r setup_ex}
library(stringr)
library(dplyr)
library(tidyr)

# load data set and take a peek
washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)

# Remove missing data, then pick just the first 500 rows
washb_data <- washb_data %>%
  drop_na() %>%
  slice(1:500)

outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]
```

Here's a look at the data:

```{r origami_washb_example_table2, echo=FALSE}
if (knitr::is_latex_output()) {
  head(washb_data) %>%
    kable(format = "latex")
} else if (knitr::is_html_output()) {
  head(washb_data) %>%
    kable() %>%
    kable_styling(fixed_thead = TRUE) %>%
    scroll_box(width = "100%", height = "300px")
}
```

We can see the covariates used in the prediction:

```{r covariates}
outcome
covars
```

Next, we fit a linear model on the complete data, with the goal of predicting the
weight-for-height Z-score `whz` using all of the available covariate data. Let's
try it out:

```{r linear_mod}
lm_mod <- lm(whz ~ ., data = washb_data)
summary(lm_mod)
```

We can assess how well the model fits the data by comparing the predictions of
the linear model to the true outcomes observed in the data set. This is the well
known (and standard) mean squared error. We can extract that from the `lm` model
object as follows:

```{r get_naive_error}
(err <- mean(resid(lm_mod)^2))
```

The mean squared error is `r err`. There is an important problem that arises
when we assess the model in this way - that is, we have trained our linear
regression model on the complete data set and assessed the error on the complete data
set, using up all of our data. We, of course, are generally not interested in
how well the model explains variation in the observed data; rather, we are
interested in how the explanation provided by the model generalizes to a target
population from which the sample is presumably derived. Having used all of our
available data, we cannot honestly evaluate how well the model fits (and thus
explains) variation at the population level.

To resolve this issue, cross-validation allows for a particular procedure (e.g.,
linear regression) to be implemented over subsets of the data, evaluating how
well the procedure fits on a testing ("validation") set, thereby providing an
honest evaluation of the error.

We can easily add cross-validation to our linear regression procedure using
`origami`. First, let us define a new function to perform linear regression on a
specific partition of the data (called a "fold"):

```{r define_fun_cv_lm}
cv_lm <- function(fold, data, reg_form) {
  # get name and index of outcome variable from regression formula
  out_var <- as.character(unlist(str_split(reg_form, " "))[1])
  out_var_ind <- as.numeric(which(colnames(data) == out_var))

  # split up data into training and validation sets
  train_data <- training(data)
  valid_data <- validation(data)

  # fit linear model on training set and predict on validation set
  mod <- lm(as.formula(reg_form), data = train_data)
  preds <- predict(mod, newdata = valid_data)
  valid_data <- as.data.frame(valid_data)

  # capture results to be returned as output
  out <- list(
    coef = data.frame(t(coef(mod))),
    SE = (preds - valid_data[, out_var_ind])^2
  )
  return(out)
}
```

Our `cv_lm` function is rather simple: we merely split the available data into a
training and validation sets (using the eponymous functions provided in
`origami`) fit the linear model on the training set, and evaluate the model on
the validation set. This is a simple example of what `origami` considers to be
`cv_fun` --- functions for using cross-validation to perform a particular routine
over an input data set. Having defined such a function, we can simply generate a
set of partitions using `origami`'s `make_folds` function, and apply our `cv_lm`
function over the resultant `folds` object. Below, we replicate the
re-substitution estimate of the error -- we did this "by hand" above -- using
the functions `make_folds` and `cv_lm`.

```{r cv_lm_resub}
# re-substitution estimate
resub <- make_folds(washb_data, fold_fun = folds_resubstitution)[[1]]
resub_results <- cv_lm(fold = resub, data = washb_data, reg_form = "whz ~ .")
mean(resub_results$SE, na.rm = TRUE)
```

This (nearly) matches the estimate of the error that we obtained above.

We can more honestly evaluate the error by V-fold cross-validation, which
partitions the data into $v$ subsets, fitting the model on $v - 1$ of the
subsets and evaluating on the subset that was held out for testing. This is
repeated such that each subset is used for validation. We can easily apply our
`cv_lm` function using `origami`'s `cross_validate` (n.b., by default this
performs 10-fold cross-validation):

```{r cv_lm_cross_valdate}
# cross-validated estimate
folds <- make_folds(washb_data)
cvlm_results <- cross_validate(
  cv_fun = cv_lm, folds = folds, data = washb_data, reg_form = "whz ~ .",
  use_future = FALSE
)
mean(cvlm_results$SE, na.rm = TRUE)
```

Having performed 10-fold cross-validation, we quickly notice that our previous
estimate of the model error (by resubstitution) was a bit optimistic. The honest
estimate of the error is larger!

### Cross-validation with random forests

To examine `origami` further, let us return to our example analysis using the
WASH data set. Here, we will write a new `cv_fun` type object. As an example, we
will use Breiman's `randomForest` [@breiman2001random]:

```{r cv_fun_randomForest}
# make sure to load the package!
library(randomForest)

cv_rf <- function(fold, data, reg_form) {
  # get name and index of outcome variable from regression formula
  out_var <- as.character(unlist(str_split(reg_form, " "))[1])
  out_var_ind <- as.numeric(which(colnames(data) == out_var))

  # define training and validation sets based on input object of class "folds"
  train_data <- training(data)
  valid_data <- validation(data)

  # fit Random Forest regression on training set and predict on holdout set
  mod <- randomForest(formula = as.formula(reg_form), data = train_data)
  preds <- predict(mod, newdata = valid_data)
  valid_data <- as.data.frame(valid_data)

  # define output object to be returned as list (for flexibility)
  out <- list(
    coef = data.frame(mod$coefs),
    SE = ((preds - valid_data[, out_var_ind])^2)
  )
  return(out)
}
```

Above, in writing our `cv_rf` function to cross-validate `randomForest`, we used
our previous function `cv_lm` as an example. For now, individual `cv_fun` must
be written by hand; however, in future releases, a wrapper may be available to
support auto-generating `cv_fun`s to be used with `origami`.

Below, we use `cross_validate` to apply our new `cv_rf` function over the `folds`
object generated by `make_folds`.

```{r cv_fun_randomForest_run}
# now, let's cross-validate...
folds <- make_folds(washb_data)
cvrf_results <- cross_validate(
  cv_fun = cv_rf, folds = folds, 
  data = washb_data, reg_form = "whz ~ .",
  use_future = FALSE
)
mean(cvrf_results$SE)
```

Using 10-fold cross-validation (the default), we obtain an honest estimate of
the prediction error of random forests. From this, we gather that the use of
`origami`'s `cross_validate` procedure can be generalized to arbitrary estimation
techniques, given availability of an appropriate `cv_fun` function.

### Cross-validation with arima

Cross-validation can also be used for forecast model selection in a time series
setting. Here, the partitioning scheme mirrors the application of the
forecasting model: we'll train the data on past observations (either all
available or a recent subset), and then use the model fit to predict the next
few observations. We consider the `AirPassengers` dataset again, a monthly time
series of passenger air traffic in thousands of people.

```{r load_airpass}
data(AirPassengers)
print(AirPassengers)
```

Suppose we want to pick between two forecasting models with different `arima`
configurations. We can do that by evaluating their forecasting performance.
First, we set up the appropriate cross-validation scheme for time-series.

```{r folds_airpass}
folds <- make_folds(AirPassengers,
  fold_fun = folds_rolling_origin,
  first_window = 36, validation_size = 24, batch = 10
)

# How many folds where generated?
length(folds)

# Examine the first 2 folds.
folds[[1]]
folds[[2]]
```

By default, `folds_rolling_origin` will increase the size of the training set by
one time point each fold. Had we followed the default option, we would have 85
folds to train! Luckily, we can pass the `batch` as option to
`folds_rolling_origin` that tells it to increase the size of the training set by
10 points each iteration.  Since we want to forecast the immediate next point,
`gap` argument remains the default (0).

```{r fit_airpass}
# make sure to load the package!
library(forecast)

# function to calculate cross-validated squared error
cv_forecasts <- function(fold, data) {
  # Get training and validation data
  train_data <- training(data)
  valid_data <- validation(data)
  valid_size <- length(valid_data)

  train_ts <- ts(log10(train_data), frequency = 12)

  # First arima model
  arima_fit <- arima(train_ts, c(0, 1, 1),
    seasonal = list(
      order = c(0, 1, 1),
      period = 12
    )
  )
  raw_arima_pred <- predict(arima_fit, n.ahead = valid_size)
  arima_pred <- 10^raw_arima_pred$pred
  arima_MSE <- mean((arima_pred - valid_data)^2)

  # Second arima model
  arima_fit2 <- arima(train_ts, c(5, 1, 1),
    seasonal = list(
      order = c(0, 1, 1),
      period = 12
    )
  )
  raw_arima_pred2 <- predict(arima_fit2, n.ahead = valid_size)
  arima_pred2 <- 10^raw_arima_pred2$pred
  arima_MSE2 <- mean((arima_pred2 - valid_data)^2)

  out <- list(mse = data.frame(
    fold = fold_index(),
    arima = arima_MSE, arima2 = arima_MSE2
  ))
  return(out)
}

mses <- cross_validate(
  cv_fun = cv_forecasts, folds = folds, data = AirPassengers,
  use_future = FALSE
)
mses$mse
colMeans(mses$mse[, c("arima", "arima2")])
```

The arima model with no AR component seems to be a better fit for this data.

## Exercises

### Review of Key Concepts

1. Compare and contrast V-fold cross-validation with resubstitution
   cross-validation. What are some of the differences between the two methods?
   How are they similar? Describe a scenario when you would use one over the
   other.

2. What are the advantages and disadvantages of $v$-fold CV relative to:
   a. holdout CV?
   b. leave-one-out CV?

3. Why can't we use V-fold cross-validation for time-series data?

4. Would you use rolling window or origin for non-stationary time-series? Why?

### The Ideas in Action

1. Let $Y$ be a binary variable with $P(Y=1 \mid W) = 0.01$. What kind of
   cross-validation scheme should be used for a rare outcome? How can we do this
   with the `origami` package?

2. Consider the WASH benefits dataset presented in this chapter. How can we
   include cluster information into cross-validation? How can we do this with
   the `origami` package?

### Advanced Topics

1. Think about a dataset with arbitrary spatial dependence, where we know
   the extent of dependence, and groups formed by such dependence are clear
   with no spillover effects. What kind of cross-validation can we use?

2. Continuing on the last problem, what kind of procedure, and cross-validation
   method, can we use if the spatial dependence is not clearly defined as in the
   previous problem?

3. Consider a classification problem with a large number of predictors. A
   statistician proposes the following analysis:

   a. First screen the predictors, leaving only covariates with a strong
      correlation with the class labels.
   b. Fit some algorithm using only the subset of highly correlated covariates.
   c. Use cross-validation to estimate the tuning parameters and the performance
      of the proposed algorithm.

   Is this a correct application of cross-validation? Why?

<!--
## Appendix

### Exercise solutions
-->
