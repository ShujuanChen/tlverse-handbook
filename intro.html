<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 The Roadmap for Targeted Learning | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown  and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 The Roadmap for Targeted Learning | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/tlverse-handbook/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/tlverse-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 The Roadmap for Targeted Learning | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard, Mark van der Laan" />


<meta name="date" content="2019-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="motivation.html">
<link rel="next" href="tlverse.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-is-not"><i class="fa fa-check"></i>What this book is not</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#jeremy-coyle"><i class="fa fa-check"></i>Jeremy Coyle</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nima-hejazi"><i class="fa fa-check"></i>Nima Hejazi</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i>Ivana Malenica</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rachael-phillips"><i class="fa fa-check"></i>Rachael Phillips</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alan-hubbard"><i class="fa fa-check"></i>Alan Hubbard</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mark-van-der-laan"><i class="fa fa-check"></i>Mark van der Laan</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> The Roadmap for Targeted Learning</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-roadmap"><i class="fa fa-check"></i><b>1.3</b> The Roadmap</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#step1"><i class="fa fa-check"></i><b>1.3.1</b> (1) Data as a random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#step2"><i class="fa fa-check"></i><b>1.3.2</b> (2) The statistical model, <span class="math inline">\(P_0 \in \mathcal{M}\)</span></a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#step3"><i class="fa fa-check"></i><b>1.3.3</b> (3) The statistical target parameter, <span class="math inline">\(\Psi: \mathcal{M}\rightarrow\mathbb{R}\)</span></a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#step4"><i class="fa fa-check"></i><b>1.3.4</b> (4) The estimand, <span class="math inline">\(\Psi(P_0)\)</span></a></li>
<li class="chapter" data-level="1.3.5" data-path="intro.html"><a href="intro.html#step5"><i class="fa fa-check"></i><b>1.3.5</b> (5) The estimator, <span class="math inline">\(\hat{\Psi} : \mathcal{M}_{NP} \rightarrow \mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="1.3.6" data-path="intro.html"><a href="intro.html#step6"><i class="fa fa-check"></i><b>1.3.6</b> (6) The estimate, <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
<li class="chapter" data-level="1.3.7" data-path="intro.html"><a href="intro.html#step7"><i class="fa fa-check"></i><b>1.3.7</b> (7) Sampling distribution of <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#statistical-inference"><i class="fa fa-check"></i><b>1.4</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.5</b> Summary</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#appendix"><i class="fa fa-check"></i><b>1.6</b> Appendix</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#causal-models"><i class="fa fa-check"></i><b>1.7</b> Causal models</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#identifiability"><i class="fa fa-check"></i><b>1.8</b> Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tlverse.html"><a href="tlverse.html"><i class="fa fa-check"></i><b>2</b> Welcome to the <code>tlverse</code></a><ul>
<li class="chapter" data-level="2.1" data-path="tlverse.html"><a href="tlverse.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="tlverse.html"><a href="tlverse.html#what-is-the-tlverse"><i class="fa fa-check"></i><b>2.2</b> What is the <code>tlverse</code>?</a></li>
<li class="chapter" data-level="2.3" data-path="tlverse.html"><a href="tlverse.html#tlverse-components"><i class="fa fa-check"></i><b>2.3</b> <code>tlverse</code> components</a></li>
<li class="chapter" data-level="2.4" data-path="tlverse.html"><a href="tlverse.html#installation"><i class="fa fa-check"></i><b>2.4</b> Installation</a></li>
<li class="chapter" data-level="2.5" data-path="tlverse.html"><a href="tlverse.html#example-data---wash-benefits"><i class="fa fa-check"></i><b>2.5</b> Example Data - WASH Benefits</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html"><i class="fa fa-check"></i><b>3</b> Modern Super (Machine) Learning with <code>sl3</code></a><ul>
<li class="chapter" data-level="3.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#introduction-1"><i class="fa fa-check"></i><b>3.2</b> Introduction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#background"><i class="fa fa-check"></i><b>3.2.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#basic-implementation"><i class="fa fa-check"></i><b>3.3</b> Basic Implementation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#wash-benefits-study-example"><i class="fa fa-check"></i><b>3.3.1</b> WASH Benefits Study Example</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#extensions"><i class="fa fa-check"></i><b>3.4</b> Extensions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#customize-learner-hyperparameters"><i class="fa fa-check"></i><b>3.4.1</b> Customize learner hyperparameters</a></li>
<li class="chapter" data-level="3.4.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#screening-covariates"><i class="fa fa-check"></i><b>3.4.2</b> Screening covariates</a></li>
<li class="chapter" data-level="3.4.3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#cross-validated-super-learner"><i class="fa fa-check"></i><b>3.4.3</b> Cross-validated Super Learner</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#variable-importance"><i class="fa fa-check"></i><b>3.5</b> Variable importance</a></li>
<li class="chapter" data-level="3.6" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#exercise"><i class="fa fa-check"></i><b>3.6</b> Exercise</a><ul>
<li class="chapter" data-level="3.6.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#predicting-myocardial-infarction"><i class="fa fa-check"></i><b>3.6.1</b> Predicting myocardial infarction</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#concluding-remarks"><i class="fa fa-check"></i><b>3.7</b> Concluding Remarks</a></li>
<li class="chapter" data-level="3.8" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#appendix-1"><i class="fa fa-check"></i><b>3.8</b> Appendix</a><ul>
<li class="chapter" data-level="3.8.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#exercise-solution"><i class="fa fa-check"></i><b>3.8.1</b> Exercise solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html"><i class="fa fa-check"></i><b>4</b> <code>tmle3</code> – The Targeted Learning Framework</a><ul>
<li class="chapter" data-level="4.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#learning-objectives-3"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#example-tmle3-for-ate"><i class="fa fa-check"></i><b>4.2</b> Example: <code>tmle3</code> for ATE</a><ul>
<li class="chapter" data-level="4.2.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#load-the-data"><i class="fa fa-check"></i><b>4.2.1</b> Load the Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#define-the-variable-roles"><i class="fa fa-check"></i><b>4.2.2</b> Define the variable roles</a></li>
<li class="chapter" data-level="4.2.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#handle-missingness"><i class="fa fa-check"></i><b>4.2.3</b> Handle Missingness</a></li>
<li class="chapter" data-level="4.2.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#create-a-spec-object"><i class="fa fa-check"></i><b>4.2.4</b> Create a “Spec” Object</a></li>
<li class="chapter" data-level="4.2.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#define-the-learners"><i class="fa fa-check"></i><b>4.2.5</b> Define the learners</a></li>
<li class="chapter" data-level="4.2.6" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fit-the-tmle"><i class="fa fa-check"></i><b>4.2.6</b> Fit the TMLE</a></li>
<li class="chapter" data-level="4.2.7" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#evaluate-the-estimates"><i class="fa fa-check"></i><b>4.2.7</b> Evaluate the Estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#tmle3-components"><i class="fa fa-check"></i><b>4.3</b> <code>tmle3</code> Components</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#tmle3_task"><i class="fa fa-check"></i><b>4.3.1</b> <code>tmle3_task</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#initial-likelihood"><i class="fa fa-check"></i><b>4.3.2</b> Initial Likelihood</a></li>
<li class="chapter" data-level="4.3.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#targeted-likelihood-updater"><i class="fa fa-check"></i><b>4.3.3</b> Targeted Likelihood (updater)</a></li>
<li class="chapter" data-level="4.3.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#parameter-mapping"><i class="fa fa-check"></i><b>4.3.4</b> Parameter Mapping</a></li>
<li class="chapter" data-level="4.3.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#putting-it-all-together"><i class="fa fa-check"></i><b>4.3.5</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fitting-tmle3-with-multiple-parameters"><i class="fa fa-check"></i><b>4.4</b> Fitting <code>tmle3</code> with multiple parameters</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#delta-method"><i class="fa fa-check"></i><b>4.4.1</b> Delta Method</a></li>
<li class="chapter" data-level="4.4.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fit"><i class="fa fa-check"></i><b>4.4.2</b> Fit</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#exercise-1"><i class="fa fa-check"></i><b>4.5</b> Exercise</a></li>
<li class="chapter" data-level="4.6" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>5</b> Optimal Individualized Treatment Regimes</a><ul>
<li class="chapter" data-level="5.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-objectives-4"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#introduction-to-optimal-individualized-interventions"><i class="fa fa-check"></i><b>5.2</b> Introduction to Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="5.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#data-structure-and-notation"><i class="fa fa-check"></i><b>5.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="5.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#defining-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.4</b> Defining the Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="5.4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment"><i class="fa fa-check"></i><b>5.4.1</b> Binary treatment</a></li>
<li class="chapter" data-level="5.4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment"><i class="fa fa-check"></i><b>5.4.2</b> Categorical treatment</a></li>
<li class="chapter" data-level="5.4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#on-inference"><i class="fa fa-check"></i><b>5.4.3</b> On Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#why-cv-tmle"><i class="fa fa-check"></i><b>5.4.4</b> Why CV-TMLE?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interpreting-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.5</b> Interpreting the Causal Effect of an Optimal Individualized Intervention</a></li>
<li class="chapter" data-level="5.6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with"><i class="fa fa-check"></i><b>5.6</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with</a></li>
<li class="chapter" data-level="5.7" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment-1"><i class="fa fa-check"></i><b>5.7</b> Categorical Treatment</a><ul>
<li class="chapter" data-level="5.7.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data"><i class="fa fa-check"></i><b>5.7.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3"><i class="fa fa-check"></i><b>5.7.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.7.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized"><i class="fa fa-check"></i><b>5.7.3</b> Targeted Estimation of the Mean under the Optimal Individualized</a></li>
<li class="chapter" data-level="5.7.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interventions-effects"><i class="fa fa-check"></i><b>5.7.4</b> Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-1"><i class="fa fa-check"></i><b>5.8</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with</a></li>
<li class="chapter" data-level="5.9" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment-1"><i class="fa fa-check"></i><b>5.9</b> Binary Treatment</a><ul>
<li class="chapter" data-level="5.9.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-1"><i class="fa fa-check"></i><b>5.9.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.9.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-1"><i class="fa fa-check"></i><b>5.9.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.9.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-1"><i class="fa fa-check"></i><b>5.9.3</b> Targeted Estimation of the Mean under the Optimal Individualized</a></li>
<li class="chapter" data-level="5.9.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interventions-effects-1"><i class="fa fa-check"></i><b>5.9.4</b> Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#q-learning"><i class="fa fa-check"></i><b>5.10</b> Q-learning</a></li>
<li class="chapter" data-level="5.11" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extensions-to-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.11</b> Extensions to Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="5.11.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simpler-rules"><i class="fa fa-check"></i><b>5.11.1</b> Simpler Rules</a></li>
<li class="chapter" data-level="5.11.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#realistic-optimal-individual-regimes"><i class="fa fa-check"></i><b>5.11.2</b> Realistic Optimal Individual Regimes</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-analysis-with-optimal-individualized-interventions"><i class="fa fa-check"></i><b>5.12</b> Variable Importance Analysis with Optimal Individualized Interventions</a><ul>
<li class="chapter" data-level="5.12.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-2"><i class="fa fa-check"></i><b>5.12.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.12.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-2"><i class="fa fa-check"></i><b>5.12.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.12.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-using-targeted-estimation-of-the-value-of-the-itr"><i class="fa fa-check"></i><b>5.12.3</b> Variable Importance using Targeted Estimation of the value of the ITR</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#example-with-the-wash-benefits-data"><i class="fa fa-check"></i><b>5.13</b> Example with the WASH Benefits Data</a></li>
<li class="chapter" data-level="5.14" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercises"><i class="fa fa-check"></i><b>5.14</b> Exercises</a><ul>
<li class="chapter" data-level="5.14.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#review-of-key-concepts"><i class="fa fa-check"></i><b>5.14.1</b> Review of Key Concepts</a></li>
<li class="chapter" data-level="5.14.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#the-ideas-in-action"><i class="fa fa-check"></i><b>5.14.2</b> The Ideas in Action</a></li>
<li class="chapter" data-level="5.14.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#advanced-topics"><i class="fa fa-check"></i><b>5.14.3</b> Advanced Topics</a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#appendix-2"><i class="fa fa-check"></i><b>5.15</b> Appendix</a><ul>
<li class="chapter" data-level="5.15.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercise-solutions"><i class="fa fa-check"></i><b>5.15.1</b> Exercise solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>6</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#learning-objectives-5"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#introduction-to-stochastic-interventions"><i class="fa fa-check"></i><b>6.2</b> Introduction to Stochastic Interventions</a></li>
<li class="chapter" data-level="6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#data-structure-and-notation-1"><i class="fa fa-check"></i><b>6.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="6.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.4</b> Defining the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="6.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#interpreting-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.5</b> Interpreting the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="6.6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#evaluating-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.6</b> Evaluating the Causal Effect of a Stochastic Intervention</a><ul>
<li class="chapter" data-level="6.6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-simulated-data"><i class="fa fa-check"></i><b>6.6.1</b> Example with Simulated Data</a></li>
<li class="chapter" data-level="6.6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>6.6.2</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="6.6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>6.6.3</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>6.7</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="6.7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>6.7.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="6.7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>6.7.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="6.7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects-1"><i class="fa fa-check"></i><b>6.7.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="6.7.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>6.7.4</b> Inference with Marginal Structural Models</a></li>
<li class="chapter" data-level="6.7.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-the-wash-benefits-data-1"><i class="fa fa-check"></i><b>6.7.5</b> Example with the WASH Benefits Data</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises-1"><i class="fa fa-check"></i><b>6.8</b> Exercises</a><ul>
<li class="chapter" data-level="6.8.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#review-of-key-concepts-1"><i class="fa fa-check"></i><b>6.8.1</b> Review of Key Concepts</a></li>
<li class="chapter" data-level="6.8.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#the-ideas-in-action-1"><i class="fa fa-check"></i><b>6.8.2</b> The Ideas in Action</a></li>
<li class="chapter" data-level="6.8.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#advanced-topics-1"><i class="fa fa-check"></i><b>6.8.3</b> Advanced Topics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> The Roadmap for Targeted Learning</h1>
<div id="learning-objectives" class="section level2">
<h2><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>By the end of this chapter you will be able to:
1. Translate scientific questions to statistical questions.
2. Define a statistical model based on the knowledge of the experiment that
generated the data.
3. Identify a causal parameter as a function of the observed data distribution.
4. Explain the following causal and statistical assumptions and their
implications: i.i.d., consistency, interference, positivity, SUTVA.</p>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.2</span> Introduction</h2>
<p>The roadmap of statistical learning is concerned with the translation from
real-world data applications to a mathematical and statistical formulation of
the relevant estimation problem. This involves data as a random variable having
a probability distribution, scientific knowledge represented by a statistical
model, a statistical target parameter representing an answer to the question of
interest, and the notion of an estimator and sampling distribution of the
estimator.</p>
</div>
<div id="the-roadmap" class="section level2">
<h2><span class="header-section-number">1.3</span> The Roadmap</h2>
<div id="step1" class="section level3">
<h3><span class="header-section-number">1.3.1</span> (1) Data as a random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span></h3>
<p>The data set we’re confronted with is the result of an experiment and we can
view the data as a random variable, <span class="math inline">\(O\)</span>, because if we repeat the experiment
we would have a different realization of this experiment. In particular, if we
repeat the experiment many times we could learn the probability distribution,
<span class="math inline">\(P_0\)</span>, of our data. So, the observed data <span class="math inline">\(O\)</span> with probability distribution
<span class="math inline">\(P_0\)</span> are <span class="math inline">\(n\)</span> independent identically distributed (i.i.d.) observations of the
random variable $O; <span class="math inline">\(O_1, \ldots, O_n\)</span>. Note that while not all data are i.i.d.,
there are ways to handle non-i.i.d. data, such as establishing conditional
independence, stratifying data to create sets of identically distributed data,
etc. It is crucial that researchers be absolutely clear about what they actually
know about the data-generating distribution for a given problem of interest.
Unfortunately, communication between statisticians and researchers is often
fraught with misinterpretation. The roadmap provides a mechanism by which to
ensure clear communication between research and statistician – it truly helps
with this communication!</p>
<div id="the-empirical-probability-measure-p_n" class="section level4">
<h4><span class="header-section-number">1.3.1.1</span> The empirical probability measure, <span class="math inline">\(P_n\)</span>:</h4>
<p>Once we have <span class="math inline">\(n\)</span> of such i.i.d. observations we have an empirical probability
measure, <span class="math inline">\(P_n\)</span>. The empirical probability measure is an approximation of the
true probability measure <span class="math inline">\(P_0\)</span>, allowing us to learn from our data. For
example, we can define the empirical probability measure of a set, <span class="math inline">\(A\)</span>, to be
the proportion of observations which end up in <span class="math inline">\(A\)</span>. That is,
<span class="math display">\[\begin{equation*}
  P_n(A) = \frac{1}{n}\sum_{i=1}^{n}I(O_i \in A)
\end{equation*}\]</span></p>
<p>In order to start learning something, we need to ask <em>“What do we know about the
probability distribution of the data?”</em> This brings us to Step 2.</p>
</div>
</div>
<div id="step2" class="section level3">
<h3><span class="header-section-number">1.3.2</span> (2) The statistical model, <span class="math inline">\(P_0 \in \mathcal{M}\)</span></h3>
<p>The statistical model <span class="math inline">\(\mathcal{M}\)</span> is defined by the question we asked at the
end of . It is defined as the set of possible probability
distributions for our observed data. Often <span class="math inline">\(\mathcal{M}\)</span> is very large (possibly
infinite-dimensional), to reflect the fact that statistical knowledge is
limited. In the case that <span class="math inline">\(\mathcal{M}\)</span> is infinite-dimensional, we deem this a
nonparametric statistical model.</p>
<p>Alternatively, if the probability distribution of the data at hand is described
by a finite number of parameters, then the statistical model is parametric. In
this case, we prescribe to the belief that the random variable <span class="math inline">\(O\)</span> being
observed has, e.g., a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance
<span class="math inline">\(\sigma^2\)</span>. More formally, a parametric model may be defined
<span class="math display">\[\begin{equation*}
  \mathcal{M} = \{P_{\theta} : \theta \in \mathcal{R}^d \}
\end{equation*}\]</span></p>
<p>Sadly, it is all-too-common to assume the data-generating distributions have
specific forms when such a statement is a leap of faith. This lack of truth
in the current culture of analysis typically trumps any attempt at trying to
answer the scientific question at hand; alas, such statements as the
ever-popular quip of Box that “All models are wrong but some are useful,”
encourage the data analyst to make arbitrary choices even when differences in
these choices may drive differences in the resultant answers to the same
estimation problem. The Targeted Learning methodology does not suffer from this
bias since it defines the statistical model as a representation for the true
data-generating distribution that produced the observed data.</p>
<p>Our next question becomes, <em>``What are we trying to learn from the data?&quot;</em>
This brings us to Step 3.</p>
</div>
<div id="step3" class="section level3">
<h3><span class="header-section-number">1.3.3</span> (3) The statistical target parameter, <span class="math inline">\(\Psi: \mathcal{M}\rightarrow\mathbb{R}\)</span></h3>
<!--
Nima's edits paused here. 06 May 2019.
-->
<p>The statistical target parameter, <span class="math inline">\(\Psi\)</span> is defined as a mapping from the
statistical model, <span class="math inline">\(\mathcal{M}\)</span> to the parameter space (i.e., a number),
<span class="math inline">\(\mathcal{R}\)</span>. It is defined by the query mentioned at the end of Step 2. Target
parameters of interest represent questions, which are often causal.
Causal target parameters require identifiability, and causal models (see
appendix of this chapter for more detail).</p>
<p>For example, say we observe a survival time on every subject and our question
of interest is “What’s the probability that someone lives longer than five
years?” We have,</p>
<p><span class="math display">\[\Psi(P_0) = P_0(O &gt; 5)\]</span></p>
<p>This answer to this question brings us to Step 4.</p>
</div>
<div id="step4" class="section level3">
<h3><span class="header-section-number">1.3.4</span> (4) The estimand, <span class="math inline">\(\Psi(P_0)\)</span></h3>
<p>The answer to the query asked in Step 3. This estimand is quantity we’re really
trying to learn. Once we have defined <span class="math inline">\(O\)</span>, <span class="math inline">\(\mathcal{M}\)</span> and <span class="math inline">\(\Psi(P_0)\)</span> we have
formally defined the statistical estimation problem.</p>
<p>After formally defining the statistical estimation problem, we use the data to
estimate the estimand and hopefully the estimator has good statistical
properties to approximate the estimate. Additionally, we try to end up with
statistical inference (i.e., we try to quantify the uncertainty in our
estimator). We need statistical theory to guide us in the construction of our
estimators.</p>
</div>
<div id="step5" class="section level3">
<h3><span class="header-section-number">1.3.5</span> (5) The estimator, <span class="math inline">\(\hat{\Psi} : \mathcal{M}_{NP} \rightarrow \mathbb{R}^d\)</span></h3>
<p>To come up with a good estimand we need an estimator, an a-priori specified
algorithm defined as a mapping from the set of possible empirical distributions,
<span class="math inline">\(P_n\)</span>, which live in a non-parametric statistical model, <span class="math inline">\(\mathcal{M}_{NP}\)</span>
(<span class="math inline">\(P_n \in \mathcal{M}_{NP}\)</span>), to the parameter space for our parameter of
interest. It is a function that takes as input the observed data, a realization
of <span class="math inline">\(P_n\)</span>, and gives as output a value in the parameter space.</p>
</div>
<div id="step6" class="section level3">
<h3><span class="header-section-number">1.3.6</span> (6) The estimate, <span class="math inline">\(\hat{\Psi}(P_n)\)</span></h3>
<p>The output mentioned at the end of Step 5 is defined as the estimate. It is a
function of the empirical probability distribution of the data that is an
element of the parameter space. If we plug in a realization of <span class="math inline">\(P_n\)</span> (based on
a sample size <span class="math inline">\(n\)</span> of the random variable <span class="math inline">\(O\)</span>), we get back an estimate
<span class="math inline">\(\hat{\Psi}(P_n)\)</span> of the true parameter value <span class="math inline">\(\Psi(P_0)\)</span>.\</p>
<p>In order to have any hope of coming up with the quantification of the
uncertainty (i.e. statistical inference), we need to understand the sampling
distribution of our estimator. This brings us to step 7.</p>
</div>
<div id="step7" class="section level3">
<h3><span class="header-section-number">1.3.7</span> (7) Sampling distribution of <span class="math inline">\(\hat{\Psi}(P_n)\)</span></h3>
<p>The sampling distribution of <span class="math inline">\(\hat{\Psi}(P_n)\)</span> says that the estimator itself
is a random variable. So, if we repeat the experiment of drawing <span class="math inline">\(n\)</span>
observations we would every time end up with a different realization of our
estimator and our estimator has a sampling/probability distribution. Hopefully
this sampling distribution can be theoretically validated to be approximately
normally distributed.</p>
</div>
</div>
<div id="statistical-inference" class="section level2">
<h2><span class="header-section-number">1.4</span> Statistical Inference</h2>
<p>The  (CLT) allows us to make statements regarding
the  approximating a normal
distribution when the sample size gets large enough. For large <span class="math inline">\(n\)</span> we have,</p>
<p><span class="math display">\[\hat{\Psi}(P_n) \sim N\Big( \Psi(P_0), \frac{\sigma^2}{n} \Big)\]</span></p>
<p>This permits statistical inference. Now we can quantify the uncertainty in our
estimator. For example, we can construct a 95% confidence interval for our
estimand, <span class="math inline">\(\Psi(P_0)\)</span>:</p>
<p><span class="math display">\[\hat{\Psi}(P_n) \pm 1.96 \Big( \frac{\sigma}{\sqrt{n}} \Big)\]</span></p>
<p>Note: we typically have to estimate the standard error,
<span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>.\</p>
<p>A 95% confidence interval means that if we were to take 100 different samples
of size <span class="math inline">\(n\)</span> and compute a 95% confidence interval for each sample then
approximately 95 of the 100 confidence intervals would contain the estimand,
<span class="math inline">\(\Psi(P_0)\)</span>. More practically, this means that there is a 95% probability
(or 95% confidence) that the confidence interval procedure will contain the
true estimand. However, any single estimated confidence interval either will
contain the true estimand or will not.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">1.5</span> Summary</h2>
<p>Data, <span class="math inline">\(O\)</span>, is viewed as a random variable that has a probability distribution.
We often have <span class="math inline">\(n\)</span> units of independent identically distributed units with
probability distribution <span class="math inline">\(P_0\)</span> (<span class="math inline">\(O_1, \ldots, O_n \simiid P_0\)</span>). We have
statistical knowledge about the experiment that generated this data. In other
words, we make a statement that the true data distribution <span class="math inline">\(P_0\)</span> falls in a
certain set called a statistical model, <span class="math inline">\(\mathcal{M}\)</span>. Often these sets are very
large because statistical knowledge is very limited so these statistical models
are often infinite dimensional models. Our statistical query is, “What are we
trying to learn from the data?” denoted by the statistical target parameter,
<span class="math inline">\(\Psi\)</span>, which maps the <span class="math inline">\(P_0\)</span> into the estimand, <span class="math inline">\(\Psi(P_0)\)</span>. At this point the
statistical estimation problem is formally defined and now we will need
statistical theory to guide us in the construction of estimators. There’s a lot
of statistical theory we will review in this course that, in particular, relies
on the Central Limit Theorem, allowing us to come up with estimators that are
approximately normally distributed and also allowing us to come with statistical
inference (i.e., confidence intervals).</p>
</div>
<div id="appendix" class="section level2">
<h2><span class="header-section-number">1.6</span> Appendix</h2>
</div>
<div id="causal-models" class="section level2">
<h2><span class="header-section-number">1.7</span> Causal models</h2>
<p>A causal framework may be employed to formalize the experiment and enrich the
parameter of interest, or research question, by asking a causal question opposed
to a statistical (i.e., associational/relational) question. Causal graphs are
one useful tool to express what we know about the causal relations among
variables that are relevant to the question under study <span class="citation">(<span class="citeproc-not-found" data-reference-id="pearl_causality_2000"><strong>???</strong></span>)</span>.
An illustration shows a simple causal graph, specifically a <em>directed acyclic
graph or DAG</em>, to depict the causal relations between variables with a simple
example of a binary exposure <span class="math inline">\(A\)</span>, a binary outcome <span class="math inline">\(Y\)</span>, and one categorical
confounding variable <span class="math inline">\(W\)</span>. The DAG for these relations is depicted below.</p>
<p>The <span class="math inline">\(U_W\)</span>, <span class="math inline">\(U_A\)</span>, and <span class="math inline">\(U_Y\)</span> are the unmeasured exogenous background
characteristics that influence the value of each variable. Alternatively, the
same causal relations among variables can be represented with a series of equations:</p>

<p>Here <span class="math inline">\(f_W, f_A\)</span> and <span class="math inline">\(f_Y\)</span> denote that each variable (<span class="math inline">\(W, A\)</span> and <span class="math inline">\(Y\)</span>
respectively) is a function of its parents and unmeasured background
characteristics, but there is no imposition of any particular functional
constraints. For this reason, these equations represent a <em>non-parametric
structural equation model (NPSEM)</em> <span class="citation">Pearl (<a href="#ref-pearl2009causality">2009</a>)</span>. The DAG and this series
of non-parametric structural equations represent the same information.</p>
<p>Let’s consider a hypothetical experiment in which we assign the
exposure to the whole population and observe the outcome, and then assign no
exposure to the whole population and observe the outcome. This hypothetical
experiment is “counterfactual” and thus can never be observed in practice, but
allows one to imagine an ideal experiment in which we observe everyone’s
potential outcome. On the NPSEM, this corresponds to a comparison of the outcome
distribution in the population under two interventions: 1) <span class="math inline">\(A\)</span> is set to 1 for
all individuals, and 2) <span class="math inline">\(A\)</span> is set to 0 for all individuals. These interventions
imply two post-intervention NPSEMs with first being:
<span class="math display">\[\begin{eqnarray*}
W &amp;=&amp; f_W(U_W) \\
A &amp;=&amp; 1 \\
Y(1)&amp;=&amp;f_Y(W,1,U_Y),
\end{eqnarray*}\]</span>
and second just replacing the intervention of <span class="math inline">\(A=1\)</span> to <span class="math inline">\(A=0\)</span>,
<span class="math display">\[\begin{eqnarray*}
W &amp;=&amp; f_W(U_W) \\
A &amp;=&amp; 0 \\
Y(0)&amp;=&amp;f_Y(W,0,U_Y).
\end{eqnarray*}\]</span></p>
<p>In these equations, <span class="math inline">\(A\)</span> is no longer a function of <span class="math inline">\(W\)</span> because we have
intervened on the graph and set <span class="math inline">\(A\)</span> to the values 1 and 0. The new symbols
<span class="math inline">\(Y(1)\)</span> and <span class="math inline">\(Y(0)\)</span> indicate the outcome variable in our population if it were
generated by the respective NPSEMs above; often called counterfactuals. The
difference between the means of the outcome under these two interventions
defines a parameter that is often called the “Average Treatment Effect (ATE), or
<span class="math display">\[\begin{equation}
\label{ate}
ATE = E_X(Y(1)-Y(0)),
\end{equation}\]</span>
where <span class="math inline">\(E_X\)</span> is the mean under the theoretical full data: <span class="math inline">\(X=(W,Y(1),Y(0))\)</span>.
Because we can never observe both <span class="math inline">\(Y(0)\)</span> (counterfactual when <span class="math inline">\(A=0\)</span>) and <span class="math inline">\(Y(1)\)</span>,
we can not estimate  directly.</p>
</div>
<div id="identifiability" class="section level2">
<h2><span class="header-section-number">1.8</span> Identifiability</h2>
<p>We have to make assumptions to estimate this quantity from <span class="math inline">\(O \sim P_0\)</span>, or the
data-generating distribution. Fortunately, given our causal model shown in the
graph above, we can, with a couple of more assumptions, estimate the ATE even
from observational data. First, the causal graph implies that <span class="math inline">\(Y(a) \perp A\)</span> for
all <span class="math inline">\(a \in \mathcal{A}\)</span>, which is the randomization assumption. The
randomization assumption is also referred to as no unmeasured confounding or
<em>strong ignorability</em>. Outside of the graph, we also need to assume <em>no
interference</em>, the outcome for unit <span class="math inline">\(i\)</span> <span class="math inline">\(Y_i\)</span> is not affected by exposure for
unit <span class="math inline">\(j\)</span> <span class="math inline">\(A_j\)</span> unless <span class="math inline">\(i=j\)</span>; <em>consistency</em>: the outcome for unit <span class="math inline">\(i\)</span> is <span class="math inline">\(Y_i(a)\)</span>
whenever <span class="math inline">\(A_i = a\)</span>, also known as “no other versions of treatment”, and these
two assumptions (consistency and no interference) are jointly referred to as
<em>stable unit value of treatment assignment (SUTVA)</em>. We also need to make the
positivity assumption (<span class="math inline">\(0&lt;P_0(A=a\mid W)&lt; 1\)</span> for all <span class="math inline">\(a\)</span> and <span class="math inline">\(W\)</span>) for our target
parameter to be well defined. Given these assumptions, then we can re-write the
target parameter as a function of the observed data distribution.</p>
<p>Continuing from our example above, we may write the ATE as a function of <span class="math inline">\(P_0\)</span>.
Specifically,
<span class="math display">\[\begin{equation}
\label{estimand}
ATE = E_0(Y(1)=Y(0)) = E_0\left( E_0[Y \mid A=1,W]-E_0[Y \mid A=0,W] \right),
\end{equation}\]</span>
or the difference in predicted values for each subject in population and then
averaging over those subjects.</p>
<p>Thus, a parameter of a theoretical “full” data distribution can be represented
as an estimand of the observed data distribution. Moreover, there is nothing
about the representation in  that requires parameteric
assumptions, so that the regressions on the right hand side of the equation
above can be estimated freely with machine learning. With
different parameters, there may be different/additional identifiability
assumptions and the resulting estimands can be functions of different components
of <span class="math inline">\(P_0\)</span>. We discuss several more complex ones in this workshop later.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-pearl2009causality">
<p>Pearl, Judea. 2009. <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="motivation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tlverse.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/tlverse-handbook/edit/master/02-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["handbook.pdf", "handbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
