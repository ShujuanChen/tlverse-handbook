<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Super (Machine) Learning | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Super (Machine) Learning | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/tlverse-handbook/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/tlverse-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Super (Machine) Learning | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard, Mark van der Laan" />


<meta name="date" content="2020-02-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="data.html"/>
<link rel="next" href="tmle3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-is-not"><i class="fa fa-check"></i>What this book is not</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#jeremy-coyle"><i class="fa fa-check"></i>Jeremy Coyle</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nima-hejazi"><i class="fa fa-check"></i>Nima Hejazi</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i>Ivana Malenica</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rachael-phillips"><i class="fa fa-check"></i>Rachael Phillips</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alan-hubbard"><i class="fa fa-check"></i>Alan Hubbard</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mark-van-der-laan"><i class="fa fa-check"></i>Mark van der Laan</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#learn"><i class="fa fa-check"></i><b>0.1</b> Recommended Learning Resources</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#setup-instructions"><i class="fa fa-check"></i><b>0.2</b> Setup instructions</a><ul>
<li class="chapter" data-level="0.2.1" data-path="index.html"><a href="index.html#r-and-rstudio"><i class="fa fa-check"></i><b>0.2.1</b> R and RStudio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> The Roadmap for Targeted Learning</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-roadmap"><i class="fa fa-check"></i><b>1.3</b> The Roadmap</a><ul>
<li><a href="intro.html#data-as-a-random-variable-with-a-probability-distribution-o-sim-p_0">(1) Data as a random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span></a></li>
<li><a href="intro.html#the-statistical-model-mathcalm-such-that-p_0-in-mathcalm">(2) The statistical model <span class="math inline">\(\mathcal{M}\)</span> such that <span class="math inline">\(P_0 \in \mathcal{M}\)</span></a></li>
<li><a href="intro.html#the-statistical-target-parameter-psi-and-estimand-psip_0">(3) The statistical target parameter <span class="math inline">\(\Psi\)</span> and estimand <span class="math inline">\(\Psi(P_0)\)</span></a></li>
<li><a href="intro.html#the-estimator-hatpsi-and-estimate-hatpsip_n">(4) The estimator <span class="math inline">\(\hat{\Psi}\)</span> and estimate <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
<li><a href="intro.html#a-measure-of-uncertainty-for-the-estimate-hatpsip_n">(5) A measure of uncertainty for the estimate <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#summary-of-the-roadmap"><i class="fa fa-check"></i><b>1.4</b> Summary of the Roadmap</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#causal"><i class="fa fa-check"></i><b>1.5</b> Causal Target Parameters</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#the-causal-model"><i class="fa fa-check"></i><b>1.5.1</b> The Causal Model</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#identifiability"><i class="fa fa-check"></i><b>1.5.2</b> Identifiability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tlverse.html"><a href="tlverse.html"><i class="fa fa-check"></i><b>2</b> Welcome to the <code>tlverse</code></a><ul>
<li class="chapter" data-level="2.1" data-path="tlverse.html"><a href="tlverse.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="tlverse.html"><a href="tlverse.html#what-is-the-tlverse"><i class="fa fa-check"></i><b>2.2</b> What is the <code>tlverse</code>?</a></li>
<li class="chapter" data-level="2.3" data-path="tlverse.html"><a href="tlverse.html#tlverse-components"><i class="fa fa-check"></i><b>2.3</b> <code>tlverse</code> components</a></li>
<li class="chapter" data-level="2.4" data-path="tlverse.html"><a href="tlverse.html#installation"><i class="fa fa-check"></i><b>2.4</b> Installation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#wash"><i class="fa fa-check"></i><b>3.1</b> WASH Benefits Example Dataset</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#ist"><i class="fa fa-check"></i><b>3.2</b> International Stroke Trial Example Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#vet"><i class="fa fa-check"></i><b>3.3</b> Veterans’ Administration Lung Cancer Trial Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sl3.html"><a href="sl3.html"><i class="fa fa-check"></i><b>4</b> Super (Machine) Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="sl3.html"><a href="sl3.html#learning-objectives-2"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="sl3.html"><a href="sl3.html#motivation-1"><i class="fa fa-check"></i><b>4.2</b> Motivation</a></li>
<li class="chapter" data-level="4.3" data-path="sl3.html"><a href="sl3.html#introduction-1"><i class="fa fa-check"></i><b>4.3</b> Introduction</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sl3.html"><a href="sl3.html#background"><i class="fa fa-check"></i><b>4.3.1</b> Background</a></li>
<li class="chapter" data-level="4.3.2" data-path="sl3.html"><a href="sl3.html#super-learner-for-prediction"><i class="fa fa-check"></i><b>4.3.2</b> Super Learner for Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sl3.html"><a href="sl3.html#sl3-microwave-dinner-implementation"><i class="fa fa-check"></i><b>4.4</b> <code>sl3</code> “Microwave Dinner” Implementation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sl3.html"><a href="sl3.html#wash-benefits-study-example"><i class="fa fa-check"></i><b>4.4.1</b> WASH Benefits Study Example</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#load-the-necessary-libraries-and-data"><i class="fa fa-check"></i>0. Load the necessary libraries and data</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#define-the-machine-learning-task"><i class="fa fa-check"></i>1. Define the machine learning task</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#make-a-super-learner"><i class="fa fa-check"></i>2. Make a Super Learner</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#train-the-super-learner-on-the-machine-learning-task"><i class="fa fa-check"></i>3. Train the Super Learner on the machine learning task</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#obtain-predicted-values"><i class="fa fa-check"></i>4. Obtain predicted values</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sl3.html"><a href="sl3.html#cross-validated-super-learner"><i class="fa fa-check"></i><b>4.5</b> Cross-validated Super Learner</a></li>
<li class="chapter" data-level="4.6" data-path="sl3.html"><a href="sl3.html#variable-importance-measures-with-sl3"><i class="fa fa-check"></i><b>4.6</b> Variable Importance Measures with <code>sl3</code></a><ul>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#international-stroke-trial-example"><i class="fa fa-check"></i>International Stroke Trial Example</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="sl3.html"><a href="sl3.html#exercise"><i class="fa fa-check"></i><b>4.7</b> Exercise</a><ul>
<li class="chapter" data-level="4.7.1" data-path="sl3.html"><a href="sl3.html#sl3ex"><i class="fa fa-check"></i><b>4.7.1</b> Predicting Myocardial Infarction with <code>sl3</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="sl3.html"><a href="sl3.html#concluding-remarks"><i class="fa fa-check"></i><b>4.8</b> Concluding Remarks</a><ul>
<li class="chapter" data-level="4.8.1" data-path="sl3.html"><a href="sl3.html#exercise-1-solution"><i class="fa fa-check"></i><b>4.8.1</b> Exercise 1 Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tmle3.html"><a href="tmle3.html"><i class="fa fa-check"></i><b>5</b> The TMLE Framework</a></li>
<li class="chapter" data-level="6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>6</b> Optimal Individualized Treatment Regimes</a></li>
<li class="chapter" data-level="7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>7</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="7.0.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.0.1</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>7.1</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="7.1.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>7.1.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="7.1.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>7.1.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="7.1.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>7.1.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="7.1.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>7.1.4</b> Inference with Marginal Structural Models</a></li>
<li class="chapter" data-level="7.1.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-the-wash-benefits-data"><i class="fa fa-check"></i><b>7.1.5</b> Example with the WASH Benefits Data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises"><i class="fa fa-check"></i><b>7.2</b> Exercises</a><ul>
<li class="chapter" data-level="7.2.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#the-ideas-in-action"><i class="fa fa-check"></i><b>7.2.1</b> The Ideas in Action</a></li>
<li class="chapter" data-level="7.2.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#review-of-key-concepts"><i class="fa fa-check"></i><b>7.2.2</b> Review of Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="r6.html"><a href="r6.html"><i class="fa fa-check"></i><b>8</b> A Primer on the <code>R6</code> Class System</a><ul>
<li class="chapter" data-level="8.1" data-path="r6.html"><a href="r6.html#classes-fields-and-methods"><i class="fa fa-check"></i><b>8.1</b> Classes, Fields, and Methods</a></li>
<li class="chapter" data-level="8.2" data-path="r6.html"><a href="r6.html#object-oriented-programming-python-and-r"><i class="fa fa-check"></i><b>8.2</b> Object Oriented Programming: <code>Python</code> and <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sl3" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Super (Machine) Learning</h1>
<p><em>Rachael Phillips</em></p>
<p>Based on the <a href="https://github.com/tlverse/sl3"><code>sl3</code> <code>R</code> package</a> by <em>Jeremy
Coyle, Nima Hejazi, Ivana Malenica, and Oleg Sofrygin</em>.</p>
<p>Updated: 2020-02-07</p>
<div id="learning-objectives-2" class="section level2">
<h2><span class="header-section-number">4.1</span> Learning Objectives</h2>
<p>By the end of this chapter you will be able to:</p>
<ol style="list-style-type: decimal">
<li>Select a loss function that is appropriate for the functional parameter to be
estimated.</li>
<li>Assemble an ensemble of learners based on the properties that identify what
features they support.</li>
<li>Customize learner hyperparameters to incorporate a diversity of different
settings.</li>
<li>Select a subset of available covariates and pass only those variables to the
modeling algorithm.</li>
<li>Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.</li>
<li>Obtain <code>sl3</code> variable importance metrics.</li>
<li>Interpret the discrete and continuous Super Learner fits.</li>
<li>Rationalize the need to remove bias from the Super Learner to make an optimal
bias–variance tradeoff for the parameter of interest.</li>
</ol>
</div>
<div id="motivation-1" class="section level2">
<h2><span class="header-section-number">4.2</span> Motivation</h2>
<ul>
<li>A common task in statistical data analysis is estimator selection (e.g., for
prediction).</li>
<li>There is no universally optimal machine learning algorithm for density
estimation or prediction.</li>
<li>For some data, one needs learners that can model a complex function.</li>
<li>For others, possibly as a result of noise or insufficient sample size, a
simple, parametric model might fit best.</li>
<li>The Super Learner, an ensemble learner, solves this issue, by allowing a
combination of learners from the simplest (intercept-only) to most complex
(neural nets, random forests, SVM, etc).</li>
<li>It works by using cross-validation in a manner which guarantees that the
resulting fit will be as good as possible, given the learners provided.</li>
</ul>
</div>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">4.3</span> Introduction</h2>
<p>In <a href="intro.html#intro">Chapter 1</a>, we introduced the Roadmap for Targeted Learning as a
general template to translate real-world data applications into formal
statistical estimation problems. The first steps of this roadmap define the
<em>statistical estimation problem</em>, which establish</p>
<ol style="list-style-type: decimal">
<li>Data as a realization of a random variable, or equivalently, an outcome of a
particular experiment.</li>
<li>A statistical model, representing the true knowledge about the
data-generating experiment.</li>
<li>A translation of the scientific question, which is often causal, into a
target parameter.</li>
</ol>
<p>Note that if the target parameter is causal, step 3 also requires
establishing identifiability of the target quantity from the observed data
distribution, under possible non-testable assumptions that may not necessarily
be reasonable. Still, the target quantity does have a valid statistical
interpretation. See <a href="intro.html#causal">causal target parameters</a> for more detail on
causal models and identifiability.</p>
<p>Now that we have defined the statistical estimation problem, we are ready to
construct the TMLE; an asymptotically linear and efficient substitution
estimator of this target quantity. The first step in this estimation procedure
is an initial estimate of the data-generating distribution, or the relevant part
of this distribution that is needed to evaluate the target parameter. For this
initial estimation, we use the Super Learner <span class="citation">(van der Laan, Polley, and Hubbard <a href="#ref-vdl2007super">2007</a>)</span>.</p>
<p>The Super Learner provides an important step in creating a robust estimator. It
is a loss-function-based tool that uses cross-validation to obtain the best
prediction of our target parameter, based on a weighted average of a library of
machine learning algorithms.</p>
<p>The library of machine learning algorithms consists of functions (“learners” in
the <code>sl3</code> nomenclature) that we think might be consistent with the true
data-generating distribution (i.e. algorithms selected based on contextual
knowledge of the experiment that generated the data). Also, the library should<br />
contain a large set of “default” algorithms that may range from a simple linear
regression model to multi-step algorithms involving screening covariates,
penalizations, optimizing tuning parameters, etc.</p>
<p>The ensembling of the collection of algorithms with weights (“metalearning” in
the <code>sl3</code> nomenclature) has been shown to be adaptive and robust, even in small
samples <span class="citation">(Polley and van der Laan <a href="#ref-polley2010super">2010</a>)</span>. The Super Learner is proven to be asymptotically as
accurate as the best possible prediction algorithm in the library
<span class="citation">(van der Laan and Dudoit <a href="#ref-vdl2003unified">2003</a>; van der Vaart, Dudoit, and van der Laan <a href="#ref-van2006oracle">2006</a>)</span>.</p>
<div id="background" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Background</h3>
<p>A <em>loss function</em> <span class="math inline">\(L\)</span> is defined as a function of the observed data and a
candidate parameter value <span class="math inline">\(\psi\)</span>, which has unknown true value <span class="math inline">\(\psi_0\)</span>,
<span class="math inline">\(L(\psi)(O)\)</span>. We can estimate the loss by substituting the empirical
distribution <span class="math inline">\(P_n\)</span> for the true (but unknown) distribution of the observed data
<span class="math inline">\(P_0\)</span>. A valid loss function will have expectation (risk) that is minimized at
the true value of the parameter <span class="math inline">\(\psi_0\)</span>. For example, the conditional mean
minimizes the risk of the squared error loss. Thus, it is a valid loss function
when estimating the conditional mean.</p>
<p>The <em>discrete Super Learner</em>, or <em>cross-validation selector</em>, is the algorithm
in the library that minimizes the cross-validated empirical risk.</p>
<p>The <em>cross-validated empirical risk</em> of an algorithm is defined as the empirical
mean over a validation sample of the loss of the algorithm fitted on the
training sample, averaged across the splits of the data.</p>
<p>The <em>continuous/ensemble Super Learner</em>, often referred to as <em>Super Learner</em>
is a weighted average of the library of algorithms, where the weights are chosen
to minimize the cross-validated empirical risk of the library. Restricting the
weights to be positive and sum to one (i.e., a convex combination) has been
shown to improve upon the discrete Super Learner <span class="citation">(Polley and van der Laan <a href="#ref-polley2010super">2010</a>; van der Laan, Polley, and Hubbard <a href="#ref-vdl2007super">2007</a>)</span>. This notion of weighted combinations was introduced in
<span class="citation">Wolpert (<a href="#ref-wolpert1992stacked">1992</a>)</span> for neural networks and adapted for regressions in
<span class="citation">Breiman (<a href="#ref-breiman1996stacked">1996</a>)</span>.</p>
<p>Cross-validation is proven to be optimal for selection among estimators. This
result was established through the oracle inequality for the cross-validation
selector among a collection of candidate estimators <span class="citation">(van der Laan and Dudoit <a href="#ref-vdl2003unified">2003</a>; van der Vaart, Dudoit, and van der Laan <a href="#ref-van2006oracle">2006</a>)</span>. The only condition is that loss function is uniformly bounded,
which is guaranteed in <code>sl3</code>.</p>
<!--
The *oracle results* prove that, if the number of algorithms in the library are
polynomial in sample size, then the cross-validation selector (i.e., discrete
Super Learner) (1) is equivalent with the oracle selector asymptotically (based
on sample of size of training samples), or (2) achieves the parametric rate (log
$n/n$) for convergence with respect to the loss-based dissimilarity (risk)
between a candidate estimate $\psi$ and the true parameter value $\psi_0$.
-->
<div id="general-overview-of-the-algorithm" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> General Overview of the Algorithm</h4>
<p><strong>What is cross-validation and how does it work?</strong></p>
<p>There are many different cross-validation schemes, designed to accommodate
different study designs and data structures. The figure below shows an example
of 10-fold cross-validation.</p>
<embed src="img/misc/vs.pdf" width="    extwidth" style="display: block; margin: auto;" type="application/pdf" />
<p><strong>General step-by-step overview of the Super Learner algorithm:</strong></p>
<ul>
<li>Break up the sample evenly into V-folds (say V=10).</li>
<li>For each of these 10 folds, remove that portion of the sample (kept out as
validation sample) and the remaining will be used to fit learners (training
sample).</li>
<li>Fit each learner on the training sample (note, some learners will have their
own internal cross-validation procedure or other methods to select tuning
parameters).</li>
<li>For each observation in the corresponding validation sample, predict the outcome
using each of the learners, so if there are <span class="math inline">\(p\)</span> learners, then there would be
<span class="math inline">\(p\)</span> predictions.</li>
<li>Take out another validation sample and repeat until each of the V-sets of data
are removed.</li>
<li>Compare the cross-validated fit of the learners across all observations based
on specified loss function (e.g., squared error, negative log-likelihood, …)
by calculating the corresponding average loss (risk).</li>
<li><p>Either:</p>
<ul>
<li>choose the learner with smallest risk and apply that learner to entire data
set (resulting SL fit),</li>
<li><p>do a weighted average of the learners to minimize the cross-validated risk
(construct an ensemble of learners), by</p>
<ul>
<li>re-fitting the learners on the original data set, and</li>
<li>use the weights above to get the SL fit.</li>
</ul></li>
</ul></li>
</ul>
<p>This entire procedure can be itself cross-validated to get a consistent
estimate of the future performance of the Super Learner, and we implement this
procedure later in this chapter.</p>
<embed src="img/misc/SLKaiserNew.pdf" width="   extwidth" style="display: block; margin: auto;" type="application/pdf" />
</div>
</div>
<div id="super-learner-for-prediction" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Super Learner for Prediction</h3>
<p>Say we observe a learning data set <span class="math inline">\(X_i=(Y_i,W_i)\)</span>, for <span class="math inline">\(i=1, ..., n\)</span>, where
<span class="math inline">\(Y_i\)</span> is the outcome of interest, <span class="math inline">\(W_i\)</span> is a <span class="math inline">\(p\)</span>-dimensional set of
covariates, and our objective is to estimate the function <span class="math inline">\(\psi_0(W) = E(Y|W)\)</span>.
This function can be expressed as the minimizer of the expected loss:
<span class="math inline">\(\psi_0(W) = \text{argmin}_{\psi} E[L(X,\psi(W))]\)</span>. Here, the loss function is
represented as <span class="math inline">\(L\)</span> (e.g., squared error loss, <span class="math inline">\(L: (Y-\psi(W))^2)\)</span>).</p>
<p>For prediction, one can use the cross-validated risk to empirically determine
the relative performance of the Super Learner . When we
have tested different algorithms on actual data and looked at the performance
(e.g., MSE of prediction), never does one algorithm always win. Below, we show
the results of such a study, comparing the fits of several different learners,
including the SL algorithms.</p>
<embed src="img/misc/ericSL.pdf" width="    extwidth" style="display: block; margin: auto;" type="application/pdf" />
<p>For more detail on Super Learner we refer the reader to <span class="citation">van der Laan, Polley, and Hubbard (<a href="#ref-vdl2007super">2007</a>)</span> and
<span class="citation">Polley and van der Laan (<a href="#ref-polley2010super">2010</a>)</span>. The optimality results for the cross-validation selector
among a family of algorithms were established in <span class="citation">van der Laan and Dudoit (<a href="#ref-vdl2003unified">2003</a>)</span> and extended
in <span class="citation">van der Vaart, Dudoit, and van der Laan (<a href="#ref-van2006oracle">2006</a>)</span>.</p>
</div>
</div>
<div id="sl3-microwave-dinner-implementation" class="section level2">
<h2><span class="header-section-number">4.4</span> <code>sl3</code> “Microwave Dinner” Implementation</h2>
<p>We begin by illustrating the core functionality of the super learner algorithm
as implemented in <code>sl3</code>. For those who are interested in the internals
of <code>sl3</code>, see this <a href="https://tlverse.org/sl3/articles/intro_sl3.html"><code>sl3</code> introductory
tutorial</a>.</p>
<p>The <code>sl3</code> implementation consists of the following steps:</p>
<ol start="0" style="list-style-type: decimal">
<li>Load the necessary libraries and data</li>
<li>Define the machine learning task</li>
<li>Make a super learner by creating library of base learners and a metalearner</li>
<li>Train the super learner on the machine learning task</li>
<li>Obtain predicted values</li>
</ol>
<div id="wash-benefits-study-example" class="section level3">
<h3><span class="header-section-number">4.4.1</span> WASH Benefits Study Example</h3>
<p>Using the WASH data, we are interested in predicting weight-for-height z-score
<code>whz</code> using the available covariate data. More information on this dataset, and
all other data that we will work with in this handbook, is contained in
<span id="data">Chapter 3</span>. Let’s begin!</p>
</div>
<div id="load-the-necessary-libraries-and-data" class="section level3 unnumbered">
<h3>0. Load the necessary libraries and data</h3>
<p>First, we will load the relevant <code>R</code> packages, set a seed, and load the data.</p>
<p>If you would like to use newer <code>sl3</code> functionality that is available in the
devel branch of the <code>sl3</code> GitHub repository, you need to install that version of
the package (e.g. <code>devtools::install_github(tlverse/sl3@devel)</code>), re-start your
<code>R</code> session, and then re-load the <code>sl3</code> package.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">library</span>(here)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb12-4" data-line-number="4"><span class="kw">library</span>(kableExtra)</a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb12-6" data-line-number="6"><span class="kw">library</span>(origami)</a>
<a class="sourceLine" id="cb12-7" data-line-number="7"><span class="kw">library</span>(SuperLearner)</a>
<a class="sourceLine" id="cb12-8" data-line-number="8"><span class="kw">library</span>(sl3)</a>
<a class="sourceLine" id="cb12-9" data-line-number="9"></a>
<a class="sourceLine" id="cb12-10" data-line-number="10"><span class="kw">set.seed</span>(<span class="dv">7194</span>) </a>
<a class="sourceLine" id="cb12-11" data-line-number="11"><span class="co"># my lucky seed! or is it 9174? or 4917? many lucky seeds, thanks lysdexia!</span></a>
<a class="sourceLine" id="cb12-12" data-line-number="12"></a>
<a class="sourceLine" id="cb12-13" data-line-number="13"><span class="co"># load data set and take a peek</span></a>
<a class="sourceLine" id="cb12-14" data-line-number="14">washb_data &lt;-<span class="st"> </span><span class="kw">fread</span>(<span class="st">&quot;https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv&quot;</span>,</a>
<a class="sourceLine" id="cb12-15" data-line-number="15">                    <span class="dt">stringsAsFactors =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb12-16" data-line-number="16"><span class="kw">head</span>(washb_data) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb12-17" data-line-number="17"><span class="st">  </span><span class="kw">kable</span>(<span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb12-18" data-line-number="18"><span class="st">  </span>kableExtra<span class="op">:::</span><span class="kw">kable_styling</span>(<span class="dt">fixed_thead =</span> T) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb12-19" data-line-number="19"><span class="st">  </span><span class="kw">scroll_box</span>(<span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>, <span class="dt">height =</span> <span class="st">&quot;300px&quot;</span>)</a></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; ">
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
whz
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
tr
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
fracode
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
month
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
aged
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
sex
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
momage
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
momedu
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
momheight
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
hfiacat
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
Nlt18
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
Ncomp
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
watmin
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
elec
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
floor
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
walls
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
roof
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_wardrobe
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_table
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_chair
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_khat
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_chouki
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_tv
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_refrig
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_bike
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_moto
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_sewmach
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
asset_mobile
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N05265
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
268
</td>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:left;">
Primary (1-5y)
</td>
<td style="text-align:right;">
146.40
</td>
<td style="text-align:left;">
Food Secure
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.16
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N05265
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
286
</td>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:left;">
Primary (1-5y)
</td>
<td style="text-align:right;">
148.75
</td>
<td style="text-align:left;">
Moderately Food Insecure
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.05
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N08002
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
264
</td>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:left;">
Primary (1-5y)
</td>
<td style="text-align:right;">
152.15
</td>
<td style="text-align:left;">
Food Secure
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.26
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N08002
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
252
</td>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
Primary (1-5y)
</td>
<td style="text-align:right;">
140.25
</td>
<td style="text-align:left;">
Food Secure
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.59
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N06531
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
336
</td>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:left;">
Secondary (&gt;5y)
</td>
<td style="text-align:right;">
150.95
</td>
<td style="text-align:left;">
Food Secure
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.51
</td>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
N06531
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
304
</td>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:left;">
Secondary (&gt;5y)
</td>
<td style="text-align:right;">
154.20
</td>
<td style="text-align:left;">
Severely Food Insecure
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="define-the-machine-learning-task" class="section level3 unnumbered">
<h3>1. Define the machine learning task</h3>
<p>To define the machine learning <strong>“task”</strong> (predict weight-for-height z-score <code>whz</code>
using the available covariate data), we need to create an <code>sl3_Task</code> object.</p>
<p>The <code>sl3_Task</code> keeps track of the roles the variables play in the
machine learning problem, the data, and any metadata (e.g., observational-level
weights, id, offset).</p>
<p>Also, if we had missing outcomes, we would need to set
<code>drop_missing_outcome = TRUE</code> when we create the task. In the next analysis,
with the <a href="data.html#ist">IST stroke trial data</a>, we do have a missing outcome. In the
following chapter, we estimate the missingness mechanism and account for it in
the TMLE.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># specify the outcome and covariates</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">outcome &lt;-<span class="st"> &quot;whz&quot;</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">covars &lt;-<span class="st"> </span><span class="kw">colnames</span>(washb_data)[<span class="op">-</span><span class="kw">which</span>(<span class="kw">names</span>(washb_data) <span class="op">==</span><span class="st"> </span>outcome)]</a>
<a class="sourceLine" id="cb13-4" data-line-number="4"></a>
<a class="sourceLine" id="cb13-5" data-line-number="5"><span class="co"># create the sl3 task</span></a>
<a class="sourceLine" id="cb13-6" data-line-number="6">washb_task &lt;-<span class="st"> </span><span class="kw">make_sl3_Task</span>(</a>
<a class="sourceLine" id="cb13-7" data-line-number="7">  <span class="dt">data =</span> washb_data,</a>
<a class="sourceLine" id="cb13-8" data-line-number="8">  <span class="dt">covariates =</span> covars,</a>
<a class="sourceLine" id="cb13-9" data-line-number="9">  <span class="dt">outcome =</span> outcome</a>
<a class="sourceLine" id="cb13-10" data-line-number="10">)</a></code></pre></div>
<pre><code>Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.</code></pre>
<p><em>This warning is important.</em> The task just imputed missing covariates for us.
Specifically, for each covariate column with missing values, <code>sl3</code> uses the
median to impute missing continuous covariates, and the mode to impute binary
and categorical covariates.</p>
<p>Also, for each covariate column with missing values, <code>sl3</code> adds an additional
column indicating whether or not the value was imputed, which is particularly
handy when the missingness in the data might be informative.</p>
<p>Also, notice that we did not specify the number of folds, or the loss function
in the task. The default cross-validation scheme is V-fold, with the number of
folds <span class="math inline">\(V=10\)</span>.</p>
<p>Let’s visualize our <code>washb_task</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">washb_task</a></code></pre></div>
<pre><code>A sl3 Task with 4695 obs and these nodes:
$covariates
 [1] &quot;tr&quot;              &quot;fracode&quot;         &quot;month&quot;           &quot;aged&quot;           
 [5] &quot;sex&quot;             &quot;momage&quot;          &quot;momedu&quot;          &quot;momheight&quot;      
 [9] &quot;hfiacat&quot;         &quot;Nlt18&quot;           &quot;Ncomp&quot;           &quot;watmin&quot;         
[13] &quot;elec&quot;            &quot;floor&quot;           &quot;walls&quot;           &quot;roof&quot;           
[17] &quot;asset_wardrobe&quot;  &quot;asset_table&quot;     &quot;asset_chair&quot;     &quot;asset_khat&quot;     
[21] &quot;asset_chouki&quot;    &quot;asset_tv&quot;        &quot;asset_refrig&quot;    &quot;asset_bike&quot;     
[25] &quot;asset_moto&quot;      &quot;asset_sewmach&quot;   &quot;asset_mobile&quot;    &quot;delta_momage&quot;   
[29] &quot;delta_momheight&quot;

$outcome
[1] &quot;whz&quot;

$id
NULL

$weights
NULL

$offset
NULL</code></pre>
</div>
<div id="make-a-super-learner" class="section level3 unnumbered">
<h3>2. Make a Super Learner</h3>
<p>Now that we have defined our machine learning problem with the task, we are
ready to <strong>“make”</strong> the Super Learner. This requires specification of</p>
<ul>
<li>A library of base learning algorithms that we think might be consistent with
the true data-generating distribution.</li>
<li>A metalearner, to ensemble the base learners.</li>
</ul>
<p>We might also incorporate</p>
<ul>
<li>Feature selection, to pass only a subset of the predictors to the algorithm.</li>
<li>Hyperparameter specification, to tune base learners.</li>
</ul>
<p>Learners have properties that indicate what features they support. We may use
<code>sl3_list_properties()</code> to get a list of all properties supported by at least
one learner.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">sl3_list_properties</span>()</a></code></pre></div>
<pre><code> [1] &quot;binomial&quot;             &quot;categorical&quot;          &quot;continuous&quot;          
 [4] &quot;cv&quot;                   &quot;density&quot;              &quot;ids&quot;                 
 [7] &quot;multivariate_outcome&quot; &quot;offset&quot;               &quot;preprocessing&quot;       
[10] &quot;timeseries&quot;           &quot;weights&quot;              &quot;wrapper&quot;             </code></pre>
<p>Since we have a continuous outcome, we may identify the learners that support
this outcome type with <code>sl3_list_learners()</code>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">sl3_list_learners</span>(<span class="st">&quot;continuous&quot;</span>)</a></code></pre></div>
<pre><code> [1] &quot;Lrnr_arima&quot;                     &quot;Lrnr_bartMachine&quot;              
 [3] &quot;Lrnr_bilstm&quot;                    &quot;Lrnr_caret&quot;                    
 [5] &quot;Lrnr_condensier&quot;                &quot;Lrnr_dbarts&quot;                   
 [7] &quot;Lrnr_earth&quot;                     &quot;Lrnr_expSmooth&quot;                
 [9] &quot;Lrnr_gam&quot;                       &quot;Lrnr_gbm&quot;                      
[11] &quot;Lrnr_glm&quot;                       &quot;Lrnr_glm_fast&quot;                 
[13] &quot;Lrnr_glmnet&quot;                    &quot;Lrnr_grf&quot;                      
[15] &quot;Lrnr_h2o_glm&quot;                   &quot;Lrnr_h2o_grid&quot;                 
[17] &quot;Lrnr_hal9001&quot;                   &quot;Lrnr_HarmonicReg&quot;              
[19] &quot;Lrnr_lstm&quot;                      &quot;Lrnr_mean&quot;                     
[21] &quot;Lrnr_nnls&quot;                      &quot;Lrnr_optim&quot;                    
[23] &quot;Lrnr_pkg_SuperLearner&quot;          &quot;Lrnr_pkg_SuperLearner_method&quot;  
[25] &quot;Lrnr_pkg_SuperLearner_screener&quot; &quot;Lrnr_polspline&quot;                
[27] &quot;Lrnr_randomForest&quot;              &quot;Lrnr_ranger&quot;                   
[29] &quot;Lrnr_rpart&quot;                     &quot;Lrnr_rugarch&quot;                  
[31] &quot;Lrnr_screener_corP&quot;             &quot;Lrnr_screener_corRank&quot;         
[33] &quot;Lrnr_screener_randomForest&quot;     &quot;Lrnr_solnp&quot;                    
[35] &quot;Lrnr_stratified&quot;                &quot;Lrnr_svm&quot;                      
[37] &quot;Lrnr_tsDyn&quot;                     &quot;Lrnr_xgboost&quot;                  </code></pre>
<p>Now that we have an idea of some learners, we can construct them using the
<code>make_learner</code> function.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># choose base learners</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2">lrnr_glm &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glm)</a>
<a class="sourceLine" id="cb21-3" data-line-number="3">lrnr_mean &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_mean)</a></code></pre></div>
<p>We can customize learner hyperparameters to incorporate a diversity of different
settings. Documentation for the learners and their hyperparameters can be found
in the <a href="https://tlverse.org/sl3/reference/index.html#section-sl-learners"><code>sl3</code> Learners
Reference</a>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">lrnr_ranger50 &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_ranger, <span class="dt">num.trees =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">lrnr_hal_simple &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_hal9001, <span class="dt">max_degree =</span> <span class="dv">2</span>, <span class="dt">n_folds =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3">lrnr_lasso &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glmnet) <span class="co"># alpha default is 1</span></a>
<a class="sourceLine" id="cb22-4" data-line-number="4">lrnr_ridge &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glmnet, <span class="dt">alpha =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb22-5" data-line-number="5">lrnr_elasticnet &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glmnet, <span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<p>We can also include learners from the <code>SuperLearner</code> <code>R</code> package.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">lrnr_bayesglm &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;SL.bayesglm&quot;</span>)</a></code></pre></div>
<p>Here is a fun trick to create customized learners over a grid of parameters.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co"># I like to crock pot my super learners</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2">grid_params &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">cost =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">                    <span class="dt">gamma =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">                    <span class="dt">kernel =</span> <span class="kw">c</span>(<span class="st">&quot;polynomial&quot;</span>, <span class="st">&quot;radial&quot;</span>, <span class="st">&quot;sigmoid&quot;</span>),</a>
<a class="sourceLine" id="cb24-5" data-line-number="5">                    <span class="dt">degree =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb24-6" data-line-number="6">grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(grid_params, <span class="dt">KEEP.OUT.ATTRS =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb24-7" data-line-number="7">params_default &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">nthread =</span> <span class="kw">getOption</span>(<span class="st">&quot;sl.cores.learners&quot;</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb24-8" data-line-number="8">svm_learners &lt;-<span class="st"> </span><span class="kw">apply</span>(grid, <span class="dt">MARGIN =</span> <span class="dv">1</span>, <span class="cf">function</span>(params_tune) {</a>
<a class="sourceLine" id="cb24-9" data-line-number="9">  <span class="kw">do.call</span>(Lrnr_svm<span class="op">$</span>new, <span class="kw">c</span>(params_default, <span class="kw">as.list</span>(params_tune)))})</a></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">grid_params &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">max_depth =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>),</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">                    <span class="dt">eta =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</a>
<a class="sourceLine" id="cb25-3" data-line-number="3">                    <span class="dt">nrounds =</span> <span class="kw">c</span>(<span class="dv">20</span>, <span class="dv">50</span>))</a>
<a class="sourceLine" id="cb25-4" data-line-number="4">grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(grid_params, <span class="dt">KEEP.OUT.ATTRS =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb25-5" data-line-number="5">params_default &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">nthread =</span> <span class="kw">getOption</span>(<span class="st">&quot;sl.cores.learners&quot;</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb25-6" data-line-number="6">xgb_learners &lt;-<span class="st"> </span><span class="kw">apply</span>(grid, <span class="dt">MARGIN =</span> <span class="dv">1</span>, <span class="cf">function</span>(params_tune) {</a>
<a class="sourceLine" id="cb25-7" data-line-number="7">  <span class="kw">do.call</span>(Lrnr_xgboost<span class="op">$</span>new, <span class="kw">c</span>(params_default, <span class="kw">as.list</span>(params_tune)))})</a></code></pre></div>
<p>Did you see <code>Lrnr_caret</code> when we called <code>sl3_list_learners(c(&quot;binomial&quot;))</code>?
All we need to specify is the algorithm to use, which is passed as <code>method</code> to
<code>caret::train()</code>. The default method for parameter selection criterion with
is set to “CV” instead of the <code>caret::train()</code> default <code>boot</code>. The summary
metric to used to select the optimal model is <code>RMSE</code> for continuous outcomes
and <code>Accuracy</code> for categorical and binomial outcomes.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># I have no idea how to tune a neural net (or BART machine..) </span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2">lrnr_caret_nnet &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_caret, <span class="dt">algorithm =</span> <span class="st">&quot;nnet&quot;</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">lrnr_caret_bartMachine &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_caret, <span class="dt">algorithm =</span> <span class="st">&quot;bartMachine&quot;</span>, </a>
<a class="sourceLine" id="cb26-4" data-line-number="4">                                       <span class="dt">method =</span> <span class="st">&quot;boot&quot;</span>, <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">                                       <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a></code></pre></div>
<p>In order to assemble the library of learners, we need to <strong>“stack”</strong> them
together.</p>
<p>A <code>Stack</code> is a special learner and it has the same interface as all
other learners. What makes a stack special is that it combines multiple learners
by training them simultaneously, so that their predictions can be either
combined or compared.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">  Stack,</a>
<a class="sourceLine" id="cb27-3" data-line-number="3">  lrnr_glm, lrnr_mean, lrnr_ridge, lrnr_lasso, xgb_learners[[<span class="dv">10</span>]]</a>
<a class="sourceLine" id="cb27-4" data-line-number="4">)</a></code></pre></div>
<p>We can optionally select a subset of available covariates and pass only
those variables to the modeling algorithm.</p>
<p>Let’s consider screening covariates based on their <code>randomForest</code> variable
importance ranking (ordered by mean decrease in accuracy). We select the top 5
most important covariates according to this ranking, and we decreased the
ntree to 20.</p>
<p>Before you think it – I will confess. Bob Ross and I both know that 20 trees
makes for a lonely forest, and I shouldn’t even consider it, but these are the
sacrifices I have to make for this chapter to build in under 50 minutes!</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">screen_rf &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_screener_randomForest, <span class="dt">nVar =</span> <span class="dv">5</span>, <span class="dt">ntree =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb28-2" data-line-number="2"><span class="co"># which covariates are selected on the full data?</span></a>
<a class="sourceLine" id="cb28-3" data-line-number="3">screen_rf<span class="op">$</span><span class="kw">train</span>(washb_task)</a></code></pre></div>
<pre><code>[1] &quot;Lrnr_screener_randomForest_5_20&quot;
$selected
[1] &quot;month&quot;     &quot;aged&quot;      &quot;momage&quot;    &quot;momheight&quot; &quot;Ncomp&quot;    </code></pre>
<p>To <strong>“pipe”</strong> only the selected covariates to the modeling algorithm, we need to
make a <code>Pipeline</code>, which is a just set of learners to be fit sequentially, where
the fit from one learner is used to define the task for the next learner.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1">screen_rf_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_rf, stack)</a></code></pre></div>
<p>Now our learners will be preceded by a screening step.</p>
<p>We also consider the original <code>stack</code>, to compare how the feature selection
methods perform in comparison to the methods without feature selection, and
because</p>
<p>Analogous to what we have seen before, we have to stack the pipeline and
original <code>stack</code> together, so we may use them as base learners in our super
learner.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1">fancy_stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Stack, screen_rf_pipeline, stack)</a>
<a class="sourceLine" id="cb31-2" data-line-number="2"><span class="co"># we can visualize the stack</span></a>
<a class="sourceLine" id="cb31-3" data-line-number="3">dt_stack &lt;-<span class="st"> </span><span class="kw">delayed_learner_train</span>(fancy_stack, washb_task)</a>
<a class="sourceLine" id="cb31-4" data-line-number="4"><span class="kw">plot</span>(dt_stack, <span class="dt">color =</span> <span class="ot">FALSE</span>, <span class="dt">height =</span> <span class="st">&quot;400px&quot;</span>, <span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>)</a></code></pre></div>
<div id="htmlwidget-299736b4b999c0df2149" style="width:100%;height:400px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-299736b4b999c0df2149">{"x":{"nodes":{"id":["1741a936-49f1-11ea-bbfa-42010a1400d2","174197e8-49f1-11ea-bbfa-42010a1400d2","17411386-49f1-11ea-bbfa-42010a1400d2","1741047c-49f1-11ea-bbfa-42010a1400d2","173ffe2e-49f1-11ea-bbfa-42010a1400d2","1740e442-49f1-11ea-bbfa-42010a1400d2","1740126a-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","174043b6-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","17406422-49f1-11ea-bbfa-42010a1400d2","17408466-49f1-11ea-bbfa-42010a1400d2","1740a20c-49f1-11ea-bbfa-42010a1400d2","1740c066-49f1-11ea-bbfa-42010a1400d2","17418942-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","17412a38-49f1-11ea-bbfa-42010a1400d2","17413a8c-49f1-11ea-bbfa-42010a1400d2","17414bc6-49f1-11ea-bbfa-42010a1400d2","17415c4c-49f1-11ea-bbfa-42010a1400d2","17416c8c-49f1-11ea-bbfa-42010a1400d2"],"label":["Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1"],"level":[1,2,3,4,10,5,9,6,7,8,7,7,7,7,3,4,5,5,5,5,5],"sequential":[true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false],"state":["waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready"],"group":["none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none"]},"edges":{"from":["173ffe2e-49f1-11ea-bbfa-42010a1400d2","173ffe2e-49f1-11ea-bbfa-42010a1400d2","1740126a-49f1-11ea-bbfa-42010a1400d2","1740126a-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","174043b6-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","17406422-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","17408466-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","1740a20c-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","1740c066-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","1740e442-49f1-11ea-bbfa-42010a1400d2","1741047c-49f1-11ea-bbfa-42010a1400d2","17411386-49f1-11ea-bbfa-42010a1400d2","17412a38-49f1-11ea-bbfa-42010a1400d2","17413a8c-49f1-11ea-bbfa-42010a1400d2","17414bc6-49f1-11ea-bbfa-42010a1400d2","17415c4c-49f1-11ea-bbfa-42010a1400d2","17416c8c-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","17418942-49f1-11ea-bbfa-42010a1400d2","174197e8-49f1-11ea-bbfa-42010a1400d2"],"to":["1741047c-49f1-11ea-bbfa-42010a1400d2","1740126a-49f1-11ea-bbfa-42010a1400d2","1740e442-49f1-11ea-bbfa-42010a1400d2","174023f4-49f1-11ea-bbfa-42010a1400d2","174043b6-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","17406422-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","17408466-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","1740a20c-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","1740c066-49f1-11ea-bbfa-42010a1400d2","1740d4d4-49f1-11ea-bbfa-42010a1400d2","1740e442-49f1-11ea-bbfa-42010a1400d2","1741047c-49f1-11ea-bbfa-42010a1400d2","17411386-49f1-11ea-bbfa-42010a1400d2","174197e8-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","174179d4-49f1-11ea-bbfa-42010a1400d2","17418942-49f1-11ea-bbfa-42010a1400d2","174197e8-49f1-11ea-bbfa-42010a1400d2","1741a936-49f1-11ea-bbfa-42010a1400d2"],"label":["","","","","","","","","","","","","","","","","","","","","","","","","",""]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot"},"manipulation":{"enabled":false},"edges":{"arrows":"to"},"layout":{"hierarchical":{"enabled":true,"levelSeparation":500,"nodeSpacing":200,"direction":"RL"}},"groups":{"useDefaultGroups":true,"none":{"color":{"border":"black","background":"white"}}}},"groups":["none"],"width":"100%","height":"400px","idselection":{"enabled":false},"byselection":{"enabled":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)"},"evals":[],"jsHooks":[]}</script>
<p>We will use the <a href="https://github.com/tlverse/sl3/blob/master/R/default_metalearner.R">default
metalearner</a>,
which uses <a href="https://github.com/tlverse/sl3/blob/master/R/Lrnr_solnp.R"><code>Lrnr_solnp()</code></a>
to provide fitting procedures for a pairing of <a href="https://github.com/tlverse/sl3/blob/master/R/loss_functions.R">loss
function</a> and
<a href="https://github.com/tlverse/sl3/blob/master/R/metalearners.R">metalearner
function</a>. This
default metalearner selects a loss and metalearner pairing based on the outcome
type. Note that any learner can be used as a metalearner.</p>
<p>We have made a library/stack of base learners, so we are ready to make the super
learner. The super learner algorithm fits a metalearner on the validation-set
predictions.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">sl &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl,</a>
<a class="sourceLine" id="cb32-2" data-line-number="2">  <span class="dt">learners =</span> fancy_stack</a>
<a class="sourceLine" id="cb32-3" data-line-number="3">)</a></code></pre></div>
<p>We can also use <code>Lrnr_cv</code> to build a super learner, cross-validate a stack of
learners to compare performance of the learners in the stack, or cross-validate
any single learner (see “Cross-validation” section of this <a href="https://tlverse.org/sl3/articles/intro_sl3.html"><code>sl3</code>
introductory tutorial</a>).</p>
<p>Furthermore, we can <a href="https://tlverse.org/sl3/articles/custom_lrnrs.html">Define New <code>sl3</code>
Learners</a> which can be used
in all the places you could otherwise use any other <code>sl3</code> learners, including
<code>Pipelines</code>, <code>Stacks</code>, and the Super Learner.</p>
<p>In the plot below, we visualize the steps for executing the Super Learner in the
<code>tlverse/delayed</code> framework. For those like myself who are not particularly
keen on understanding the intricacies of <code>delayed</code>, let’s focus on the main
point of this figure: we can see there are 10 realizations of the stack which
represent the 10 cross-validation folds and there is a separate hold-out
(top branch of the figure) that will not be used to fit the Super Learner.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1">dt_sl &lt;-<span class="st"> </span><span class="kw">delayed_learner_train</span>(sl, washb_task)</a>
<a class="sourceLine" id="cb33-2" data-line-number="2"><span class="kw">plot</span>(dt_sl, <span class="dt">color =</span> <span class="ot">FALSE</span>, <span class="dt">height =</span> <span class="st">&quot;400px&quot;</span>, <span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>)</a></code></pre></div>
<div id="htmlwidget-6df9f40f0d9476035693" style="width:100%;height:400px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-6df9f40f0d9476035693">{"x":{"nodes":{"id":["17797e38-49f1-11ea-bbfa-42010a1400d2","17796e70-49f1-11ea-bbfa-42010a1400d2","17792fa0-49f1-11ea-bbfa-42010a1400d2","17791f56-49f1-11ea-bbfa-42010a1400d2","174eaad2-49f1-11ea-bbfa-42010a1400d2","174e9d62-49f1-11ea-bbfa-42010a1400d2","174e1b80-49f1-11ea-bbfa-42010a1400d2","174e0d52-49f1-11ea-bbfa-42010a1400d2","174d17e4-49f1-11ea-bbfa-42010a1400d2","174df132-49f1-11ea-bbfa-42010a1400d2","174d2630-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174d58f8-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174d7702-49f1-11ea-bbfa-42010a1400d2","174d9458-49f1-11ea-bbfa-42010a1400d2","174db17c-49f1-11ea-bbfa-42010a1400d2","174dd17a-49f1-11ea-bbfa-42010a1400d2","174e8f7a-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e30de-49f1-11ea-bbfa-42010a1400d2","174e40d8-49f1-11ea-bbfa-42010a1400d2","174e50b4-49f1-11ea-bbfa-42010a1400d2","174e5ffa-49f1-11ea-bbfa-42010a1400d2","174e72a6-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17556750-49f1-11ea-bbfa-42010a1400d2","17555990-49f1-11ea-bbfa-42010a1400d2","1754d308-49f1-11ea-bbfa-42010a1400d2","1754c444-49f1-11ea-bbfa-42010a1400d2","1753c954-49f1-11ea-bbfa-42010a1400d2","1754a7a2-49f1-11ea-bbfa-42010a1400d2","1753d962-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","17540dc4-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17542c14-49f1-11ea-bbfa-42010a1400d2","17544af0-49f1-11ea-bbfa-42010a1400d2","17546940-49f1-11ea-bbfa-42010a1400d2","17548948-49f1-11ea-bbfa-42010a1400d2","17554b76-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","1754e906-49f1-11ea-bbfa-42010a1400d2","1754f95a-49f1-11ea-bbfa-42010a1400d2","17550954-49f1-11ea-bbfa-42010a1400d2","17551a16-49f1-11ea-bbfa-42010a1400d2","17552db2-49f1-11ea-bbfa-42010a1400d2","17650fe8-49f1-11ea-bbfa-42010a1400d2","1764fe68-49f1-11ea-bbfa-42010a1400d2","17645a80-49f1-11ea-bbfa-42010a1400d2","17644b76-49f1-11ea-bbfa-42010a1400d2","17633164-49f1-11ea-bbfa-42010a1400d2","17642cae-49f1-11ea-bbfa-42010a1400d2","176344c4-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","176382ae-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","1763a7e8-49f1-11ea-bbfa-42010a1400d2","1763cc5a-49f1-11ea-bbfa-42010a1400d2","1763ed98-49f1-11ea-bbfa-42010a1400d2","17640d3c-49f1-11ea-bbfa-42010a1400d2","1764eec8-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","17647cfe-49f1-11ea-bbfa-42010a1400d2","176490d6-49f1-11ea-bbfa-42010a1400d2","1764a83c-49f1-11ea-bbfa-42010a1400d2","1764bc1e-49f1-11ea-bbfa-42010a1400d2","1764ce16-49f1-11ea-bbfa-42010a1400d2","17678462-49f1-11ea-bbfa-42010a1400d2","1767751c-49f1-11ea-bbfa-42010a1400d2","1766e606-49f1-11ea-bbfa-42010a1400d2","1766d274-49f1-11ea-bbfa-42010a1400d2","1765c46a-49f1-11ea-bbfa-42010a1400d2","1766b320-49f1-11ea-bbfa-42010a1400d2","1765d338-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","17660da8-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","17662efa-49f1-11ea-bbfa-42010a1400d2","1766522c-49f1-11ea-bbfa-42010a1400d2","176672f2-49f1-11ea-bbfa-42010a1400d2","17669336-49f1-11ea-bbfa-42010a1400d2","176763b0-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","1766fe84-49f1-11ea-bbfa-42010a1400d2","17671040-49f1-11ea-bbfa-42010a1400d2","176721ac-49f1-11ea-bbfa-42010a1400d2","17673318-49f1-11ea-bbfa-42010a1400d2","17674470-49f1-11ea-bbfa-42010a1400d2","176af62e-49f1-11ea-bbfa-42010a1400d2","176ae6e8-49f1-11ea-bbfa-42010a1400d2","176a5318-49f1-11ea-bbfa-42010a1400d2","176a43fa-49f1-11ea-bbfa-42010a1400d2","176945f4-49f1-11ea-bbfa-42010a1400d2","176a2604-49f1-11ea-bbfa-42010a1400d2","17695abc-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","17698b68-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","1769a918-49f1-11ea-bbfa-42010a1400d2","1769c790-49f1-11ea-bbfa-42010a1400d2","1769e7a2-49f1-11ea-bbfa-42010a1400d2","176a07be-49f1-11ea-bbfa-42010a1400d2","176ad824-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176a69c0-49f1-11ea-bbfa-42010a1400d2","176a7bd6-49f1-11ea-bbfa-42010a1400d2","176a93fa-49f1-11ea-bbfa-42010a1400d2","176aa67e-49f1-11ea-bbfa-42010a1400d2","176ab808-49f1-11ea-bbfa-42010a1400d2","176d4d48-49f1-11ea-bbfa-42010a1400d2","176d3f42-49f1-11ea-bbfa-42010a1400d2","176cb770-49f1-11ea-bbfa-42010a1400d2","176ca96a-49f1-11ea-bbfa-42010a1400d2","176ba81c-49f1-11ea-bbfa-42010a1400d2","176c8b60-49f1-11ea-bbfa-42010a1400d2","176bb99c-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176bea84-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176c0ae6-49f1-11ea-bbfa-42010a1400d2","176c2a44-49f1-11ea-bbfa-42010a1400d2","176c4a10-49f1-11ea-bbfa-42010a1400d2","176c6c84-49f1-11ea-bbfa-42010a1400d2","176d30a6-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176cce22-49f1-11ea-bbfa-42010a1400d2","176cdf34-49f1-11ea-bbfa-42010a1400d2","176cf140-49f1-11ea-bbfa-42010a1400d2","176d020c-49f1-11ea-bbfa-42010a1400d2","176d12f6-49f1-11ea-bbfa-42010a1400d2","176f9dc8-49f1-11ea-bbfa-42010a1400d2","176f8f18-49f1-11ea-bbfa-42010a1400d2","176f06ce-49f1-11ea-bbfa-42010a1400d2","176ef792-49f1-11ea-bbfa-42010a1400d2","176e00b2-49f1-11ea-bbfa-42010a1400d2","176eda14-49f1-11ea-bbfa-42010a1400d2","176e0f08-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176e3f28-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e5de6-49f1-11ea-bbfa-42010a1400d2","176e7bfa-49f1-11ea-bbfa-42010a1400d2","176e9a5e-49f1-11ea-bbfa-42010a1400d2","176eb82c-49f1-11ea-bbfa-42010a1400d2","176f7e6a-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f1db2-49f1-11ea-bbfa-42010a1400d2","176f2e06-49f1-11ea-bbfa-42010a1400d2","176f3e96-49f1-11ea-bbfa-42010a1400d2","176f4f12-49f1-11ea-bbfa-42010a1400d2","176f5fc0-49f1-11ea-bbfa-42010a1400d2","1771ea24-49f1-11ea-bbfa-42010a1400d2","1771db42-49f1-11ea-bbfa-42010a1400d2","177156d6-49f1-11ea-bbfa-42010a1400d2","1771481c-49f1-11ea-bbfa-42010a1400d2","17704b38-49f1-11ea-bbfa-42010a1400d2","177126e8-49f1-11ea-bbfa-42010a1400d2","1770597a-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","1770871a-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770aa06-49f1-11ea-bbfa-42010a1400d2","1770ca9a-49f1-11ea-bbfa-42010a1400d2","1770ea2a-49f1-11ea-bbfa-42010a1400d2","1771094c-49f1-11ea-bbfa-42010a1400d2","1771c9cc-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","17716d1a-49f1-11ea-bbfa-42010a1400d2","17717d0a-49f1-11ea-bbfa-42010a1400d2","17718dea-49f1-11ea-bbfa-42010a1400d2","17719d6c-49f1-11ea-bbfa-42010a1400d2","1771add4-49f1-11ea-bbfa-42010a1400d2","177449b8-49f1-11ea-bbfa-42010a1400d2","1774389c-49f1-11ea-bbfa-42010a1400d2","1773adbe-49f1-11ea-bbfa-42010a1400d2","17739bd0-49f1-11ea-bbfa-42010a1400d2","177295dc-49f1-11ea-bbfa-42010a1400d2","17737bbe-49f1-11ea-bbfa-42010a1400d2","1772a40a-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","1772d1aa-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","1772f7e8-49f1-11ea-bbfa-42010a1400d2","17731cfa-49f1-11ea-bbfa-42010a1400d2","17733dfc-49f1-11ea-bbfa-42010a1400d2","17735dbe-49f1-11ea-bbfa-42010a1400d2","177429f6-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","1773c524-49f1-11ea-bbfa-42010a1400d2","1773d668-49f1-11ea-bbfa-42010a1400d2","1773e6d0-49f1-11ea-bbfa-42010a1400d2","1773f85a-49f1-11ea-bbfa-42010a1400d2","17740b2e-49f1-11ea-bbfa-42010a1400d2","1776a136-49f1-11ea-bbfa-42010a1400d2","17769312-49f1-11ea-bbfa-42010a1400d2","17760910-49f1-11ea-bbfa-42010a1400d2","1775faec-49f1-11ea-bbfa-42010a1400d2","1774fa3e-49f1-11ea-bbfa-42010a1400d2","1775dcf6-49f1-11ea-bbfa-42010a1400d2","17750a38-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","17753c7e-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17755c54-49f1-11ea-bbfa-42010a1400d2","17757e5a-49f1-11ea-bbfa-42010a1400d2","17759e26-49f1-11ea-bbfa-42010a1400d2","1775bdde-49f1-11ea-bbfa-42010a1400d2","17768444-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","1776238c-49f1-11ea-bbfa-42010a1400d2","177634c6-49f1-11ea-bbfa-42010a1400d2","177645ec-49f1-11ea-bbfa-42010a1400d2","177656cc-49f1-11ea-bbfa-42010a1400d2","17766720-49f1-11ea-bbfa-42010a1400d2","1778ff1c-49f1-11ea-bbfa-42010a1400d2","1778ed42-49f1-11ea-bbfa-42010a1400d2","17785940-49f1-11ea-bbfa-42010a1400d2","17784b12-49f1-11ea-bbfa-42010a1400d2","17775284-49f1-11ea-bbfa-42010a1400d2","17782d26-49f1-11ea-bbfa-42010a1400d2","177762d8-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","177792b2-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","1777b26a-49f1-11ea-bbfa-42010a1400d2","1777d0ce-49f1-11ea-bbfa-42010a1400d2","1777efdc-49f1-11ea-bbfa-42010a1400d2","17780e68-49f1-11ea-bbfa-42010a1400d2","1778da3c-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","17787060-49f1-11ea-bbfa-42010a1400d2","177881c2-49f1-11ea-bbfa-42010a1400d2","17789554-49f1-11ea-bbfa-42010a1400d2","1778a6ac-49f1-11ea-bbfa-42010a1400d2","1778b908-49f1-11ea-bbfa-42010a1400d2","17793f72-49f1-11ea-bbfa-42010a1400d2","17795d22-49f1-11ea-bbfa-42010a1400d2"],"label":["CV_","bundle","CV_Stack","bundle","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","bundle","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Pipeline(Lrnr_screener_randomForest_5_20->Stack)","bundle","Lrnr_screener_randomForest_5_20","Stack","chain","bundle","Lrnr_glm_TRUE","subset","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","Stack","bundle","Lrnr_glm_TRUE","Lrnr_mean","Lrnr_glmnet_NULL_deviance_10_0_100_TRUE","Lrnr_glmnet_NULL_deviance_10_1_100_TRUE","Lrnr_xgboost_20_1_4_0.1","chain","Lrnr_solnp_TRUE_TRUE_FALSE_1e-05"],"level":[1,2,5,6,7,8,9,10,16,11,15,12,13,14,13,13,13,13,9,10,11,11,11,11,11,7,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,8,9,10,11,17,12,16,13,14,15,14,14,14,14,10,11,12,12,12,12,12,4,3],"sequential":[true,true,true,true,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,true,true,true,false,true,true,true,false,false,false,false,false,false,true,true,false,false,false,false,false,true,false],"state":["waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting","waiting","waiting","ready","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","waiting","ready","ready","ready","ready","ready","waiting","waiting"],"group":["none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none","none"]},"edges":{"from":["174d17e4-49f1-11ea-bbfa-42010a1400d2","174d17e4-49f1-11ea-bbfa-42010a1400d2","174d2630-49f1-11ea-bbfa-42010a1400d2","174d2630-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174d58f8-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174d7702-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174d9458-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174db17c-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174dd17a-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174df132-49f1-11ea-bbfa-42010a1400d2","174e0d52-49f1-11ea-bbfa-42010a1400d2","174e1b80-49f1-11ea-bbfa-42010a1400d2","174e30de-49f1-11ea-bbfa-42010a1400d2","174e40d8-49f1-11ea-bbfa-42010a1400d2","174e50b4-49f1-11ea-bbfa-42010a1400d2","174e5ffa-49f1-11ea-bbfa-42010a1400d2","174e72a6-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8f7a-49f1-11ea-bbfa-42010a1400d2","174e9d62-49f1-11ea-bbfa-42010a1400d2","174eaad2-49f1-11ea-bbfa-42010a1400d2","1753c954-49f1-11ea-bbfa-42010a1400d2","1753c954-49f1-11ea-bbfa-42010a1400d2","1753d962-49f1-11ea-bbfa-42010a1400d2","1753d962-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17540dc4-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17542c14-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17544af0-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17546940-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17548948-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","1754a7a2-49f1-11ea-bbfa-42010a1400d2","1754c444-49f1-11ea-bbfa-42010a1400d2","1754d308-49f1-11ea-bbfa-42010a1400d2","1754e906-49f1-11ea-bbfa-42010a1400d2","1754f95a-49f1-11ea-bbfa-42010a1400d2","17550954-49f1-11ea-bbfa-42010a1400d2","17551a16-49f1-11ea-bbfa-42010a1400d2","17552db2-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17554b76-49f1-11ea-bbfa-42010a1400d2","17555990-49f1-11ea-bbfa-42010a1400d2","17556750-49f1-11ea-bbfa-42010a1400d2","17633164-49f1-11ea-bbfa-42010a1400d2","17633164-49f1-11ea-bbfa-42010a1400d2","176344c4-49f1-11ea-bbfa-42010a1400d2","176344c4-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","176382ae-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","1763a7e8-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","1763cc5a-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","1763ed98-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","17640d3c-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","17642cae-49f1-11ea-bbfa-42010a1400d2","17644b76-49f1-11ea-bbfa-42010a1400d2","17645a80-49f1-11ea-bbfa-42010a1400d2","17647cfe-49f1-11ea-bbfa-42010a1400d2","176490d6-49f1-11ea-bbfa-42010a1400d2","1764a83c-49f1-11ea-bbfa-42010a1400d2","1764bc1e-49f1-11ea-bbfa-42010a1400d2","1764ce16-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764eec8-49f1-11ea-bbfa-42010a1400d2","1764fe68-49f1-11ea-bbfa-42010a1400d2","17650fe8-49f1-11ea-bbfa-42010a1400d2","1765c46a-49f1-11ea-bbfa-42010a1400d2","1765c46a-49f1-11ea-bbfa-42010a1400d2","1765d338-49f1-11ea-bbfa-42010a1400d2","1765d338-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","17660da8-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","17662efa-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","1766522c-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","176672f2-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","17669336-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","1766b320-49f1-11ea-bbfa-42010a1400d2","1766d274-49f1-11ea-bbfa-42010a1400d2","1766e606-49f1-11ea-bbfa-42010a1400d2","1766fe84-49f1-11ea-bbfa-42010a1400d2","17671040-49f1-11ea-bbfa-42010a1400d2","176721ac-49f1-11ea-bbfa-42010a1400d2","17673318-49f1-11ea-bbfa-42010a1400d2","17674470-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","176763b0-49f1-11ea-bbfa-42010a1400d2","1767751c-49f1-11ea-bbfa-42010a1400d2","17678462-49f1-11ea-bbfa-42010a1400d2","176945f4-49f1-11ea-bbfa-42010a1400d2","176945f4-49f1-11ea-bbfa-42010a1400d2","17695abc-49f1-11ea-bbfa-42010a1400d2","17695abc-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","17698b68-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","1769a918-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","1769c790-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","1769e7a2-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","176a07be-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","176a2604-49f1-11ea-bbfa-42010a1400d2","176a43fa-49f1-11ea-bbfa-42010a1400d2","176a5318-49f1-11ea-bbfa-42010a1400d2","176a69c0-49f1-11ea-bbfa-42010a1400d2","176a7bd6-49f1-11ea-bbfa-42010a1400d2","176a93fa-49f1-11ea-bbfa-42010a1400d2","176aa67e-49f1-11ea-bbfa-42010a1400d2","176ab808-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ad824-49f1-11ea-bbfa-42010a1400d2","176ae6e8-49f1-11ea-bbfa-42010a1400d2","176af62e-49f1-11ea-bbfa-42010a1400d2","176ba81c-49f1-11ea-bbfa-42010a1400d2","176ba81c-49f1-11ea-bbfa-42010a1400d2","176bb99c-49f1-11ea-bbfa-42010a1400d2","176bb99c-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176bea84-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176c0ae6-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176c2a44-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176c4a10-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176c6c84-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c8b60-49f1-11ea-bbfa-42010a1400d2","176ca96a-49f1-11ea-bbfa-42010a1400d2","176cb770-49f1-11ea-bbfa-42010a1400d2","176cce22-49f1-11ea-bbfa-42010a1400d2","176cdf34-49f1-11ea-bbfa-42010a1400d2","176cf140-49f1-11ea-bbfa-42010a1400d2","176d020c-49f1-11ea-bbfa-42010a1400d2","176d12f6-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d30a6-49f1-11ea-bbfa-42010a1400d2","176d3f42-49f1-11ea-bbfa-42010a1400d2","176d4d48-49f1-11ea-bbfa-42010a1400d2","176e00b2-49f1-11ea-bbfa-42010a1400d2","176e00b2-49f1-11ea-bbfa-42010a1400d2","176e0f08-49f1-11ea-bbfa-42010a1400d2","176e0f08-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e3f28-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e5de6-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e7bfa-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e9a5e-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176eb82c-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176eda14-49f1-11ea-bbfa-42010a1400d2","176ef792-49f1-11ea-bbfa-42010a1400d2","176f06ce-49f1-11ea-bbfa-42010a1400d2","176f1db2-49f1-11ea-bbfa-42010a1400d2","176f2e06-49f1-11ea-bbfa-42010a1400d2","176f3e96-49f1-11ea-bbfa-42010a1400d2","176f4f12-49f1-11ea-bbfa-42010a1400d2","176f5fc0-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f7e6a-49f1-11ea-bbfa-42010a1400d2","176f8f18-49f1-11ea-bbfa-42010a1400d2","176f9dc8-49f1-11ea-bbfa-42010a1400d2","17704b38-49f1-11ea-bbfa-42010a1400d2","17704b38-49f1-11ea-bbfa-42010a1400d2","1770597a-49f1-11ea-bbfa-42010a1400d2","1770597a-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770871a-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770aa06-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770ca9a-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770ea2a-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1771094c-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","177126e8-49f1-11ea-bbfa-42010a1400d2","1771481c-49f1-11ea-bbfa-42010a1400d2","177156d6-49f1-11ea-bbfa-42010a1400d2","17716d1a-49f1-11ea-bbfa-42010a1400d2","17717d0a-49f1-11ea-bbfa-42010a1400d2","17718dea-49f1-11ea-bbfa-42010a1400d2","17719d6c-49f1-11ea-bbfa-42010a1400d2","1771add4-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771c9cc-49f1-11ea-bbfa-42010a1400d2","1771db42-49f1-11ea-bbfa-42010a1400d2","1771ea24-49f1-11ea-bbfa-42010a1400d2","177295dc-49f1-11ea-bbfa-42010a1400d2","177295dc-49f1-11ea-bbfa-42010a1400d2","1772a40a-49f1-11ea-bbfa-42010a1400d2","1772a40a-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","1772d1aa-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","1772f7e8-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","17731cfa-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","17733dfc-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","17735dbe-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","17737bbe-49f1-11ea-bbfa-42010a1400d2","17739bd0-49f1-11ea-bbfa-42010a1400d2","1773adbe-49f1-11ea-bbfa-42010a1400d2","1773c524-49f1-11ea-bbfa-42010a1400d2","1773d668-49f1-11ea-bbfa-42010a1400d2","1773e6d0-49f1-11ea-bbfa-42010a1400d2","1773f85a-49f1-11ea-bbfa-42010a1400d2","17740b2e-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177429f6-49f1-11ea-bbfa-42010a1400d2","1774389c-49f1-11ea-bbfa-42010a1400d2","177449b8-49f1-11ea-bbfa-42010a1400d2","1774fa3e-49f1-11ea-bbfa-42010a1400d2","1774fa3e-49f1-11ea-bbfa-42010a1400d2","17750a38-49f1-11ea-bbfa-42010a1400d2","17750a38-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17753c7e-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17755c54-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17757e5a-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17759e26-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","1775bdde-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","1775dcf6-49f1-11ea-bbfa-42010a1400d2","1775faec-49f1-11ea-bbfa-42010a1400d2","17760910-49f1-11ea-bbfa-42010a1400d2","1776238c-49f1-11ea-bbfa-42010a1400d2","177634c6-49f1-11ea-bbfa-42010a1400d2","177645ec-49f1-11ea-bbfa-42010a1400d2","177656cc-49f1-11ea-bbfa-42010a1400d2","17766720-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","17768444-49f1-11ea-bbfa-42010a1400d2","17769312-49f1-11ea-bbfa-42010a1400d2","1776a136-49f1-11ea-bbfa-42010a1400d2","17775284-49f1-11ea-bbfa-42010a1400d2","17775284-49f1-11ea-bbfa-42010a1400d2","177762d8-49f1-11ea-bbfa-42010a1400d2","177762d8-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","177792b2-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","1777b26a-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","1777d0ce-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","1777efdc-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","17780e68-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","17782d26-49f1-11ea-bbfa-42010a1400d2","17784b12-49f1-11ea-bbfa-42010a1400d2","17785940-49f1-11ea-bbfa-42010a1400d2","17787060-49f1-11ea-bbfa-42010a1400d2","177881c2-49f1-11ea-bbfa-42010a1400d2","17789554-49f1-11ea-bbfa-42010a1400d2","1778a6ac-49f1-11ea-bbfa-42010a1400d2","1778b908-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778da3c-49f1-11ea-bbfa-42010a1400d2","1778ed42-49f1-11ea-bbfa-42010a1400d2","1778ff1c-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17791f56-49f1-11ea-bbfa-42010a1400d2","17792fa0-49f1-11ea-bbfa-42010a1400d2","17792fa0-49f1-11ea-bbfa-42010a1400d2","17793f72-49f1-11ea-bbfa-42010a1400d2","17793f72-49f1-11ea-bbfa-42010a1400d2","17795d22-49f1-11ea-bbfa-42010a1400d2","17796e70-49f1-11ea-bbfa-42010a1400d2"],"to":["174e0d52-49f1-11ea-bbfa-42010a1400d2","174d2630-49f1-11ea-bbfa-42010a1400d2","174df132-49f1-11ea-bbfa-42010a1400d2","174d399a-49f1-11ea-bbfa-42010a1400d2","174d58f8-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174d7702-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174d9458-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174db17c-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174dd17a-49f1-11ea-bbfa-42010a1400d2","174de1ba-49f1-11ea-bbfa-42010a1400d2","174df132-49f1-11ea-bbfa-42010a1400d2","174e0d52-49f1-11ea-bbfa-42010a1400d2","174e1b80-49f1-11ea-bbfa-42010a1400d2","174e9d62-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8066-49f1-11ea-bbfa-42010a1400d2","174e8f7a-49f1-11ea-bbfa-42010a1400d2","174e9d62-49f1-11ea-bbfa-42010a1400d2","174eaad2-49f1-11ea-bbfa-42010a1400d2","17791f56-49f1-11ea-bbfa-42010a1400d2","1754c444-49f1-11ea-bbfa-42010a1400d2","1753d962-49f1-11ea-bbfa-42010a1400d2","1754a7a2-49f1-11ea-bbfa-42010a1400d2","1753ecfe-49f1-11ea-bbfa-42010a1400d2","17540dc4-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","17542c14-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","17544af0-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","17546940-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","17548948-49f1-11ea-bbfa-42010a1400d2","1754982a-49f1-11ea-bbfa-42010a1400d2","1754a7a2-49f1-11ea-bbfa-42010a1400d2","1754c444-49f1-11ea-bbfa-42010a1400d2","1754d308-49f1-11ea-bbfa-42010a1400d2","17555990-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17553bc2-49f1-11ea-bbfa-42010a1400d2","17554b76-49f1-11ea-bbfa-42010a1400d2","17555990-49f1-11ea-bbfa-42010a1400d2","17556750-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17644b76-49f1-11ea-bbfa-42010a1400d2","176344c4-49f1-11ea-bbfa-42010a1400d2","17642cae-49f1-11ea-bbfa-42010a1400d2","1763570c-49f1-11ea-bbfa-42010a1400d2","176382ae-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","1763a7e8-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","1763cc5a-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","1763ed98-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","17640d3c-49f1-11ea-bbfa-42010a1400d2","17641ce6-49f1-11ea-bbfa-42010a1400d2","17642cae-49f1-11ea-bbfa-42010a1400d2","17644b76-49f1-11ea-bbfa-42010a1400d2","17645a80-49f1-11ea-bbfa-42010a1400d2","1764fe68-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764dd7a-49f1-11ea-bbfa-42010a1400d2","1764eec8-49f1-11ea-bbfa-42010a1400d2","1764fe68-49f1-11ea-bbfa-42010a1400d2","17650fe8-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","1766d274-49f1-11ea-bbfa-42010a1400d2","1765d338-49f1-11ea-bbfa-42010a1400d2","1766b320-49f1-11ea-bbfa-42010a1400d2","1765e9d6-49f1-11ea-bbfa-42010a1400d2","17660da8-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","17662efa-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","1766522c-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","176672f2-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","17669336-49f1-11ea-bbfa-42010a1400d2","1766a24a-49f1-11ea-bbfa-42010a1400d2","1766b320-49f1-11ea-bbfa-42010a1400d2","1766d274-49f1-11ea-bbfa-42010a1400d2","1766e606-49f1-11ea-bbfa-42010a1400d2","1767751c-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","17675302-49f1-11ea-bbfa-42010a1400d2","176763b0-49f1-11ea-bbfa-42010a1400d2","1767751c-49f1-11ea-bbfa-42010a1400d2","17678462-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","176a43fa-49f1-11ea-bbfa-42010a1400d2","17695abc-49f1-11ea-bbfa-42010a1400d2","176a2604-49f1-11ea-bbfa-42010a1400d2","17696cc8-49f1-11ea-bbfa-42010a1400d2","17698b68-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","1769a918-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","1769c790-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","1769e7a2-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","176a07be-49f1-11ea-bbfa-42010a1400d2","176a16a0-49f1-11ea-bbfa-42010a1400d2","176a2604-49f1-11ea-bbfa-42010a1400d2","176a43fa-49f1-11ea-bbfa-42010a1400d2","176a5318-49f1-11ea-bbfa-42010a1400d2","176ae6e8-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ac780-49f1-11ea-bbfa-42010a1400d2","176ad824-49f1-11ea-bbfa-42010a1400d2","176ae6e8-49f1-11ea-bbfa-42010a1400d2","176af62e-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","176ca96a-49f1-11ea-bbfa-42010a1400d2","176bb99c-49f1-11ea-bbfa-42010a1400d2","176c8b60-49f1-11ea-bbfa-42010a1400d2","176bca86-49f1-11ea-bbfa-42010a1400d2","176bea84-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c0ae6-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c2a44-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c4a10-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c6c84-49f1-11ea-bbfa-42010a1400d2","176c7bde-49f1-11ea-bbfa-42010a1400d2","176c8b60-49f1-11ea-bbfa-42010a1400d2","176ca96a-49f1-11ea-bbfa-42010a1400d2","176cb770-49f1-11ea-bbfa-42010a1400d2","176d3f42-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d20d4-49f1-11ea-bbfa-42010a1400d2","176d30a6-49f1-11ea-bbfa-42010a1400d2","176d3f42-49f1-11ea-bbfa-42010a1400d2","176d4d48-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","176ef792-49f1-11ea-bbfa-42010a1400d2","176e0f08-49f1-11ea-bbfa-42010a1400d2","176eda14-49f1-11ea-bbfa-42010a1400d2","176e1e9e-49f1-11ea-bbfa-42010a1400d2","176e3f28-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176e5de6-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176e7bfa-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176e9a5e-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176eb82c-49f1-11ea-bbfa-42010a1400d2","176ec9ac-49f1-11ea-bbfa-42010a1400d2","176eda14-49f1-11ea-bbfa-42010a1400d2","176ef792-49f1-11ea-bbfa-42010a1400d2","176f06ce-49f1-11ea-bbfa-42010a1400d2","176f8f18-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f6f38-49f1-11ea-bbfa-42010a1400d2","176f7e6a-49f1-11ea-bbfa-42010a1400d2","176f8f18-49f1-11ea-bbfa-42010a1400d2","176f9dc8-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","1771481c-49f1-11ea-bbfa-42010a1400d2","1770597a-49f1-11ea-bbfa-42010a1400d2","177126e8-49f1-11ea-bbfa-42010a1400d2","177068f2-49f1-11ea-bbfa-42010a1400d2","1770871a-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","1770aa06-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","1770ca9a-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","1770ea2a-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","1771094c-49f1-11ea-bbfa-42010a1400d2","177117f2-49f1-11ea-bbfa-42010a1400d2","177126e8-49f1-11ea-bbfa-42010a1400d2","1771481c-49f1-11ea-bbfa-42010a1400d2","177156d6-49f1-11ea-bbfa-42010a1400d2","1771db42-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771bb08-49f1-11ea-bbfa-42010a1400d2","1771c9cc-49f1-11ea-bbfa-42010a1400d2","1771db42-49f1-11ea-bbfa-42010a1400d2","1771ea24-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17739bd0-49f1-11ea-bbfa-42010a1400d2","1772a40a-49f1-11ea-bbfa-42010a1400d2","17737bbe-49f1-11ea-bbfa-42010a1400d2","1772b382-49f1-11ea-bbfa-42010a1400d2","1772d1aa-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","1772f7e8-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","17731cfa-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","17733dfc-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","17735dbe-49f1-11ea-bbfa-42010a1400d2","17736c82-49f1-11ea-bbfa-42010a1400d2","17737bbe-49f1-11ea-bbfa-42010a1400d2","17739bd0-49f1-11ea-bbfa-42010a1400d2","1773adbe-49f1-11ea-bbfa-42010a1400d2","1774389c-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177419c0-49f1-11ea-bbfa-42010a1400d2","177429f6-49f1-11ea-bbfa-42010a1400d2","1774389c-49f1-11ea-bbfa-42010a1400d2","177449b8-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","1775faec-49f1-11ea-bbfa-42010a1400d2","17750a38-49f1-11ea-bbfa-42010a1400d2","1775dcf6-49f1-11ea-bbfa-42010a1400d2","17751b04-49f1-11ea-bbfa-42010a1400d2","17753c7e-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","17755c54-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","17757e5a-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","17759e26-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","1775bdde-49f1-11ea-bbfa-42010a1400d2","1775cd74-49f1-11ea-bbfa-42010a1400d2","1775dcf6-49f1-11ea-bbfa-42010a1400d2","1775faec-49f1-11ea-bbfa-42010a1400d2","17760910-49f1-11ea-bbfa-42010a1400d2","17769312-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","177674a4-49f1-11ea-bbfa-42010a1400d2","17768444-49f1-11ea-bbfa-42010a1400d2","17769312-49f1-11ea-bbfa-42010a1400d2","1776a136-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17784b12-49f1-11ea-bbfa-42010a1400d2","177762d8-49f1-11ea-bbfa-42010a1400d2","17782d26-49f1-11ea-bbfa-42010a1400d2","177772c8-49f1-11ea-bbfa-42010a1400d2","177792b2-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","1777b26a-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","1777d0ce-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","1777efdc-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","17780e68-49f1-11ea-bbfa-42010a1400d2","17781d7c-49f1-11ea-bbfa-42010a1400d2","17782d26-49f1-11ea-bbfa-42010a1400d2","17784b12-49f1-11ea-bbfa-42010a1400d2","17785940-49f1-11ea-bbfa-42010a1400d2","1778ed42-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778c830-49f1-11ea-bbfa-42010a1400d2","1778da3c-49f1-11ea-bbfa-42010a1400d2","1778ed42-49f1-11ea-bbfa-42010a1400d2","1778ff1c-49f1-11ea-bbfa-42010a1400d2","17790ea8-49f1-11ea-bbfa-42010a1400d2","17791f56-49f1-11ea-bbfa-42010a1400d2","17792fa0-49f1-11ea-bbfa-42010a1400d2","17796e70-49f1-11ea-bbfa-42010a1400d2","17793f72-49f1-11ea-bbfa-42010a1400d2","17796e70-49f1-11ea-bbfa-42010a1400d2","17795d22-49f1-11ea-bbfa-42010a1400d2","17796e70-49f1-11ea-bbfa-42010a1400d2","17797e38-49f1-11ea-bbfa-42010a1400d2"],"label":["","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot"},"manipulation":{"enabled":false},"edges":{"arrows":"to"},"layout":{"hierarchical":{"enabled":true,"levelSeparation":500,"nodeSpacing":200,"direction":"RL"}},"groups":{"useDefaultGroups":true,"none":{"color":{"border":"black","background":"white"}}}},"groups":["none"],"width":"100%","height":"400px","idselection":{"enabled":false},"byselection":{"enabled":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="train-the-super-learner-on-the-machine-learning-task" class="section level3 unnumbered">
<h3>3. Train the Super Learner on the machine learning task</h3>
<p>The Super Learner algorithm fits a metalearner on the validation-set
predictions in a cross-validated manner, thereby avoiding overfitting.</p>
<p>Now we are ready to <strong>“train”</strong> our Super Learner on our <code>sl3_task</code> object,
<code>washb_task</code>.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">sl_fit &lt;-<span class="st"> </span>sl<span class="op">$</span><span class="kw">train</span>(washb_task)</a></code></pre></div>
</div>
<div id="obtain-predicted-values" class="section level3 unnumbered">
<h3>4. Obtain predicted values</h3>
<p>Now that we have fit the super learner, we are ready to calculate the predicted
outcome for each subject.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="co"># we did it! now we have super learner predictions</span></a>
<a class="sourceLine" id="cb35-2" data-line-number="2">sl_preds &lt;-<span class="st"> </span>sl_fit<span class="op">$</span><span class="kw">predict</span>()</a>
<a class="sourceLine" id="cb35-3" data-line-number="3"><span class="kw">head</span>(sl_preds)</a></code></pre></div>
<pre><code>[1] -0.6552463 -0.7620363 -0.6536599 -0.6462727 -0.6185737 -0.6811842</code></pre>
<!--
Below we visualize the observed versus predicted values. 

For fun, we will also
include the cross-validated predictions from most popular learner on the block, 
main terms linear regression. 



```r
df_plot <- data.frame(Observed = washb_data$whz, 
                      Predicted = sl_preds,
                      count = c(1:nrow(washb_data)))
                      
df_plot_melted <- melt(df_plot, 
                       id.vars = "count", 
                       measure.vars = c("Observed", "Predicted"))
```

```
Warning in melt(df_plot, id.vars = "count", measure.vars = c("Observed", :
The melt generic in data.table has been passed a data.frame and will attempt
to redirect to the relevant reshape2 method; please note that reshape2 is
deprecated, and this redirection is now deprecated as well. To continue using
melt methods from reshape2 while both libraries are attached, e.g. melt.list,
you can prepend the namespace like reshape2::melt(df_plot). In the next version,
this warning will become an error.
```

```r
ggplot(df_plot_melted, aes(value, count, color = variable)) + geom_point() 
```

<img src="handbook_files/figure-html/plot-predvobs-woohoo-1.png" width="    extwidth" style="display: block; margin: auto;" />
-->
<p>We can also obtain a summary of the results.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">sl_fit_summary &lt;-<span class="st"> </span>sl_fit<span class="op">$</span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>[1] &quot;SuperLearner:&quot;
List of 2
 $ : chr &quot;Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)&quot;
 $ : chr &quot;Stack&quot;
[1] &quot;Lrnr_solnp_TRUE_TRUE_FALSE_1e-05&quot;
$pars
 [1] 8.293560e-05 1.863897e-05 8.568024e-05 7.309641e-05 2.306510e-01
 [6] 2.833343e-01 1.863897e-05 1.984075e-01 2.872964e-01 3.174670e-05

$convergence
[1] 0

$values
[1] 1.019909 1.009929 1.009919 1.009912

$lagrange
            [,1]
[1,] -0.04655413

$hessian
           [,1]        [,2]        [,3]        [,4]        [,5]        [,6]
 [1,] 1.0055866  0.13706318  0.10833574  0.14947677  0.17349836  0.43795340
 [2,] 0.1370632  0.74007983  0.09002864  0.06616989 -0.01119723  0.08403612
 [3,] 0.1083357  0.09002864  0.98977880 -0.05242687  0.14097932  0.39783768
 [4,] 0.1494768  0.06616989 -0.05242687  0.87067622  0.10368371  0.36065304
 [5,] 0.1734984 -0.01119723  0.14097932  0.10368371  0.29613698  0.10998158
 [6,] 0.4379534  0.08403612  0.39783768  0.36065304  0.10998158  0.88516264
 [7,] 0.1370632 -0.25992017  0.09002864  0.06616989 -0.01119723  0.08403612
 [8,] 0.3471744  0.05814308  0.31366991  0.27536650  0.03497104 -0.01063976
 [9,] 0.4200326  0.14954198  0.30784801  0.28501724  0.12736781  0.25255353
[10,] 0.1514691  0.03133613  0.12547494  0.09015151  0.32556055  0.03515649
             [,7]         [,8]      [,9]        [,10]
 [1,]  0.13706318  0.347174439 0.4200326  0.151469085
 [2,] -0.25992017  0.058143078 0.1495420  0.031336129
 [3,]  0.09002864  0.313669914 0.3078480  0.125474936
 [4,]  0.06616989  0.275366498 0.2850172  0.090151510
 [5,] -0.01119723  0.034971044 0.1273678  0.325560549
 [6,]  0.08403612 -0.010639760 0.2525535  0.035156492
 [7,]  0.74007983  0.058143078 0.1495420  0.031336129
 [8,]  0.05814308  1.018137289 0.3512299 -0.007925275
 [9,]  0.14954198  0.351229948 0.9203863  0.506528702
[10,]  0.03133613 -0.007925275 0.5065287  0.652145512

$ineqx0
NULL

$nfuneval
[1] 225

$outer.iter
[1] 3

$elapsed
Time difference of 0.06281233 secs

$vscale
 [1] 1.009919 0.000010 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000
 [9] 1.000000 1.000000 1.000000 1.000000

$coefficients
                          Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glm_TRUE 
                                                                               0.0000000 
                              Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_mean 
                                                                               0.0000000 
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE 
                                                                               0.0000000 
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE 
                                                                               0.0000000 
                Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_xgboost_20_1_4_0.1 
                                                                               0.2307227 
                                                                     Stack_Lrnr_glm_TRUE 
                                                                               0.2834224 
                                                                         Stack_Lrnr_mean 
                                                                               0.0000000 
                                           Stack_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE 
                                                                               0.1984692 
                                           Stack_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE 
                                                                               0.2873857 
                                                           Stack_Lrnr_xgboost_20_1_4_0.1 
                                                                               0.0000000 

$training_offset
[1] FALSE

$name
[1] &quot;solnp&quot;

[1] &quot;Cross-validated risk (MSE, squared error loss):&quot;
                                                                                     learner
 1:                           Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glm_TRUE
 2:                               Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_mean
 3: Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE
 4: Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE
 5:                 Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_xgboost_20_1_4_0.1
 6:                                                                      Stack_Lrnr_glm_TRUE
 7:                                                                          Stack_Lrnr_mean
 8:                                            Stack_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE
 9:                                            Stack_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE
10:                                                            Stack_Lrnr_xgboost_20_1_4_0.1
11:                                                                             SuperLearner
    coefficients mean_risk    SE_risk    fold_SD fold_min_risk fold_max_risk
 1:    0.0000000  1.035485 0.02446142 0.06008226     0.9352596      1.119394
 2:    0.0000000  1.065401 0.02503198 0.05999366     0.9689145      1.143488
 3:    0.0000000  1.035503 0.02446220 0.06007976     0.9354275      1.118983
 4:    0.0000000  1.035640 0.02446288 0.06036504     0.9352523      1.119315
 5:    0.2307227  1.044729 0.02405570 0.06265341     0.9211017      1.117049
 6:    0.2834224  1.018949 0.02372195 0.05817436     0.9095780      1.088981
 7:    0.0000000  1.065401 0.02503198 0.05999366     0.9689145      1.143488
 8:    0.1984692  1.014209 0.02361288 0.05599624     0.9196952      1.093245
 9:    0.2873857  1.012309 0.02348067 0.05725410     0.9184286      1.095675
10:    0.0000000  1.035503 0.02371762 0.06206027     0.9341196      1.119005
11:           NA  1.009925 0.02345064 0.05810586     0.9055711      1.087765</code></pre>
<p>From the table of the printed Super Learner fit, we note that the Super Learner
had a mean risk of 1.0099253
and that this ensemble weighted the <code>ranger</code> and <code>glmnet</code> learners highest while
not weighting the <code>mean</code> learner highly.</p>
<p>We can also see that the <code>glmnet</code> learner had the lowest cross-validated mean
risk, thus making it the cross-validated selector (or the <em>discrete</em> Super
Learner). The mean risk of the Super Learner is calculated using the hold-out
set that we visualized in the <code>dt_sl</code> plot.</p>
</div>
</div>
<div id="cross-validated-super-learner" class="section level2">
<h2><span class="header-section-number">4.5</span> Cross-validated Super Learner</h2>
<p>We can cross-validate the Super Learner to see how well the Super Learner
performs on unseen data, and obtain an estimate of the cross-validated risk of
the Super Learner.</p>
<p>This estimation procedure requires an “external” layer of cross-validation,
also called nested cross-validation, which involves setting aside a separate
holdout sample that we don’t use to fit the Super Learner. This
external cross validation procedure may also incorporate 10 folds, which is the
default in <code>sl3</code>. However, we will incorporate 2 outer/external folds of
cross-validation for computational efficiency.</p>
<p>We also need to specify a loss function to evaluate Super Learner.
Documentation for the available loss functions can be found in the <a href="https://tlverse.org/sl3/reference/loss_functions.html"><code>sl3</code> Loss
Function Reference</a>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">washb_task_new &lt;-<span class="st"> </span><span class="kw">make_sl3_Task</span>(</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">  <span class="dt">data =</span> washb_data,</a>
<a class="sourceLine" id="cb39-3" data-line-number="3">  <span class="dt">covariates =</span> covars,</a>
<a class="sourceLine" id="cb39-4" data-line-number="4">  <span class="dt">outcome =</span> outcome,</a>
<a class="sourceLine" id="cb39-5" data-line-number="5">  <span class="dt">folds =</span> <span class="kw">make_folds</span>(washb_data, <span class="dt">fold_fun =</span> folds_vfold, <span class="dt">V =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb39-6" data-line-number="6">)</a></code></pre></div>
<pre><code>Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1">CVsl &lt;-<span class="st"> </span><span class="kw">CV_lrnr_sl</span>(sl_fit, washb_task_new, loss_squared_error)</a>
<a class="sourceLine" id="cb41-2" data-line-number="2">CVsl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb41-3" data-line-number="3"><span class="st">  </span><span class="kw">kable</span>(<span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb41-4" data-line-number="4"><span class="st">  </span>kableExtra<span class="op">:::</span><span class="kw">kable_styling</span>(<span class="dt">fixed_thead =</span> T) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb41-5" data-line-number="5"><span class="st">  </span><span class="kw">scroll_box</span>(<span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>, <span class="dt">height =</span> <span class="st">&quot;300px&quot;</span>)</a></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; ">
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
learner
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
coefficients
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
mean_risk
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
SE_risk
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
fold_SD
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
fold_min_risk
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
fold_max_risk
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glm_TRUE
</td>
<td style="text-align:right;">
0.0410
</td>
<td style="text-align:right;">
1.0363
</td>
<td style="text-align:right;">
0.0245
</td>
<td style="text-align:right;">
0.0294
</td>
<td style="text-align:right;">
1.0155
</td>
<td style="text-align:right;">
1.0571
</td>
</tr>
<tr>
<td style="text-align:left;">
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_mean
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
1.0653
</td>
<td style="text-align:right;">
0.0250
</td>
<td style="text-align:right;">
0.0342
</td>
<td style="text-align:right;">
1.0411
</td>
<td style="text-align:right;">
1.0894
</td>
</tr>
<tr>
<td style="text-align:left;">
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
1.0367
</td>
<td style="text-align:right;">
0.0245
</td>
<td style="text-align:right;">
0.0303
</td>
<td style="text-align:right;">
1.0152
</td>
<td style="text-align:right;">
1.0581
</td>
</tr>
<tr>
<td style="text-align:left;">
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE
</td>
<td style="text-align:right;">
0.0095
</td>
<td style="text-align:right;">
1.0362
</td>
<td style="text-align:right;">
0.0245
</td>
<td style="text-align:right;">
0.0302
</td>
<td style="text-align:right;">
1.0149
</td>
<td style="text-align:right;">
1.0576
</td>
</tr>
<tr>
<td style="text-align:left;">
Pipeline(Lrnr_screener_randomForest_5_20-&gt;Stack)_Lrnr_xgboost_20_1_4_0.1
</td>
<td style="text-align:right;">
0.0986
</td>
<td style="text-align:right;">
1.0559
</td>
<td style="text-align:right;">
0.0242
</td>
<td style="text-align:right;">
0.0154
</td>
<td style="text-align:right;">
1.0450
</td>
<td style="text-align:right;">
1.0668
</td>
</tr>
<tr>
<td style="text-align:left;">
Stack_Lrnr_glm_TRUE
</td>
<td style="text-align:right;">
0.1660
</td>
<td style="text-align:right;">
1.0293
</td>
<td style="text-align:right;">
0.0238
</td>
<td style="text-align:right;">
0.0303
</td>
<td style="text-align:right;">
1.0079
</td>
<td style="text-align:right;">
1.0507
</td>
</tr>
<tr>
<td style="text-align:left;">
Stack_Lrnr_mean
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
1.0653
</td>
<td style="text-align:right;">
0.0250
</td>
<td style="text-align:right;">
0.0342
</td>
<td style="text-align:right;">
1.0411
</td>
<td style="text-align:right;">
1.0894
</td>
</tr>
<tr>
<td style="text-align:left;">
Stack_Lrnr_glmnet_NULL_deviance_10_0_100_TRUE
</td>
<td style="text-align:right;">
0.2272
</td>
<td style="text-align:right;">
1.0183
</td>
<td style="text-align:right;">
0.0238
</td>
<td style="text-align:right;">
0.0337
</td>
<td style="text-align:right;">
0.9945
</td>
<td style="text-align:right;">
1.0421
</td>
</tr>
<tr>
<td style="text-align:left;">
Stack_Lrnr_glmnet_NULL_deviance_10_1_100_TRUE
</td>
<td style="text-align:right;">
0.3696
</td>
<td style="text-align:right;">
1.0160
</td>
<td style="text-align:right;">
0.0237
</td>
<td style="text-align:right;">
0.0365
</td>
<td style="text-align:right;">
0.9902
</td>
<td style="text-align:right;">
1.0418
</td>
</tr>
<tr>
<td style="text-align:left;">
Stack_Lrnr_xgboost_20_1_4_0.1
</td>
<td style="text-align:right;">
0.0881
</td>
<td style="text-align:right;">
1.0370
</td>
<td style="text-align:right;">
0.0237
</td>
<td style="text-align:right;">
0.0194
</td>
<td style="text-align:right;">
1.0233
</td>
<td style="text-align:right;">
1.0507
</td>
</tr>
<tr>
<td style="text-align:left;">
SuperLearner
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
1.0183
</td>
<td style="text-align:right;">
0.0237
</td>
<td style="text-align:right;">
0.0304
</td>
<td style="text-align:right;">
0.9968
</td>
<td style="text-align:right;">
1.0398
</td>
</tr>
</tbody>
</table>
</div>
<!-- Explain summary!!!! -->
</div>
<div id="variable-importance-measures-with-sl3" class="section level2">
<h2><span class="header-section-number">4.6</span> Variable Importance Measures with <code>sl3</code></h2>
<p>Variable importance can be interesting and informative. It can also be
contradictory and confusing. Nevertheless, we like it, and so do
collaborators, so we created a variable importance function in <code>sl3</code>! The <code>sl3</code>
<code>varimp</code> function returns a table with variables listed in decreasing order of
importance (i.e. most important on the first row).</p>
<p>The measure of importance in <code>sl3</code> is based on a risk difference between the
learner fit with a permuted covariate and the learner fit with the true
covariate, across all covariates. In this manner, the larger the risk
difference, the more important the variable is in the prediction.</p>
<p>The intuition of this measure is that it calculates the risk (in terms of the
average loss in predictive accuracy) of losing one covariate, while keeping
everything else fixed, and compares it to the risk if the covariate was not
lost. If this risk difference is zero then losing that covariate had no
impact, and is thus not important by this measure. We do this across all of the
covariates. As stated above, we don’t actually remove the covariate, we just
permute/shuffle it, but the idea is that this shuffling distorts potentially
meaningful information that was present in the covariate. This idea of permuting
instead of removing saves a lot of time, and is also incorporated in the
<code>randomForest</code> variable importance measures.</p>
<p>Let’s explore the <code>sl3</code> variable importance measurements for the <code>washb</code> data.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">washb_varimp &lt;-<span class="st"> </span><span class="kw">varimp</span>(sl_fit, loss_squared_error)</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">washb_varimp <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb42-3" data-line-number="3"><span class="st">  </span><span class="kw">kable</span>(<span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb42-4" data-line-number="4"><span class="st">  </span>kableExtra<span class="op">:::</span><span class="kw">kable_styling</span>(<span class="dt">fixed_thead =</span> T) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb42-5" data-line-number="5"><span class="st">  </span><span class="kw">scroll_box</span>(<span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>, <span class="dt">height =</span> <span class="st">&quot;300px&quot;</span>)</a></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; ">
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
X
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
risk_diff
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
aged
</td>
<td style="text-align:right;">
0.0367
</td>
</tr>
<tr>
<td style="text-align:left;">
momedu
</td>
<td style="text-align:right;">
0.0082
</td>
</tr>
<tr>
<td style="text-align:left;">
momheight
</td>
<td style="text-align:right;">
0.0044
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_chair
</td>
<td style="text-align:right;">
0.0041
</td>
</tr>
<tr>
<td style="text-align:left;">
month
</td>
<td style="text-align:right;">
0.0028
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_refrig
</td>
<td style="text-align:right;">
0.0027
</td>
</tr>
<tr>
<td style="text-align:left;">
Nlt18
</td>
<td style="text-align:right;">
0.0016
</td>
</tr>
<tr>
<td style="text-align:left;">
floor
</td>
<td style="text-align:right;">
0.0015
</td>
</tr>
<tr>
<td style="text-align:left;">
momage
</td>
<td style="text-align:right;">
0.0011
</td>
</tr>
<tr>
<td style="text-align:left;">
elec
</td>
<td style="text-align:right;">
0.0010
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_mobile
</td>
<td style="text-align:right;">
0.0010
</td>
</tr>
<tr>
<td style="text-align:left;">
tr
</td>
<td style="text-align:right;">
0.0009
</td>
</tr>
<tr>
<td style="text-align:left;">
walls
</td>
<td style="text-align:right;">
0.0009
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_sewmach
</td>
<td style="text-align:right;">
0.0008
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_chouki
</td>
<td style="text-align:right;">
0.0004
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_khat
</td>
<td style="text-align:right;">
0.0003
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_wardrobe
</td>
<td style="text-align:right;">
0.0003
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_table
</td>
<td style="text-align:right;">
0.0003
</td>
</tr>
<tr>
<td style="text-align:left;">
hfiacat
</td>
<td style="text-align:right;">
0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
delta_momheight
</td>
<td style="text-align:right;">
0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_tv
</td>
<td style="text-align:right;">
0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_moto
</td>
<td style="text-align:right;">
0.0000
</td>
</tr>
<tr>
<td style="text-align:left;">
roof
</td>
<td style="text-align:right;">
0.0000
</td>
</tr>
<tr>
<td style="text-align:left;">
sex
</td>
<td style="text-align:right;">
-0.0001
</td>
</tr>
<tr>
<td style="text-align:left;">
asset_bike
</td>
<td style="text-align:right;">
-0.0002
</td>
</tr>
<tr>
<td style="text-align:left;">
Ncomp
</td>
<td style="text-align:right;">
-0.0003
</td>
</tr>
<tr>
<td style="text-align:left;">
delta_momage
</td>
<td style="text-align:right;">
-0.0003
</td>
</tr>
<tr>
<td style="text-align:left;">
watmin
</td>
<td style="text-align:right;">
-0.0010
</td>
</tr>
<tr>
<td style="text-align:left;">
fracode
</td>
<td style="text-align:right;">
-0.0017
</td>
</tr>
</tbody>
</table>
</div>
<!-- Explain summary!!!! -->
<div id="international-stroke-trial-example" class="section level3 unnumbered">
<h3>International Stroke Trial Example</h3>
<p>Using the IST data, we are interested in predicting recurrent stroke <code>DRSISC</code>
using the available covariate data.</p>
<p>MORE COMING SOON. STAY TUNED :)</p>
</div>
</div>
<div id="exercise" class="section level2">
<h2><span class="header-section-number">4.7</span> Exercise</h2>
<div id="sl3ex" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Predicting Myocardial Infarction with <code>sl3</code></h3>
<p>Follow the steps below to predict myocardial infarction (<code>mi</code>) using the
available covariate data. We thank Prof. David Benkeser at Emory University for
making the this Cardiovascular Health Study (CHS) data accessible.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="co"># load the data set</span></a>
<a class="sourceLine" id="cb43-2" data-line-number="2">db_data &lt;-</a>
<a class="sourceLine" id="cb43-3" data-line-number="3"><span class="st"> </span><span class="kw">url</span>(<span class="st">&quot;https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv&quot;</span>)</a>
<a class="sourceLine" id="cb43-4" data-line-number="4">chspred &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> db_data, <span class="dt">col_names =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb43-5" data-line-number="5"><span class="co"># take a quick peek</span></a>
<a class="sourceLine" id="cb43-6" data-line-number="6"><span class="kw">head</span>(chspred) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb43-7" data-line-number="7"><span class="st">  </span><span class="kw">kable</span>(<span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb43-8" data-line-number="8"><span class="st">  </span>kableExtra<span class="op">:::</span><span class="kw">kable_styling</span>(<span class="dt">fixed_thead =</span> T) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb43-9" data-line-number="9"><span class="st">  </span><span class="kw">scroll_box</span>(<span class="dt">width =</span> <span class="st">&quot;100%&quot;</span>, <span class="dt">height =</span> <span class="st">&quot;300px&quot;</span>)</a></code></pre></div>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; ">
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
waist
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
alcoh
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
hdl
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
beta
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
smoke
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
ace
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
ldl
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
bmi
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
aspirin
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
gend
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
age
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
estrgn
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
glu
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
ins
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
cysgfr
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
dm
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
fetuina
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
whr
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
hsed
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
race
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
logcystat
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
logtrig
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
logcrp
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
logcre
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
health
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
logkcal
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
sysbp
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;position: sticky; top:0; background-color: #FFFFFF;">
mi
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
110.1642
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
66.4974
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
114.2162
</td>
<td style="text-align:right;">
27.9975
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
73.5179
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
159.9314
</td>
<td style="text-align:right;">
70.3343
</td>
<td style="text-align:right;">
75.0078
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.1752
</td>
<td style="text-align:right;">
1.1690
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-0.3420
</td>
<td style="text-align:right;">
5.4063
</td>
<td style="text-align:right;">
2.0126
</td>
<td style="text-align:right;">
-0.6739
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4.3926
</td>
<td style="text-align:right;">
177.1345
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
89.9763
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
50.0652
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
103.7766
</td>
<td style="text-align:right;">
20.8931
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
61.7723
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
153.3888
</td>
<td style="text-align:right;">
33.9695
</td>
<td style="text-align:right;">
82.7433
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5717
</td>
<td style="text-align:right;">
0.9011
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.0847
</td>
<td style="text-align:right;">
4.8592
</td>
<td style="text-align:right;">
3.2933
</td>
<td style="text-align:right;">
-0.5551
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6.2071
</td>
<td style="text-align:right;">
136.3742
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
106.1941
</td>
<td style="text-align:right;">
8.4174
</td>
<td style="text-align:right;">
40.5059
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
165.7158
</td>
<td style="text-align:right;">
28.4554
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
72.9312
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
121.7145
</td>
<td style="text-align:right;">
-17.3017
</td>
<td style="text-align:right;">
74.6989
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.3517
</td>
<td style="text-align:right;">
1.1797
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-0.4451
</td>
<td style="text-align:right;">
4.5088
</td>
<td style="text-align:right;">
0.3013
</td>
<td style="text-align:right;">
-0.0115
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
6.7320
</td>
<td style="text-align:right;">
135.1993
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
90.0566
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
36.1750
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
45.2035
</td>
<td style="text-align:right;">
23.9608
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
79.1191
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
53.9691
</td>
<td style="text-align:right;">
11.7315
</td>
<td style="text-align:right;">
95.7823
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.5439
</td>
<td style="text-align:right;">
1.1360
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.4807
</td>
<td style="text-align:right;">
5.1832
</td>
<td style="text-align:right;">
3.0243
</td>
<td style="text-align:right;">
-0.5751
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
7.3972
</td>
<td style="text-align:right;">
139.0182
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
78.6143
</td>
<td style="text-align:right;">
2.9790
</td>
<td style="text-align:right;">
71.0642
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
131.3121
</td>
<td style="text-align:right;">
10.9656
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
69.0179
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
94.3153
</td>
<td style="text-align:right;">
9.7112
</td>
<td style="text-align:right;">
72.7109
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.4916
</td>
<td style="text-align:right;">
1.1028
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.3121
</td>
<td style="text-align:right;">
4.2190
</td>
<td style="text-align:right;">
-0.7057
</td>
<td style="text-align:right;">
0.0053
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.2779
</td>
<td style="text-align:right;">
88.0470
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
91.6593
</td>
<td style="text-align:right;">
0.0000
</td>
<td style="text-align:right;">
59.4963
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
171.1872
</td>
<td style="text-align:right;">
29.1317
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
81.8346
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
212.9066
</td>
<td style="text-align:right;">
-28.2269
</td>
<td style="text-align:right;">
69.2184
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.4621
</td>
<td style="text-align:right;">
0.9529
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.2872
</td>
<td style="text-align:right;">
5.1773
</td>
<td style="text-align:right;">
0.9705
</td>
<td style="text-align:right;">
0.2127
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5.9942
</td>
<td style="text-align:right;">
69.5943
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
</div>
<ol style="list-style-type: decimal">
<li>Create an <code>sl3</code> task, setting myocardial infarction <code>mi</code> as the outcome and
using all available covariate data.</li>
<li>Make a library of seven relatively fast base learning algorithms (i.e., do
not consider BART or HAL). Customize hyperparameters for one of your
learners. Feel free to use learners from <code>sl3</code> or <code>SuperLearner</code>. You may
use the same base learning library that is presented above.</li>
<li>Incorporate feature selection with the <code>SuperLearner</code> screener <code>screen.corP</code>.</li>
<li>Fit the metalearning step with non-negative least squares, <code>Lrnr_nnls</code>.</li>
<li>With the metalearner and base learners, make the Super Learner and train it
on the task.</li>
<li>Print your Super Learner fit by calling <code>print()</code> with <code>$</code>.</li>
<li>Cross-validate your Super Learner fit to see how well it performs on unseen
data. Specify <code>loss_squared_error</code> as the loss function to evaluate the
Super Learner.</li>
</ol>
<!--
## Super Learning of a Conditional Density

### Super learning of a conditional density

Suppose we want to construct a Super Learner of the conditional probability
distribution $g_0(a\mid W)=P_0(A=a\mid W)$, where $a\in {\cal A}$.
Let's denote the values of $a$ with $\{0,1,\ldots,K\}$. A valid loss function
for the conditional density is
\[
L(g)(O)=-\log g(A\mid W).\]
That is, $g_0=\arg\min_g P_0L(g)$, i.e., $g_0$ is the minimizer of the
expectation of the log-likelihood loss.

**Candidate estimators**

1. Candidate estimators based on multinomial logistic regression: To start
with, one can use existing parametric model based MLE and machine learning
algorithms in `R` that fit a multinomial regression. For example, parametric
model multinomial logistic regression is available in `R` so that one can
already build a rich library of such estimators based on  different candidate
parametric models. In addition, `polyclass()` is a multinomial logistic
regression machine learning algorithm in `R`.

2. Candidate estimators based on machine learning for multinomial logistic
regression: Secondly, one can use a machine learning algorithm such as
`polyclass()` in `R` that data adaptively fits a multinomial logistic
regression, which itself has tuning parameters, again generating a class of
candidate estimators.

3. Incorporating screening: Note that one can also marry any of these choices
with a screening algorithm, thereby creating more candidate estimators of
interest. The screening can be particularly important when there are many
variables.

4. Candidate estimators by fitting separate logistic regressions and using
post-normalization

* Code $A$ in terms of Bernoullis $B_k=I(A=k)$, $k=0,\ldots,K$.
* Construct an estimator $\bar{g}_{nk}$ of $\bar{g}_{0k}(W)\equiv P_0(B_k=1\mid
  W)$ using any of the logistic regression algorithms, for all $k=0,\ldots,K$.
* This implies an estimator
\[
g_n(a\mid W)=\frac{\bar{g}_{na}(W)}{\sum_{k=0}^K \bar{g}_{nk}(W)}.\]
* In other words, we simply normalize these separate logistic regression
estimators so that we obtain a valid conditional distribution.
* This generates an enormous amount of interesting algorithms, since we have
available the whole machine learning literature for binary outcome regression.

5. Candidate estimators by estimating the conditional "hazard" with pooled
logistic regression.
Note that
\[
g_0(a\mid W)=\lambda_0(a\mid W) S_0(a\mid W),\]
where \[
\lambda_0(a\mid W)=P_0(A=a\mid A\geq a,W),\]

and $S_0(a\mid W)=\prod_{s\leq a}(1-\lambda_0(s\mid W))$ is the conditional
survival function $P_0(A>a\mid W)$. So we have now parameterized the
conditional distribution of $A$, given $W$, by a conditional hazard
$\lambda_0(a\mid W)$: $g_0=g_{\lambda_0}$.

* We could now focus on constructing candidate estimators of
$\lambda_0(a\mid W)$, which implies candidate estimators of $g_0$.

* For every observation $A_i$, we can create $A_i+1$ rows of data
$(W,s,I(A_i=s))$, $s=0,\ldots,A_i$, $i=1,\ldots,n$. We now run a logistic
regression estimator based on the pooled data set, ignoring ID, where we
regress the binary outcome $I(A_i=s)$ on the covariates $(W,s)$.

* If one assumes a parametric model, then this is nothing else then using the
maximum likelihood estimator, demonstrating that ignoring the ID is not
inefficient.

* This defines now an estimator of $\lambda_0(s\mid W)=P_0(A=s\mid W,A\geq s)$
as a function of $(s,W)$.  

* Different choices of logistic regression based estimators will define
different estimators.

* The pooling across $s$ is not very sensible if $A$ is not an ordered variable
If $A$ is categorical, we recommend to compute  a separate logistic regression
estimator of $\lambda_0(a\mid W)$ for each $a$ (i.e., stratify by $s$ in the
  above pooled data set).

* For non-categorical $A$, one could include both stratified (by level) as well
as pooled (across levels) based logistic regression estimators.


## Exercise 2 -- Estimating the Propensity Score with `sl3` {#sl3ex2}

exercise where we can look at positivity and maybe modify target population,
address issues related to this

## Super Learning of an Optimal Individualized Treatment Rule

* Data $O=(W,A,Y)$, and nonparametric model \mathcal{M} potentially containing
assumptions on the conditional probability distribution of $A$ given $W$
$g_0(A\mid W)$.
* Target: Optimal treatment rule $\psi_0(W)=I(B_0(W)>0)$, where
$B_0(W)=E_0(Y\mid A=1,W)-E_0(Y\mid A=0,W)$, the conditional treatment effect.
* Possible loss function for $\psi_0$ is an IPCW-loss:
\[
L_{g_0}(\psi)=\frac{I(A=\psi(W))}{g(A\mid W)}Y.\]

Indeed, $\psi_0$ is the minimizer of $EL_{g_0}(\psi)$ over all rules $\psi$.
* Construct library of candidate estimators of $\psi_0=I(B_0>0)$. This can
include estimators based on plugging in an estimator of $B_0$.
* One could also include a candidate estimator $I(B_n>0)$ where $B_n$ is a
Super Learner of $B_0$, e.g. based on loss function
\[
L_{g_0}(B)=\big(\frac{2A-1}{/g(A\mid W)}Y-B(W)\big)^2\]
that directly targets $B_0=\arg\min_B P_0L_{g_0}(B)$. This loss function is
still a squared error loss but its minimized by the true $B_0$.
* Estimate $g_0$ if not known.
* Compute cross-validation selector:
\[
k_n=\arg\min_k E_{B_n}P_{n,B_n}^1 L_{\hat{g}(P_{n,B_n}^0)}
(\hat{\Psi}_k(P_{n,B_n}^0)).\]
where $B_n = \{0,1\}^n$ is used for a binary vector of $n$ defining sample
splits, where the validation sample is ${i:B_n(i) = 1}$ and ${i:B_n(i) = 0}$ is
the training sample. The empirical distribution $P_{n,B_n}^0$ corresponds to
the split $B_n$ of the training sample and the empirical distribution of the
validation sample is $P_{n,B_n}^1$.
* Super-learner of optimal rule $\psi_0$: $\hat{\Psi}_{k_n}(P_n)$.

## Exercise 3 -- Estimating the Blip {#sl3ex3}

-->
</div>
</div>
<div id="concluding-remarks" class="section level2">
<h2><span class="header-section-number">4.8</span> Concluding Remarks</h2>
<ul>
<li><p>The general ensemble learning approach of Super Learner can be applied to a
diversity of estimation and prediction problems that can be defined by a loss
function.</p></li>
<li><p>We just discussed conditional mean estimation, outcome prediction and
variable importance. In future updates of this chapter, we will delve into
prediction of a conditional density, and the optimal individualized treatment
rule.</p></li>
<li><p>If we plug in the estimator returned by super learner into the target
parameter mapping, then we would end up with an estimator that has the same
bias as what we plugged in, and would not be asymptotically linear. It also
would not be a plug-in estimator or efficient.</p>
<ul>
<li>An asymptotically linear estimator is important to have, since
they converge to the estimand at <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> rate, and thereby permit
formal statistical inference (i.e. confidence intervals and <span class="math inline">\(p\)</span>-values).</li>
<li>Plug-in estimators of the estimand are desirable because they respect both
the local and global constraints of the statistical model (e.g. bounds), and
have they have better finite-sample properties.</li>
<li>An efficient estimator is optimal in the sense that it has the lowest
possible variance, and is thus the most precise. An estimator is efficient if
and only if is asymptotically linear with influence curve equal to the
canonical gradient. The canonical gradient is a mathematical object that is
specific to the target estimand, and it provides information on the level of
difficulty of the estimation problem. The canonical gradient is shown in the
chapters that follow. Practitioner’s do not need to know how to calculate a
canonical gradient in order to understand efficiency and use Targeted Maximum
Likelihood Estimation (TMLE). Metaphorically, you do not need to be Yoda in
order to be a Jedi.</li>
</ul></li>
<li><p>TMLE is a general strategy that succeeds in constructing efficient and
asymptotically linear plug-in estimators.</p></li>
<li><p>Super Learner is fantastic for pure prediction, and for obtaining an initial
estimate in the first step of TMLE, but we need the second step of TMLE to
have the desirable statistical properties mentioned above.</p></li>
<li><p>In the chapters that follow, we focus on the targeted maximum likelihood
estimator and the targeted minimum loss-based estimator, both referred to as
TMLE.
<!--
We could just plug-in the estimator returned by Super
Learner; however, this is problematic because the Super Learner estimators are
trading off bias and variance in an optimal way and as a result their bias is
essentially the rate of convergence of these algorithms, which is always slower
than $1/\sqrt{n}$. Therefore, if we plug-in the estimator returned by super
learner into the target parameter mapping, we would end up with an
estimator which has the same bias as what we plugged in, which is greater than
$1/\sqrt{n}$. Thus, we end up with an estimator which is not asymptotically
normal, since it does not converge to the estimand at $1/\sqrt{n}$ rate.

An asymptotically linear estimator has no meaningful bias ($ < 1/\sqrt{n}$), and
can be written as an empirical mean in first order of a function of the data,
the influence curve, plus some negligible remainder term. Once an estimator
is asymptotically linear with an influence curve it’s normally distributed, so
the standardized estimator converges to a normal distribution with mean 0 and
variance is the variance of the influence curve. Thus, it is advantageous to
construct asymptotically linear estimators since they permit formal statistical
inference. Among the class of regular asymptotically linear estimators, there is
an optimal estimator which is an efficient estimator, and that’s the one with
influence curve equal to the canonical gradient of the path-wise derivative of
the target parameter. The canonical gradient is the direction of the path
through the data distribution where the parameter is steepest. An estimator is
efficient if and only if is asymptotically linear with influence curve equal to
the canonical gradient. One can calculate the canonical gradient with the
statistical model and the statistical target parameter. Techniques for
calculating the canonical gradient entail projecting an initial gradient on the
tangent space of the model at the particular distribution in the model in which
you want to calculate the canonical gradient.

Now we know what it takes to construct an efficient estimator. Namely, we need
to construct an estimator which is asymptotically linear with influence curve
the canonical gradient. There are three general classes of estimators which
succeed in constructing asymptotically linear estimators: (1) the one-step
estimator, but it is not a plug-in estimator; (2) the targeted maximum
likelihood estimator, which is a Super Learner targeted towards the target
parameter and it is a plug-in estimator; and (3) estimating equation based
estimators, which use the canonical gradient but as an estimating function in
the target parameter. In the chapters that follow, we focus on the targeted
maximum likelihood estimator and the targeted minimum loss-based estimator,
both referred to as TMLE.
-->
## Appendix</p></li>
</ul>
<div id="exercise-1-solution" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Exercise 1 Solution</h3>
<p>Here is a potential solution to the (<code>sl3</code> Exercise – Predicting Myocardial
Infarction with <code>sl3</code>)<span class="citation">(<span class="citeproc-not-found" data-reference-id="sl3ex"><strong>???</strong></span>)</span>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="co"># make task</span></a>
<a class="sourceLine" id="cb44-2" data-line-number="2">chspred_task &lt;-<span class="st"> </span><span class="kw">make_sl3_Task</span>(</a>
<a class="sourceLine" id="cb44-3" data-line-number="3">  <span class="dt">data =</span> chspred,</a>
<a class="sourceLine" id="cb44-4" data-line-number="4">  <span class="dt">covariates =</span> <span class="kw">head</span>(<span class="kw">colnames</span>(chspred), <span class="dv">-1</span>),</a>
<a class="sourceLine" id="cb44-5" data-line-number="5">  <span class="dt">outcome =</span> <span class="st">&quot;mi&quot;</span></a>
<a class="sourceLine" id="cb44-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb44-7" data-line-number="7"></a>
<a class="sourceLine" id="cb44-8" data-line-number="8"><span class="co"># make learners</span></a>
<a class="sourceLine" id="cb44-9" data-line-number="9">glm_learner &lt;-<span class="st"> </span>Lrnr_glm<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb44-10" data-line-number="10">lasso_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb44-11" data-line-number="11">ridge_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb44-12" data-line-number="12">enet_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb44-13" data-line-number="13">curated_glm_learner &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>(<span class="dt">formula =</span> <span class="st">&quot;mi ~ smoke + beta + waist&quot;</span>)</a>
<a class="sourceLine" id="cb44-14" data-line-number="14">mean_learner &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>() <span class="co"># That is one mean learner!</span></a>
<a class="sourceLine" id="cb44-15" data-line-number="15">glm_fast_learner &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb44-16" data-line-number="16">ranger_learner &lt;-<span class="st"> </span>Lrnr_ranger<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb44-17" data-line-number="17">svm_learner &lt;-<span class="st"> </span>Lrnr_svm<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb44-18" data-line-number="18">xgb_learner &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb44-19" data-line-number="19">screen_cor &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_screener_corP)</a>
<a class="sourceLine" id="cb44-20" data-line-number="20">glm_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_cor, glm_learner)</a>
<a class="sourceLine" id="cb44-21" data-line-number="21"></a>
<a class="sourceLine" id="cb44-22" data-line-number="22"><span class="co"># stack learners together</span></a>
<a class="sourceLine" id="cb44-23" data-line-number="23">stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(</a>
<a class="sourceLine" id="cb44-24" data-line-number="24">  Stack,</a>
<a class="sourceLine" id="cb44-25" data-line-number="25">  glm_pipeline, glm_learner,</a>
<a class="sourceLine" id="cb44-26" data-line-number="26">  lasso_learner, ridge_learner, enet_learner,</a>
<a class="sourceLine" id="cb44-27" data-line-number="27">  curated_glm_learner, mean_learner, glm_fast_learner,</a>
<a class="sourceLine" id="cb44-28" data-line-number="28">  ranger_learner, svm_learner, xgb_learner</a>
<a class="sourceLine" id="cb44-29" data-line-number="29">)</a>
<a class="sourceLine" id="cb44-30" data-line-number="30"></a>
<a class="sourceLine" id="cb44-31" data-line-number="31"><span class="co"># choose metalearner</span></a>
<a class="sourceLine" id="cb44-32" data-line-number="32">metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_nnls)</a>
<a class="sourceLine" id="cb44-33" data-line-number="33"></a>
<a class="sourceLine" id="cb44-34" data-line-number="34">sl &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb44-35" data-line-number="35">  <span class="dt">learners =</span> stack,</a>
<a class="sourceLine" id="cb44-36" data-line-number="36">  <span class="dt">metalearner =</span> metalearner</a>
<a class="sourceLine" id="cb44-37" data-line-number="37">)</a>
<a class="sourceLine" id="cb44-38" data-line-number="38">sl_fit &lt;-<span class="st"> </span>sl<span class="op">$</span><span class="kw">train</span>(chspred_task)</a>
<a class="sourceLine" id="cb44-39" data-line-number="39">sl_fit<span class="op">$</span><span class="kw">print</span>()</a>
<a class="sourceLine" id="cb44-40" data-line-number="40"></a>
<a class="sourceLine" id="cb44-41" data-line-number="41">CVsl &lt;-<span class="st"> </span><span class="kw">CV_lrnr_sl</span>(sl_fit, chspred_task, loss_squared_error)</a>
<a class="sourceLine" id="cb44-42" data-line-number="42">CVsl</a></code></pre></div>
<!--
### Exercise 2 Solution

Here's a potential solution to (Exercise 2)[@sl3ex2].


### Exercise 3 Solution

Here's a potential solution to the (Exercise 3)[@sl3ex3].


-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman1996stacked">
<p>Breiman, Leo. 1996. “Stacked Regressions.” <em>Machine Learning</em> 24 (1). Springer: 49–64.</p>
</div>
<div id="ref-polley2010super">
<p>Polley, Eric C, and Mark J van der Laan. 2010. “Super Learner in Prediction.” bepress.</p>
</div>
<div id="ref-vdl2003unified">
<p>van der Laan, Mark J, and Sandrine Dudoit. 2003. “Unified Cross-Validation Methodology for Selection Among Estimators and a General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities and Examples.” bepress.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
<div id="ref-van2006oracle">
<p>van der Vaart, Aad W, Sandrine Dudoit, and Mark J van der Laan. 2006. “Oracle Inequalities for Multi-Fold Cross Validation.” <em>Statistics &amp; Decisions</em> 24 (3). Oldenbourg Wissenschaftsverlag: 351–71.</p>
</div>
<div id="ref-wolpert1992stacked">
<p>Wolpert, David H. 1992. “Stacked Generalization.” <em>Neural Networks</em> 5 (2). Elsevier: 241–59.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tmle3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/tlverse-handbook/edit/master/05-sl3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
