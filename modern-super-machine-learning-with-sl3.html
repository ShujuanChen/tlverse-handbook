<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Modern Super (Machine) Learning with sl3 | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown  and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Modern Super (Machine) Learning with sl3 | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/tlverse-handbook/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/tlverse-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Modern Super (Machine) Learning with sl3 | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard, Mark van der Laan" />


<meta name="date" content="2019-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="tlverse.html">
<link rel="next" href="tmle3-the-targeted-learning-framework.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-is-not"><i class="fa fa-check"></i>What this book is not</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#jeremy-coyle"><i class="fa fa-check"></i>Jeremy Coyle</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nima-hejazi"><i class="fa fa-check"></i>Nima Hejazi</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i>Ivana Malenica</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rachael-phillips"><i class="fa fa-check"></i>Rachael Phillips</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alan-hubbard"><i class="fa fa-check"></i>Alan Hubbard</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mark-van-der-laan"><i class="fa fa-check"></i>Mark van der Laan</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> The Roadmap for Targeted Learning</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-roadmap"><i class="fa fa-check"></i><b>1.3</b> The Roadmap</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#step1"><i class="fa fa-check"></i><b>1.3.1</b> (1) Data as a random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#step2"><i class="fa fa-check"></i><b>1.3.2</b> (2) The statistical model, <span class="math inline">\(P_0 \in \mathcal{M}\)</span></a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#step3"><i class="fa fa-check"></i><b>1.3.3</b> (3) The statistical target parameter, <span class="math inline">\(\Psi: \mathcal{M}\rightarrow\mathbb{R}\)</span></a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#step4"><i class="fa fa-check"></i><b>1.3.4</b> (4) The estimand, <span class="math inline">\(\Psi(P_0)\)</span></a></li>
<li class="chapter" data-level="1.3.5" data-path="intro.html"><a href="intro.html#step5"><i class="fa fa-check"></i><b>1.3.5</b> (5) The estimator, <span class="math inline">\(\hat{\Psi} : \mathcal{M}_{NP} \rightarrow \mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="1.3.6" data-path="intro.html"><a href="intro.html#step6"><i class="fa fa-check"></i><b>1.3.6</b> (6) The estimate, <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
<li class="chapter" data-level="1.3.7" data-path="intro.html"><a href="intro.html#step7"><i class="fa fa-check"></i><b>1.3.7</b> (7) Sampling distribution of <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#statistical-inference"><i class="fa fa-check"></i><b>1.4</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#summary"><i class="fa fa-check"></i><b>1.5</b> Summary</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#appendix"><i class="fa fa-check"></i><b>1.6</b> Appendix</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#causal-models"><i class="fa fa-check"></i><b>1.7</b> Causal models</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#identifiability"><i class="fa fa-check"></i><b>1.8</b> Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tlverse.html"><a href="tlverse.html"><i class="fa fa-check"></i><b>2</b> Welcome to the <code>tlverse</code></a><ul>
<li class="chapter" data-level="2.1" data-path="tlverse.html"><a href="tlverse.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="tlverse.html"><a href="tlverse.html#what-is-the-tlverse"><i class="fa fa-check"></i><b>2.2</b> What is the <code>tlverse</code>?</a></li>
<li class="chapter" data-level="2.3" data-path="tlverse.html"><a href="tlverse.html#tlverse-components"><i class="fa fa-check"></i><b>2.3</b> <code>tlverse</code> components</a></li>
<li class="chapter" data-level="2.4" data-path="tlverse.html"><a href="tlverse.html#installation"><i class="fa fa-check"></i><b>2.4</b> Installation</a></li>
<li class="chapter" data-level="2.5" data-path="tlverse.html"><a href="tlverse.html#example-data---wash-benefits"><i class="fa fa-check"></i><b>2.5</b> Example Data - WASH Benefits</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html"><i class="fa fa-check"></i><b>3</b> Modern Super (Machine) Learning with <code>sl3</code></a><ul>
<li class="chapter" data-level="3.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#introduction-1"><i class="fa fa-check"></i><b>3.2</b> Introduction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#background"><i class="fa fa-check"></i><b>3.2.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#basic-implementation"><i class="fa fa-check"></i><b>3.3</b> Basic Implementation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#wash-benefits-study-example"><i class="fa fa-check"></i><b>3.3.1</b> WASH Benefits Study Example</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#extensions"><i class="fa fa-check"></i><b>3.4</b> Extensions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#customize-learner-hyperparameters"><i class="fa fa-check"></i><b>3.4.1</b> Customize learner hyperparameters</a></li>
<li class="chapter" data-level="3.4.2" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#screening-covariates"><i class="fa fa-check"></i><b>3.4.2</b> Screening covariates</a></li>
<li class="chapter" data-level="3.4.3" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#cross-validated-super-learner"><i class="fa fa-check"></i><b>3.4.3</b> Cross-validated Super Learner</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#variable-importance"><i class="fa fa-check"></i><b>3.5</b> Variable importance</a></li>
<li class="chapter" data-level="3.6" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#exercise"><i class="fa fa-check"></i><b>3.6</b> Exercise</a><ul>
<li class="chapter" data-level="3.6.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#predicting-myocardial-infarction"><i class="fa fa-check"></i><b>3.6.1</b> Predicting myocardial infarction</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#concluding-remarks"><i class="fa fa-check"></i><b>3.7</b> Concluding Remarks</a></li>
<li class="chapter" data-level="3.8" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#appendix-1"><i class="fa fa-check"></i><b>3.8</b> Appendix</a><ul>
<li class="chapter" data-level="3.8.1" data-path="modern-super-machine-learning-with-sl3.html"><a href="modern-super-machine-learning-with-sl3.html#exercise-solution"><i class="fa fa-check"></i><b>3.8.1</b> Exercise solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html"><i class="fa fa-check"></i><b>4</b> <code>tmle3</code> – The Targeted Learning Framework</a><ul>
<li class="chapter" data-level="4.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#learning-objectives-3"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#example-tmle3-for-ate"><i class="fa fa-check"></i><b>4.2</b> Example: <code>tmle3</code> for ATE</a><ul>
<li class="chapter" data-level="4.2.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#load-the-data"><i class="fa fa-check"></i><b>4.2.1</b> Load the Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#define-the-variable-roles"><i class="fa fa-check"></i><b>4.2.2</b> Define the variable roles</a></li>
<li class="chapter" data-level="4.2.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#handle-missingness"><i class="fa fa-check"></i><b>4.2.3</b> Handle Missingness</a></li>
<li class="chapter" data-level="4.2.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#create-a-spec-object"><i class="fa fa-check"></i><b>4.2.4</b> Create a “Spec” Object</a></li>
<li class="chapter" data-level="4.2.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#define-the-learners"><i class="fa fa-check"></i><b>4.2.5</b> Define the learners</a></li>
<li class="chapter" data-level="4.2.6" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fit-the-tmle"><i class="fa fa-check"></i><b>4.2.6</b> Fit the TMLE</a></li>
<li class="chapter" data-level="4.2.7" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#evaluate-the-estimates"><i class="fa fa-check"></i><b>4.2.7</b> Evaluate the Estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#tmle3-components"><i class="fa fa-check"></i><b>4.3</b> <code>tmle3</code> Components</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#tmle3_task"><i class="fa fa-check"></i><b>4.3.1</b> <code>tmle3_task</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#initial-likelihood"><i class="fa fa-check"></i><b>4.3.2</b> Initial Likelihood</a></li>
<li class="chapter" data-level="4.3.3" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#targeted-likelihood-updater"><i class="fa fa-check"></i><b>4.3.3</b> Targeted Likelihood (updater)</a></li>
<li class="chapter" data-level="4.3.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#parameter-mapping"><i class="fa fa-check"></i><b>4.3.4</b> Parameter Mapping</a></li>
<li class="chapter" data-level="4.3.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#putting-it-all-together"><i class="fa fa-check"></i><b>4.3.5</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fitting-tmle3-with-multiple-parameters"><i class="fa fa-check"></i><b>4.4</b> Fitting <code>tmle3</code> with multiple parameters</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#delta-method"><i class="fa fa-check"></i><b>4.4.1</b> Delta Method</a></li>
<li class="chapter" data-level="4.4.2" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#fit"><i class="fa fa-check"></i><b>4.4.2</b> Fit</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#exercise-1"><i class="fa fa-check"></i><b>4.5</b> Exercise</a></li>
<li class="chapter" data-level="4.6" data-path="tmle3-the-targeted-learning-framework.html"><a href="tmle3-the-targeted-learning-framework.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>5</b> Optimal Individualized Treatment Regimes</a><ul>
<li class="chapter" data-level="5.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-objectives-4"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#introduction-to-optimal-individualized-interventions"><i class="fa fa-check"></i><b>5.2</b> Introduction to Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="5.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#data-structure-and-notation"><i class="fa fa-check"></i><b>5.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="5.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#defining-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.4</b> Defining the Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="5.4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment"><i class="fa fa-check"></i><b>5.4.1</b> Binary treatment</a></li>
<li class="chapter" data-level="5.4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment"><i class="fa fa-check"></i><b>5.4.2</b> Categorical treatment</a></li>
<li class="chapter" data-level="5.4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#on-inference"><i class="fa fa-check"></i><b>5.4.3</b> On Inference</a></li>
<li class="chapter" data-level="5.4.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#why-cv-tmle"><i class="fa fa-check"></i><b>5.4.4</b> Why CV-TMLE?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interpreting-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.5</b> Interpreting the Causal Effect of an Optimal Individualized Intervention</a></li>
<li class="chapter" data-level="5.6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with"><i class="fa fa-check"></i><b>5.6</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with</a></li>
<li class="chapter" data-level="5.7" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment-1"><i class="fa fa-check"></i><b>5.7</b> Categorical Treatment</a><ul>
<li class="chapter" data-level="5.7.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data"><i class="fa fa-check"></i><b>5.7.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3"><i class="fa fa-check"></i><b>5.7.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.7.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized"><i class="fa fa-check"></i><b>5.7.3</b> Targeted Estimation of the Mean under the Optimal Individualized</a></li>
<li class="chapter" data-level="5.7.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interventions-effects"><i class="fa fa-check"></i><b>5.7.4</b> Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-individualized-intervention-with-1"><i class="fa fa-check"></i><b>5.8</b> Evaluating the Causal Effect of an Optimal Individualized Intervention with</a></li>
<li class="chapter" data-level="5.9" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment-1"><i class="fa fa-check"></i><b>5.9</b> Binary Treatment</a><ul>
<li class="chapter" data-level="5.9.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-1"><i class="fa fa-check"></i><b>5.9.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.9.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-1"><i class="fa fa-check"></i><b>5.9.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.9.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-1"><i class="fa fa-check"></i><b>5.9.3</b> Targeted Estimation of the Mean under the Optimal Individualized</a></li>
<li class="chapter" data-level="5.9.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interventions-effects-1"><i class="fa fa-check"></i><b>5.9.4</b> Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#q-learning"><i class="fa fa-check"></i><b>5.10</b> Q-learning</a></li>
<li class="chapter" data-level="5.11" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extensions-to-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>5.11</b> Extensions to Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="5.11.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simpler-rules"><i class="fa fa-check"></i><b>5.11.1</b> Simpler Rules</a></li>
<li class="chapter" data-level="5.11.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#realistic-optimal-individual-regimes"><i class="fa fa-check"></i><b>5.11.2</b> Realistic Optimal Individual Regimes</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-analysis-with-optimal-individualized-interventions"><i class="fa fa-check"></i><b>5.12</b> Variable Importance Analysis with Optimal Individualized Interventions</a><ul>
<li class="chapter" data-level="5.12.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-2"><i class="fa fa-check"></i><b>5.12.1</b> Simulated Data</a></li>
<li class="chapter" data-level="5.12.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-2"><i class="fa fa-check"></i><b>5.12.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="5.12.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-using-targeted-estimation-of-the-value-of-the-itr"><i class="fa fa-check"></i><b>5.12.3</b> Variable Importance using Targeted Estimation of the value of the ITR</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#example-with-the-wash-benefits-data"><i class="fa fa-check"></i><b>5.13</b> Example with the WASH Benefits Data</a></li>
<li class="chapter" data-level="5.14" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercises"><i class="fa fa-check"></i><b>5.14</b> Exercises</a><ul>
<li class="chapter" data-level="5.14.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#review-of-key-concepts"><i class="fa fa-check"></i><b>5.14.1</b> Review of Key Concepts</a></li>
<li class="chapter" data-level="5.14.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#the-ideas-in-action"><i class="fa fa-check"></i><b>5.14.2</b> The Ideas in Action</a></li>
<li class="chapter" data-level="5.14.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#advanced-topics"><i class="fa fa-check"></i><b>5.14.3</b> Advanced Topics</a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#appendix-2"><i class="fa fa-check"></i><b>5.15</b> Appendix</a><ul>
<li class="chapter" data-level="5.15.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercise-solutions"><i class="fa fa-check"></i><b>5.15.1</b> Exercise solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>6</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#learning-objectives-5"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#introduction-to-stochastic-interventions"><i class="fa fa-check"></i><b>6.2</b> Introduction to Stochastic Interventions</a></li>
<li class="chapter" data-level="6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#data-structure-and-notation-1"><i class="fa fa-check"></i><b>6.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="6.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.4</b> Defining the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="6.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#interpreting-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.5</b> Interpreting the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="6.6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#evaluating-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>6.6</b> Evaluating the Causal Effect of a Stochastic Intervention</a><ul>
<li class="chapter" data-level="6.6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-simulated-data"><i class="fa fa-check"></i><b>6.6.1</b> Example with Simulated Data</a></li>
<li class="chapter" data-level="6.6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>6.6.2</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="6.6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>6.6.3</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>6.7</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="6.7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>6.7.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="6.7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>6.7.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="6.7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects-1"><i class="fa fa-check"></i><b>6.7.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="6.7.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>6.7.4</b> Inference with Marginal Structural Models</a></li>
<li class="chapter" data-level="6.7.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-the-wash-benefits-data-1"><i class="fa fa-check"></i><b>6.7.5</b> Example with the WASH Benefits Data</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises-1"><i class="fa fa-check"></i><b>6.8</b> Exercises</a><ul>
<li class="chapter" data-level="6.8.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#review-of-key-concepts-1"><i class="fa fa-check"></i><b>6.8.1</b> Review of Key Concepts</a></li>
<li class="chapter" data-level="6.8.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#the-ideas-in-action-1"><i class="fa fa-check"></i><b>6.8.2</b> The Ideas in Action</a></li>
<li class="chapter" data-level="6.8.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#advanced-topics-1"><i class="fa fa-check"></i><b>6.8.3</b> Advanced Topics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modern-super-machine-learning-with-sl3" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Modern Super (Machine) Learning with <code>sl3</code></h1>
<p><em>Rachael Phillips</em>, based on the <a href="https://github.com/tlverse/sl3"><code>sl3</code> package</a>
by <em>Jeremy Coyle, Nima Hejazi, Ivana Malenica, and Oleg Sofrygin</em></p>
<p>Updated: 2019-05-06</p>
<div id="learning-objectives-2" class="section level2">
<h2><span class="header-section-number">3.1</span> Learning Objectives</h2>
<p>By the end of this chapter you will be able to:</p>
<ol style="list-style-type: decimal">
<li>Select a loss function that is appropriate for the functional parameter to be
estimated.</li>
<li>Assemble an ensemble of learners based on the properties that identify what
features they support.</li>
<li>Customize learner hyperparameters to incorporate a diversity of different
settings.</li>
<li>Select a subset of available covariates and pass only those variables to the
modeling algorithm.</li>
<li>Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.</li>
<li>Obtain <code>sl3</code> variable importance metrics.</li>
<li>Interpret the discrete and continuous super learner fits.</li>
<li>Rationalize the need to remove bias from the super learner to make an optimal
bias–variance tradeoff for the parameter of interest.</li>
</ol>
</div>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Introduction</h2>
<p>In <a href="intro.html#intro">1</a>, we introduced the road map for targeted learning as a
general template to translate real-world data applications into formal
statistical estimation problems. The first steps of this roadmap define the
<em>statistical estimation problem</em>, which establish</p>
<ol style="list-style-type: decimal">
<li>Data as a realization of a random variable, or equivalently, an outcome of a
particular experiment</li>
<li>A statistical model, representing the true knowledge about the
data-generating experiment</li>
<li>A translation of the scientific question, which is often causal, into a
target parameter.</li>
</ol>
<p>Note that if the target parameter is causal, step 3 also requires
establishing identifiability of the target quantity from the observed data
distribution, under possible non-testable assumptions that may not necessarily
be reasonable. Still, the target quantity does have a valid statistical
interpretation.</p>
<p>Now that we have defined the statistical estimation problem, we are ready to
construct the TMLE; an asymptotically linear and efficient substitution
estimator of this target quantity. The first step in this estimation procedure
is an initial estimate of the data-generating distribution, or the relevant part
of this distribution that is needed to evaluate the target parameter. For this
initial estimation, we use the Super Learner <span class="citation">(<span class="citeproc-not-found" data-reference-id="van2007super"><strong>???</strong></span>)</span>. The super learner
provides an important step in creating a robust estimator. It is a
loss-function-based tool that uses cross-validation to obtain the best
prediction of our target parameter, based on a weighted average of a library of
machine learning algorithms. This library of machine learning algorithms
consists of functions (“learners” in the <code>sl3</code> nomenclature) that we think
might be consistent with the true data-generating distribution. The
ensembling of algorithms with weights (“metalearning” in the <code>sl3</code> nomenclature)
has been shown to be adaptive and robust, even in small samples
<span class="citation">(<span class="citeproc-not-found" data-reference-id="polley2010super"><strong>???</strong></span>)</span>. The Super Learner has been proven to be asymptotically as
accurate as the best possible prediction algorithm in the library
<span class="citation">(van der Laan and Dudoit <a href="#ref-vdl2003unified">2003</a>; Van der Vaart, Dudoit, and Laan <a href="#ref-van2006oracle">2006</a>)</span>.</p>
<div id="background" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Background</h3>
<p>A <em>loss function</em> <span class="math inline">\(L\)</span> is defined as a function of the observed data and a
candidate parameter value <span class="math inline">\(\psi\)</span>, which has unknown true value <span class="math inline">\(\psi_0\)</span>,
<span class="math inline">\(L(\psi)(O)\)</span>. We can estimate the loss by substituting the empirical
distribution <span class="math inline">\(P_n\)</span> for the true (but unknown) distribution of the observed data
<span class="math inline">\(P_0\)</span>. A valid loss function will have expectation (risk) that is minimized at
the true value of the parameter <span class="math inline">\(\psi_0\)</span>. For example, the conditional mean
minimizes the risk of the squared error loss. Thus, it is a valid loss function
when estimating the conditional mean.</p>
<p>The <em>discrete super learner</em>, or cross-validated selector, is the algorithm in
the library that minimizes the cross-validated empirical risk. The
cross-validated empirical risk of an algorithm is defined as the empirical mean
over a validation sample of the loss of the algorithm fitted on the training
sample, averaged across the splits of the data.</p>
<p>The <em>continuous/ensemble super learner</em> is a weighted average of the library of
algorithms, where the weights are chosen to minimize the cross-validated
empirical risk of the library. Restricting the weights (“metalearner” in <code>sl3</code>
nomenclature) to be positive and sum to one (convex combination) has been shown
to improve upon the discrete Super Learner <span class="citation">(<span class="citeproc-not-found" data-reference-id="polley2010super"><strong>???</strong></span>; <span class="citeproc-not-found" data-reference-id="van2007super"><strong>???</strong></span>)</span>.
This notion of weighted combinations was introduced in <span class="citation">Wolpert (<a href="#ref-wolpert1992stacked">1992</a>)</span> for
neural networks and adapted for regressions in <span class="citation">Breiman (<a href="#ref-breiman1996stacked">1996</a>)</span>.</p>
<p>For more detail on super learner we refer the reader to <span class="citation">(<span class="citeproc-not-found" data-reference-id="van2007super"><strong>???</strong></span>)</span> and
<span class="citation">(<span class="citeproc-not-found" data-reference-id="polley2010super"><strong>???</strong></span>)</span>. The optimality results for the cross-validation selector
among a family of algorithms were established in <span class="citation">van der Laan and Dudoit (<a href="#ref-vdl2003unified">2003</a>)</span> and extended
in <span class="citation">Van der Vaart, Dudoit, and Laan (<a href="#ref-van2006oracle">2006</a>)</span>.</p>
</div>
</div>
<div id="basic-implementation" class="section level2">
<h2><span class="header-section-number">3.3</span> Basic Implementation</h2>
<p>We begin by illustrating the basic functionality of the Super Learner
algorithm as implemented in <code>sl3</code>. The <code>sl3</code> implementation consists of the
following steps:</p>
<ol start="0" style="list-style-type: decimal">
<li>Load the necessary libraries and data</li>
<li>Define the machine learning task</li>
<li>Make a super learner by creating library of base learners and a metalearner</li>
<li>Train the super learner on the machine learning task</li>
<li>Obtain predicted values</li>
</ol>
<div id="wash-benefits-study-example" class="section level3">
<h3><span class="header-section-number">3.3.1</span> WASH Benefits Study Example</h3>
<p>Using the WASH data, we are interested in predicting weight-for-height z-score
<code>whz</code> using the available covariate data. Let’s begin!</p>
<p><strong>0. Load the necessary libraries and data</strong></p>
<p>First, we will load the relevant <code>R</code> packages, set a seed, and load the data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(here)
<span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(data.table)
<span class="kw">library</span>(sl3)
<span class="kw">library</span>(SuperLearner)
<span class="kw">library</span>(origami)
<span class="kw">set.seed</span>(<span class="dv">7194</span>)

<span class="co"># load data set and take a peek</span>
washb_data &lt;-<span class="st"> </span><span class="kw">fread</span>(<span class="kw">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;washb_data.csv&quot;</span>), <span class="dt">stringsAsFactors =</span> <span class="ot">TRUE</span>)
<span class="kw">head</span>(washb_data, <span class="dv">3</span>)</code></pre>
<pre><code>     whz      tr fracode month aged  sex momage         momedu momheight
1:  0.00 Control  N05265     9  268 male     30 Primary (1-5y)    146.40
2: -1.16 Control  N05265     9  286 male     25 Primary (1-5y)    148.75
3: -1.05 Control  N08002     9  264 male     25 Primary (1-5y)    152.15
                    hfiacat Nlt18 Ncomp watmin elec floor walls roof
1:              Food Secure     3    11      0    1     0     1    1
2: Moderately Food Insecure     2     4      0    1     0     1    1
3:              Food Secure     1    10      0    0     0     1    1
   asset_wardrobe asset_table asset_chair asset_khat asset_chouki asset_tv
1:              0           1           1          1            0        1
2:              0           1           0          1            1        0
3:              0           0           1          0            1        0
   asset_refrig asset_bike asset_moto asset_sewmach asset_mobile
1:            0          0          0             0            1
2:            0          0          0             0            1
3:            0          0          0             0            1</code></pre>
<p><strong>1. Define the machine learning task</strong></p>
<p>To define the machine learning <strong>“task”</strong> (predict weight-for-height z-score
<code>whz</code> using the available covariate data), we need to create an <code>sl3_Task</code>
object. The <code>sl3_Task</code> keeps track of the roles the variables play in the
machine learning problem, the data, and any metadata (e.g., observational-level
weights, id, offset).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># specify the outcome and covariates</span>
outcome &lt;-<span class="st"> &quot;whz&quot;</span>
covars &lt;-<span class="st"> </span><span class="kw">colnames</span>(washb_data)[<span class="op">-</span><span class="kw">which</span>(<span class="kw">names</span>(washb_data) <span class="op">==</span><span class="st"> </span>outcome)]

<span class="co"># create the sl3 task</span>
washb_task &lt;-<span class="st"> </span><span class="kw">make_sl3_Task</span>(
  <span class="dt">data =</span> washb_data,
  <span class="dt">covariates =</span> covars,
  <span class="dt">outcome =</span> outcome
)</code></pre>
<pre><code>Warning in .subset2(public_bind_env, &quot;initialize&quot;)(...): Missing Covariate Data
Found. Imputing covariates using sl3_process_missing</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># examine it</span>
washb_task</code></pre>
<pre><code>A sl3 Task with 4695 obs and these nodes:
$covariates
 [1] &quot;tr&quot;              &quot;fracode&quot;         &quot;month&quot;           &quot;aged&quot;           
 [5] &quot;sex&quot;             &quot;momage&quot;          &quot;momedu&quot;          &quot;momheight&quot;      
 [9] &quot;hfiacat&quot;         &quot;Nlt18&quot;           &quot;Ncomp&quot;           &quot;watmin&quot;         
[13] &quot;elec&quot;            &quot;floor&quot;           &quot;walls&quot;           &quot;roof&quot;           
[17] &quot;asset_wardrobe&quot;  &quot;asset_table&quot;     &quot;asset_chair&quot;     &quot;asset_khat&quot;     
[21] &quot;asset_chouki&quot;    &quot;asset_tv&quot;        &quot;asset_refrig&quot;    &quot;asset_bike&quot;     
[25] &quot;asset_moto&quot;      &quot;asset_sewmach&quot;   &quot;asset_mobile&quot;    &quot;delta_momage&quot;   
[29] &quot;delta_momheight&quot;

$outcome
[1] &quot;whz&quot;

$id
NULL

$weights
NULL

$offset
NULL</code></pre>
<p>This warning is important. The task just imputed missing covariates for us.
Specifically, for each covariate column with missing values, <code>sl3</code> uses the
median to impute missing continuous covariates, and the mode to impute binary or
categorical covariates. Also, for each covariate column with missing values,
<code>sl3</code> adds an additional column indicating whether or not the value was imputed,
which is particularly handy when the missingness in the data might be
informative.</p>
<p>Also, notice that we did not specify the number of folds, or the loss function
in the task. The default cross-validation scheme is V-fold, with the number of
folds <span class="math inline">\(V=10\)</span>.</p>
<p><strong>2. Make a super learner</strong></p>
<p>Now that we have defined our machine learning problem with the task, we are
ready to <strong>“make”</strong> the machine learning algorithms.</p>
<p>Learners have properties that indicate what features they support. We may use
<code>sl3_list_properties()</code> to get a list of all properties supported by at least
one learner.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sl3_list_properties</span>()</code></pre>
<pre><code> [1] &quot;binomial&quot;             &quot;categorical&quot;          &quot;continuous&quot;          
 [4] &quot;cv&quot;                   &quot;density&quot;              &quot;ids&quot;                 
 [7] &quot;multivariate_outcome&quot; &quot;offset&quot;               &quot;preprocessing&quot;       
[10] &quot;timeseries&quot;           &quot;weights&quot;              &quot;wrapper&quot;             </code></pre>
<p>Since we have a continuous outcome, we may identify the learners that support
this outcome type with <code>sl3_list_learners()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sl3_list_learners</span>(<span class="st">&quot;continuous&quot;</span>)</code></pre>
<pre><code> [1] &quot;Lrnr_arima&quot;                     &quot;Lrnr_bartMachine&quot;              
 [3] &quot;Lrnr_bilstm&quot;                    &quot;Lrnr_condensier&quot;               
 [5] &quot;Lrnr_dbarts&quot;                    &quot;Lrnr_expSmooth&quot;                
 [7] &quot;Lrnr_glm&quot;                       &quot;Lrnr_glm_fast&quot;                 
 [9] &quot;Lrnr_glmnet&quot;                    &quot;Lrnr_grf&quot;                      
[11] &quot;Lrnr_h2o_glm&quot;                   &quot;Lrnr_h2o_grid&quot;                 
[13] &quot;Lrnr_hal9001&quot;                   &quot;Lrnr_HarmonicReg&quot;              
[15] &quot;Lrnr_lstm&quot;                      &quot;Lrnr_mean&quot;                     
[17] &quot;Lrnr_nnls&quot;                      &quot;Lrnr_optim&quot;                    
[19] &quot;Lrnr_pkg_SuperLearner&quot;          &quot;Lrnr_pkg_SuperLearner_method&quot;  
[21] &quot;Lrnr_pkg_SuperLearner_screener&quot; &quot;Lrnr_randomForest&quot;             
[23] &quot;Lrnr_ranger&quot;                    &quot;Lrnr_rpart&quot;                    
[25] &quot;Lrnr_rugarch&quot;                   &quot;Lrnr_solnp&quot;                    
[27] &quot;Lrnr_stratified&quot;                &quot;Lrnr_svm&quot;                      
[29] &quot;Lrnr_tsDyn&quot;                     &quot;Lrnr_xgboost&quot;                  </code></pre>
<p>Now that we have an idea of some learners, we can construct them using the
<code>make_learner</code> function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># choose base learners</span>
lrnr_glm &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glm)
lrnr_mean &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_mean)
lrnr_ranger &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_ranger)
lrnr_glmnet &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_glmnet)</code></pre>
<p>In order to assemble the library of learners, we need to <strong>“stack”</strong> them
together. A <code>Stack</code> is a special learner and it has the same interface as all
other learners. What makes a stack special is that it combines multiple learners
by training them simultaneously, so that their predictions can be either
combined or compared.</p>
<pre class="sourceCode r"><code class="sourceCode r">stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(
  Stack,
  lrnr_glm, lrnr_mean, lrnr_ranger, lrnr_glmnet
)</code></pre>
<p>We’re almost ready to super learn! Just a couple more necessary specifications.</p>
<p>We will fit a non-negative least squares metalearner using <code>Lrnr_nnls</code>. Note
that any learner can be used as a metalearner. <code>Lrnr_nnls</code> is a solid choice
for a metalearner, since it creates a convex combination of the learners when
combining them.</p>
<pre class="sourceCode r"><code class="sourceCode r">metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_nnls)</code></pre>
<p>Now that we have made a library/stack of base learners and a metalearner, we
are ready to make the super learner.</p>
<pre class="sourceCode r"><code class="sourceCode r">sl &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl,
  <span class="dt">learners =</span> stack,
  <span class="dt">metalearner =</span> metalearner
)</code></pre>
<p><strong>3. Train the super learner on the machine learning task</strong></p>
<p>The super learner algorithm fits a metalearner on the validation-set
predictions in a cross-validated manner, thereby avoiding overfitting. This
procedure is referred to as the <em>continuous</em> super learner. The cross-validation
selector, or <em>discrete</em> super learner, is the base learner with the lowest
cross-validated risk.</p>
<p>Now we are ready to <strong>“train”</strong> our super learner on our <code>sl3_task</code> object.</p>
<pre class="sourceCode r"><code class="sourceCode r">sl_fit &lt;-<span class="st"> </span>sl<span class="op">$</span><span class="kw">train</span>(washb_task)</code></pre>
<p><strong>4. Obtain predicted values</strong></p>
<p>Now that we have fit the super learner, we are ready to obtain our predicted
values, and we can also obtain a summary of the results.</p>
<pre class="sourceCode r"><code class="sourceCode r">sl_preds &lt;-<span class="st"> </span>sl_fit<span class="op">$</span><span class="kw">predict</span>()
<span class="kw">head</span>(sl_preds)</code></pre>
<pre><code>[1] -0.4809031 -0.9063907 -0.7586360 -0.8081527 -0.6453236 -0.7170514</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">sl_fit<span class="op">$</span><span class="kw">print</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
<pre><code>[1] &quot;SuperLearner:&quot;
List of 4
 $ : chr &quot;Lrnr_glm_TRUE&quot;
 $ : chr &quot;Lrnr_mean&quot;
 $ : chr &quot;Lrnr_ranger_500_TRUE&quot;
 $ : chr &quot;Lrnr_glmnet_NULL_deviance_10_1_100_TRUE&quot;
[1] &quot;Lrnr_nnls&quot;
                                     lrnrs   weights
1:                           Lrnr_glm_TRUE 0.1434794
2:                               Lrnr_mean 0.0000000
3:                    Lrnr_ranger_500_TRUE 0.5080306
4: Lrnr_glmnet_NULL_deviance_10_1_100_TRUE 0.3564469
[1] &quot;Cross-validated risk (MSE, squared error loss):&quot;
                                   learner coefficients mean_risk    SE_risk
1:                           Lrnr_glm_TRUE           NA  1.018612 0.02380402
2:                               Lrnr_mean           NA  1.065282 0.02502664
3:                    Lrnr_ranger_500_TRUE           NA  1.011545 0.02335158
4: Lrnr_glmnet_NULL_deviance_10_1_100_TRUE           NA  1.012373 0.02359790
5:                            SuperLearner           NA  1.006157 0.02336433
      fold_SD fold_min_risk fold_max_risk
1: 0.07799191     0.8956048      1.134940
2: 0.09191791     0.9264292      1.196647
3: 0.08606345     0.8676055      1.151924
4: 0.07949658     0.8826979      1.130236
5: 0.08250449     0.8692515      1.136087</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">learner</th>
<th align="right">coefficients</th>
<th align="right">mean_risk</th>
<th align="right">SE_risk</th>
<th align="right">fold_SD</th>
<th align="right">fold_min_risk</th>
<th align="right">fold_max_risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Lrnr_glm_TRUE</td>
<td align="right">NA</td>
<td align="right">1.019</td>
<td align="right">0.024</td>
<td align="right">0.078</td>
<td align="right">0.896</td>
<td align="right">1.135</td>
</tr>
<tr class="even">
<td align="left">Lrnr_mean</td>
<td align="right">NA</td>
<td align="right">1.065</td>
<td align="right">0.025</td>
<td align="right">0.092</td>
<td align="right">0.926</td>
<td align="right">1.197</td>
</tr>
<tr class="odd">
<td align="left">Lrnr_ranger_500_TRUE</td>
<td align="right">NA</td>
<td align="right">1.012</td>
<td align="right">0.023</td>
<td align="right">0.086</td>
<td align="right">0.868</td>
<td align="right">1.152</td>
</tr>
<tr class="even">
<td align="left">Lrnr_glmnet_NULL_deviance_10_1_100_TRUE</td>
<td align="right">NA</td>
<td align="right">1.012</td>
<td align="right">0.024</td>
<td align="right">0.079</td>
<td align="right">0.883</td>
<td align="right">1.130</td>
</tr>
<tr class="odd">
<td align="left">SuperLearner</td>
<td align="right">NA</td>
<td align="right">1.006</td>
<td align="right">0.023</td>
<td align="right">0.083</td>
<td align="right">0.869</td>
<td align="right">1.136</td>
</tr>
</tbody>
</table>
<p>Explain summary</p>
</div>
</div>
<div id="extensions" class="section level2">
<h2><span class="header-section-number">3.4</span> Extensions</h2>
<p>In this section, we will introduce a few extensions of the <code>sl3</code> framework,
including</p>
<ol style="list-style-type: decimal">
<li>Customizing learner hyperparameters</li>
<li>Feature selection</li>
<li>Cross-validated super learner</li>
<li>Variable importance with <code>sl3</code>.</li>
</ol>
<div id="customize-learner-hyperparameters" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Customize learner hyperparameters</h3>
<p>We can customize learner hyperparameters to incorporate a diversity of different
settings. We can also include learners from the <a href="https://github.com/ecpolley/superlearner"><code>SuperLearner</code> <code>R</code>
package</a>. Documentation for the
learners and their hyperparameters can be found in the <a href="https://tlverse.org/sl3/reference/index.html#section-sl-learners"><code>sl3 Learners Reference</code></a>.</p>
<pre class="sourceCode r"><code class="sourceCode r">lrnr_ranger100 &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_ranger, <span class="dt">num.trees =</span> <span class="dv">100</span>)
lrnr_ranger1k &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_ranger, <span class="dt">num.trees =</span> <span class="dv">1000</span>)
lrnr_gam &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;SL.gam&quot;</span>)
lrnr_bayesglm &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;SL.bayesglm&quot;</span>)</code></pre>
<p>Let’s create a new stack with these new learners, so we may incorporate them in
a new super learner.</p>
<pre class="sourceCode r"><code class="sourceCode r">new_stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(
  Stack,
  lrnr_glm, lrnr_mean, lrnr_ranger, lrnr_glmnet, lrnr_ranger1k, lrnr_ranger100,
  lrnr_gam, lrnr_bayesglm
)</code></pre>
</div>
<div id="screening-covariates" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Screening covariates</h3>
<p>Why stop now? We can also select a subset of available covariates and pass only
those variables to the modeling algorithm. This screening can be particularly
important when there are many variables.</p>
<p>Let’s see how this works. Consider screening covariates based on their
correlation with the outcome (<code>cor.test</code> p-value <span class="math inline">\(\leq 0.1\)</span>), and <code>randomForest</code>
variable importance (top 10 most important variables).</p>
<pre class="sourceCode r"><code class="sourceCode r">screen_cor &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner_screener<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;screen.corP&quot;</span>)
screen_rf &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner_screener<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;screen.randomForest&quot;</span>)</code></pre>
<p>Now we need to <strong>“pipe”</strong> only those selected covariates to the modeling
algorithm. To accomplish this, we need to make a <code>Pipeline</code>, which is a just
set of learners to be fit sequentially, where the fit from one learner is used
to define the task for the next learner. Note the difference between <code>Pipeline</code>
and <code>Stack</code> here- one is necessary in order to define a sequential process,
whereas the other one establishes parallel function of learners.</p>
<pre class="sourceCode r"><code class="sourceCode r">cor_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_cor, new_stack)
rf_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_rf, new_stack)</code></pre>
<p>Lastly, we have to stack all of these pipelines together, so we may use
them as base learners in our super learner, analogous to what we have seen
before. Now however, our learners will be preceded by a screening step. Let’s
also consider the <code>new_stack</code>, just to compare how the feature selection methods
perform in comparison to the methods without feature selection.</p>
<pre class="sourceCode r"><code class="sourceCode r">fancy_stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Stack, cor_pipeline, rf_pipeline, new_stack)</code></pre>
<p>Now we can Super Learn with this fancy base learner stack.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run sl and predict on WASH</span>
sl_fancy &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(<span class="dt">learners =</span> fancy_stack, <span class="dt">metalearner =</span> metalearner)
sl_fancy_fit &lt;-<span class="st"> </span>sl_fancy<span class="op">$</span><span class="kw">train</span>(washb_task)
sl_preds &lt;-<span class="st"> </span>sl_fancy_fit<span class="op">$</span><span class="kw">predict</span>()
sl_fancy_fit<span class="op">$</span><span class="kw">print</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
</div>
<div id="cross-validated-super-learner" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Cross-validated Super Learner</h3>
<p>We can cross-validate the super learner to see how well the super learner
performs on unseen data. This requires an “external” layer of cross-validation,
also called nested cross-validation, which involves setting aside a separate
holdout sample that we don’t use to fit the super learner. This external
cross-validation procedure may also incorporate 10 folds, which is the default
in <code>sl3</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">CVsl_fancy &lt;-<span class="st"> </span><span class="kw">CV_lrnr_sl</span>(sl_fit, washb_task, loss_squared_error)
CVsl_fancy <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
</div>
</div>
<div id="variable-importance" class="section level2">
<h2><span class="header-section-number">3.5</span> Variable importance</h2>
<p>Variable importance can be interesting and informative. The <code>sl3</code> <code>varimp</code>
function returns a table with variables listed in decreasing order of
importance, in which the measure of importance is based on a risk difference
between the learner fit with a permuted covariate and the learner fit with the
true covariate, across all covariates. In this manner, the larger the risk
difference, the more important the variable is in the prediction. Let’s explore
the <code>sl3</code> variable importance measurements for the <code>washb</code> data.</p>
<pre class="sourceCode r"><code class="sourceCode r">washb_varimp &lt;-<span class="st"> </span><span class="kw">varimp</span>(sl_fit, loss_squared_error)
washb_varimp <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">X</th>
<th align="right">risk_diff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">aged</td>
<td align="right">0.485</td>
</tr>
<tr class="even">
<td align="left">momedu</td>
<td align="right">0.459</td>
</tr>
<tr class="odd">
<td align="left">tr</td>
<td align="right">0.453</td>
</tr>
<tr class="even">
<td align="left">month</td>
<td align="right">0.452</td>
</tr>
<tr class="odd">
<td align="left">momheight</td>
<td align="right">0.452</td>
</tr>
<tr class="even">
<td align="left">asset_refrig</td>
<td align="right">0.451</td>
</tr>
<tr class="odd">
<td align="left">Nlt18</td>
<td align="right">0.451</td>
</tr>
<tr class="even">
<td align="left">asset_chair</td>
<td align="right">0.450</td>
</tr>
<tr class="odd">
<td align="left">fracode</td>
<td align="right">0.449</td>
</tr>
<tr class="even">
<td align="left">elec</td>
<td align="right">0.449</td>
</tr>
<tr class="odd">
<td align="left">asset_chouki</td>
<td align="right">0.448</td>
</tr>
<tr class="even">
<td align="left">asset_wardrobe</td>
<td align="right">0.448</td>
</tr>
<tr class="odd">
<td align="left">momage</td>
<td align="right">0.448</td>
</tr>
<tr class="even">
<td align="left">hfiacat</td>
<td align="right">0.448</td>
</tr>
<tr class="odd">
<td align="left">asset_moto</td>
<td align="right">0.448</td>
</tr>
<tr class="even">
<td align="left">walls</td>
<td align="right">0.447</td>
</tr>
<tr class="odd">
<td align="left">asset_tv</td>
<td align="right">0.447</td>
</tr>
<tr class="even">
<td align="left">asset_bike</td>
<td align="right">0.447</td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="right">0.447</td>
</tr>
<tr class="even">
<td align="left">asset_sewmach</td>
<td align="right">0.447</td>
</tr>
<tr class="odd">
<td align="left">floor</td>
<td align="right">0.447</td>
</tr>
<tr class="even">
<td align="left">asset_mobile</td>
<td align="right">0.447</td>
</tr>
<tr class="odd">
<td align="left">delta_momage</td>
<td align="right">0.447</td>
</tr>
<tr class="even">
<td align="left">roof</td>
<td align="right">0.446</td>
</tr>
<tr class="odd">
<td align="left">asset_table</td>
<td align="right">0.446</td>
</tr>
<tr class="even">
<td align="left">asset_khat</td>
<td align="right">0.446</td>
</tr>
<tr class="odd">
<td align="left">delta_momheight</td>
<td align="right">0.446</td>
</tr>
<tr class="even">
<td align="left">watmin</td>
<td align="right">0.446</td>
</tr>
<tr class="odd">
<td align="left">Ncomp</td>
<td align="right">0.445</td>
</tr>
</tbody>
</table>
<p>Explain output…</p>
</div>
<div id="exercise" class="section level2">
<h2><span class="header-section-number">3.6</span> Exercise</h2>
<div id="predicting-myocardial-infarction" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Predicting myocardial infarction</h3>
<p>Answer the questions below to predict myocardial infarction (<code>mi</code>) using the
available covariate data. Special thanks to David Benkeser at Emory for making
the <code>chspred</code> data (loaded below) accessible.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the data set</span>
db_data &lt;-
<span class="st">  </span><span class="kw">url</span>(<span class="st">&quot;https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv&quot;</span>)
chspred &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> db_data, <span class="dt">col_names =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>Parsed with column specification:
cols(
  .default = col_double()
)</code></pre>
<pre><code>See spec(...) for full column specifications.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># take a quick peek</span>
<span class="kw">head</span>(chspred, <span class="dv">3</span>)</code></pre>
<pre><code># A tibble: 3 x 28
  waist alcoh   hdl  beta smoke   ace   ldl   bmi aspirin  gend   age estrgn
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
1 110.   0     66.5     0     0     1  114.  28.0       0     0  73.5      0
2  90.0  0     50.1     0     0     0  104.  20.9       0     0  61.8      0
3 106.   8.42  40.5     0     0     0  166.  28.5       1     1  72.9      0
# … with 16 more variables: glu &lt;dbl&gt;, ins &lt;dbl&gt;, cysgfr &lt;dbl&gt;, dm &lt;dbl&gt;,
#   fetuina &lt;dbl&gt;, whr &lt;dbl&gt;, hsed &lt;dbl&gt;, race &lt;dbl&gt;, logcystat &lt;dbl&gt;,
#   logtrig &lt;dbl&gt;, logcrp &lt;dbl&gt;, logcre &lt;dbl&gt;, health &lt;dbl&gt;, logkcal &lt;dbl&gt;,
#   sysbp &lt;dbl&gt;, mi &lt;dbl&gt;</code></pre>
<ol style="list-style-type: decimal">
<li>Create an <code>sl3</code> task, setting myocardial infarction <code>mi</code> as the outcome and
using all available covariate data.</li>
<li>Make a library of 10 base learning algorithms. Customize hyperparameters for
at least two of your learners. Feel free to use learners from <code>sl3</code> or
<code>SuperLearner</code>.</li>
<li>Incorporate feature selection.</li>
<li>Choose an appropriate learner to fit the metalearning step.</li>
<li>Justify your selection of metalearner and base learners.</li>
<li>With the metalearner and base learners, make the super learner and train it
on the task.</li>
<li>Print your super learner fit by calling <code>print()</code> with <code>$</code>.</li>
<li>Which learner is the discrete super learner, and what is its cross-validated
risk?</li>
<li>What is the cross-validated risk of the continuous super learner?</li>
</ol>
</div>
</div>
<div id="concluding-remarks" class="section level2">
<h2><span class="header-section-number">3.7</span> Concluding Remarks</h2>
<p>The general ensemble learning approach of super learner can be applied to a
diversity of estimation and prediction problems that can be defined by a loss
function. We just discussed conditional mean estimation, and in the appendix we
delve into prediction of a conditional density, and the optimal individualized
treatment rule. Plug-in estimators of the estimand are desirable because a
plug-in estimator respects both the local and global constraints of the
statistical model. We could just plug-in the estimator returned by Super
Learner; however, this is problematic because the Super Learner estimators are
trading off bias and variance in an optimal way and as a result their bias is
essentially the rate of convergence of these algorithms, which is always slower
than <span class="math inline">\(1/\sqrt{n}\)</span>. Therefore, if we plug-in the estimator returned by super
learner into the target parameter mapping, we would end up with an
estimator which has the same bias as what we plugged in, which is greater than
<span class="math inline">\(1/\sqrt{n}\)</span>. Thus, we end up with an estimator which is not asymptotically
normal, since it does not converge to the estimand at <span class="math inline">\(1/\sqrt{n}\)</span> rate.</p>
<p>An asymptotically linear estimator has no meaningful bias ($ &lt; 1/$), and
can be written as an empirical mean in first order of a function of the data,
the influence curve, plus some negligible remainder term. Once an estimator
is asymptotically linear with an influence curve it’s normally distributed, so
the standardized estimator converges to a normal distribution with mean 0 and
variance is the variance of the influence curve. Thus, it is advantageous to
construct asymptotically linear estimators since they permit formal statistical
inference. Among the class of regular asymptotically linear estimators, there is
an optimal estimator which is an efficient estimator, and that’s the one with
influence curve equal to the canonical gradient of the path-wise derivative of
the target parameter. The canonical gradient is the direction of the path
through the data distribution where the parameter is steepest. An estimator is
efficient if and only if is asymptotically linear with influence curve equal to
the canonical gradient. One can calculate the canonical gradient with the
statistical model and the statistical target parameter. Techniques for
calculating the canonical gradient entail projecting an initial gradient on the
tangent space of the model at the particular distribution in the model in which
you want to calculate the canonical gradient.</p>
<p>Now we know what it takes to construct an efficient estimator. Namely, we need
to construct an estimator which is asymptotically linear with influence curve
the canonical gradient. There are three general classes of estimators which
succeed in constructing asymptotically linear estimators: (1) the one-step
estimator, but it is not a plug-in estimator; (2) the targeted maximum
likelihood estimator, which is a super learner targeted towards the target
parameter and it is a plug-in estimator; and (3) estimating equation based
estimators, which use the canonical gradient but as an estimating function in
the target parameter. In the following chapters, we focus on the targeted
maximum likelihood estimator and the targeted minimum loss-based estimator,
both referred to as TMLE.</p>
</div>
<div id="appendix-1" class="section level2">
<h2><span class="header-section-number">3.8</span> Appendix</h2>
<div id="exercise-solution" class="section level3">
<h3><span class="header-section-number">3.8.1</span> Exercise solution</h3>
<p>Here’s a potential solution to the exercise above.</p>
<pre class="sourceCode r"><code class="sourceCode r">chspred_task &lt;-<span class="st"> </span><span class="kw">make_sl3_Task</span>(
  <span class="dt">data =</span> chspred,
  <span class="dt">covariates =</span> <span class="kw">head</span>(<span class="kw">colnames</span>(chspred), <span class="dv">-1</span>),
  <span class="dt">outcome =</span> <span class="st">&quot;mi&quot;</span>
)

glm_learner &lt;-<span class="st"> </span>Lrnr_glm<span class="op">$</span><span class="kw">new</span>()
lasso_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="dv">1</span>)
ridge_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="dv">0</span>)
enet_learner &lt;-<span class="st"> </span>Lrnr_glmnet<span class="op">$</span><span class="kw">new</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)
curated_glm_learner &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>(<span class="dt">formula =</span> <span class="st">&quot;mi ~ smoke + beta + waist&quot;</span>)
mean_learner &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>() <span class="co"># That is one mean learner!</span>
glm_fast_learner &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()
ranger_learner &lt;-<span class="st"> </span>Lrnr_ranger<span class="op">$</span><span class="kw">new</span>()
svm_learner &lt;-<span class="st"> </span>Lrnr_svm<span class="op">$</span><span class="kw">new</span>()
xgb_learner &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>()

screen_cor &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner_screener<span class="op">$</span><span class="kw">new</span>(<span class="st">&quot;screen.corP&quot;</span>)
glm_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_cor, glm_learner)
ranger_pipeline &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Pipeline, screen_cor, ranger_learner)

stack &lt;-<span class="st"> </span><span class="kw">make_learner</span>(
  Stack,
  glm_pipeline, ranger_pipeline, glm_learner,
  lasso_learner, ridge_learner, enet_learner,
  curated_glm_learner, mean_learner, glm_fast_learner,
  ranger_learner, svm_learner, xgb_learner
)

metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_nnls)

sl &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">learners =</span> stack,
  <span class="dt">metalearner =</span> metalearner
)
sl_fit &lt;-<span class="st"> </span>sl<span class="op">$</span><span class="kw">train</span>(task)
sl_fit<span class="op">$</span><span class="kw">print</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
<!--

### Super learning of a conditional density

Suppose we want to construct a super learner of the conditional probability
distribution $g_0(a\mid W)=P_0(A=a\mid W)$, where $a\in {\cal A}$.
Let's denote the values of $a$ with $\{0,1,\ldots,K\}$. A valid loss function
for the conditional density is
\[
L(g)(O)=-\log g(A\mid W).\]
That is, $g_0=\arg\min_g P_0L(g)$, i.e., $g_0$ is the minimizer of the
expectation of the log-likelihood loss.

**Candidate estimators**

1. Candidate estimators based on multinomial logistic regression: To start with,
one can use existing parametric model based MLE and machine learning algorithms
in `R` that fit a multinomial regression. For example, parametric model
multinomial logistic regression is available in `R` so that one can already
build a rich library of such estimators based on  different candidate parametric
models. In addition, `polyclass()` is a multinomial logistic regression machine
learning algorithm in `R`.

2. Candidate estimators based on machine learning for multinomial logistic
regression: Secondly, one can use a machine learning algorithm such as
`polyclass()` in `R` that data adaptively fits a multinomial logistic
regression, which itself has tuning parameters, again generating a class of
candidate estimators.

3. Incorporating screening: Note that one can also marry any of these choices
with a screening algorithm, thereby creating more candidate estimators of
interest. The screening can be particularly important when there are many
variables.

4. Candidate estimators by fitting separate logistic regressions and using
post-normalization

* Code $A$ in terms of Bernoullis $B_k=I(A=k)$, $k=0,\ldots,K$.
* Construct an estimator $\bar{g}_{nk}$ of $\bar{g}_{0k}(W)\equiv P_0(B_k=1\mid
  W)$ using any of the logistic regression algorithms, for all $k=0,\ldots,K$.
* This implies an estimator
\[
g_n(a\mid W)=\frac{\bar{g}_{na}(W)}{\sum_{k=0}^K \bar{g}_{nk}(W)}.\]
* In other words, we simply normalize these separate logistic regression
estimators so that we obtain a valid conditional distribution.
* This generates an enormous amount of interesting algorithms, since we have
available the whole machine learning literature for binary outcome regression.

5. Candidate estimators by estimating the conditional "hazard" with pooled
logistic regression.
Note that
\[
g_0(a\mid W)=\lambda_0(a\mid W) S_0(a\mid W),\]
where \[
\lambda_0(a\mid W)=P_0(A=a\mid A\geq a,W),\]

and $S_0(a\mid W)=\prod_{s\leq a}(1-\lambda_0(s\mid W))$ is the conditional
survival function $P_0(A>a\mid W)$. So we have now parameterized the
conditional distribution of $A$, given $W$, by a conditional hazard
$\lambda_0(a\mid W)$: $g_0=g_{\lambda_0}$.

* We could now focus on constructing candidate estimators of
$\lambda_0(a\mid W)$, which implies candidate estimators of $g_0$.

* For every observation $A_i$, we can create $A_i+1$ rows of data
$(W,s,I(A_i=s))$, $s=0,\ldots,A_i$, $i=1,\ldots,n$. We now run a logistic
regression estimator based on the pooled data set, ignoring ID, where we
regress the binary outcome $I(A_i=s)$ on the covariates $(W,s)$.

* If one assumes a parametric model, then this is nothing else then using the
maximum likelihood estimator, demonstrating that ignoring the ID is not
inefficient.

* This defines now an estimator of $\lambda_0(s\mid W)=P_0(A=s\mid W,A\geq s)$
as a function of $(s,W)$.  

* Different choices of logistic regression based estimators will define
different estimators.

* The pooling across $s$ is not very sensible if $A$ is not an ordered variable.
If $A$ is categorical, we recommend to compute  a separate logistic regression
estimator of $\lambda_0(a\mid W)$ for each $a$ (i.e., stratify by $s$ in the
  above pooled data set).

* For non-categorical $A$, one could include both stratified (by level) as well
as pooled (across levels) based logistic regression estimators.

### Super learning of an optimal individualized treatment rule

* Data $O=(W,A,Y)$, and nonparametric model \mathcal{M} potentially containing
assumptions on the conditional probability distribution of $A$ given $W$
$g_0(A\mid W)$.
* Target: Optimal treatment rule $\psi_0(W)=I(B_0(W)>0)$, where
$B_0(W)=E_0(Y\mid A=1,W)-E_0(Y\mid A=0,W)$, the conditional treatment effect.
* Possible loss function for $\psi_0$ is an IPCW-loss:
\[
L_{g_0}(\psi)=\frac{I(A=\psi(W))}{g(A\mid W)}Y.\]

Indeed, $\psi_0$ is the minimizer of $EL_{g_0}(\psi)$ over all rules $\psi$.
* Construct library of candidate estimators of $\psi_0=I(B_0>0)$. This can
include estimators based on plugging in an estimator of $B_0$.
* One could also include a candidate estimator $I(B_n>0)$ where $B_n$ is a
super learner of $B_0$, e.g. based on loss function
\[
L_{g_0}(B)=\big(\frac{2A-1}{/g(A\mid W)}Y-B(W)\big)^2\]
that directly targets $B_0=\arg\min_B P_0L_{g_0}(B)$. This loss function is
still a squared error loss but its minimized by the true $B_0$.
* Estimate $g_0$ if not known.
* Compute cross-validation selector:
\[
k_n=\arg\min_k E_{B_n}P_{n,B_n}^1 L_{\hat{g}(P_{n,B_n}^0)}(\hat{\Psi}_k(P_{n,B_n}^0)).\]
where $B_n = \{0,1\}^n$ is used for a binary vector of $n$ defining sample
splits, where the validation sample is ${i:B_n(i) = 1}$ and ${i:B_n(i) = 0}$ is
the training sample. The empirical distribution $P_{n,B_n}^0$ corresponds to the
split $B_n$ of the training sample and the empirical distribution of the
validation sample is $P_{n,B_n}^1$.
* Super-learner of optimal rule $\psi_0$: $\hat{\Psi}_{k_n}(P_n)$.
-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman1996stacked">
<p>Breiman, Leo. 1996. “Stacked Regressions.” <em>Machine Learning</em> 24 (1). Springer: 49–64.</p>
</div>
<div id="ref-vdl2003unified">
<p>van der Laan, Mark J, and Sandrine Dudoit. 2003. “Unified Cross-Validation Methodology for Selection Among Estimators and a General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities and Examples.” bepress.</p>
</div>
<div id="ref-van2006oracle">
<p>Van der Vaart, Aad W, Sandrine Dudoit, and Mark J van der Laan. 2006. “Oracle Inequalities for Multi-Fold Cross Validation.” <em>Statistics &amp; Decisions</em> 24 (3). Oldenbourg Wissenschaftsverlag: 351–71.</p>
</div>
<div id="ref-wolpert1992stacked">
<p>Wolpert, David H. 1992. “Stacked Generalization.” <em>Neural Networks</em> 5 (2). Elsevier: 241–59.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tlverse.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tmle3-the-targeted-learning-framework.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/tlverse-handbook/edit/master/04-sl3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["handbook.pdf", "handbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
