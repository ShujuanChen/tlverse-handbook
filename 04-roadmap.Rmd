# (PART) Part 2: Foundations {-}

# Learning from Data: A Roadmap {#roadmap}

_Nima Hejazi_ and _Rachael Phillips_

:::: {.infobox .tlverse data-latex=""}
:::{.center data-latex=""}
**Learning Objectives**
:::

1. Translate scientific questions to statistical questions.
2. Define a statistical model based on knowledge about the scientific experiment
   or study that generated the data.
3. Identify a causal parameter as a function of the observed data distribution.
4. Explain the following statistical and causal assumptions alongside their
   implications: independent and identically distributed (i.i.d.), consistency,
   no unmeasured confounding, interference, positivity.
::::

## Introduction {-}

The roadmap of statistical learning is concerned with the process of translating
real-world scientific questions to mathematical formalisms
necessary for formulating relevant statistical inference problems.
This involves viewing data as a random variable (complete with its own
underlying probability distribution), incorporating scientific knowledge into
the choice of statistical model, selecting a statistical target parameter that
represents an answer to the scientific question of interest, and developing
efficient estimators of the statistical estimand.

## The Roadmap {#roadmap}

The roadmap is a five-stage process of defining

1. Data as a random variable with a probability distribution, $O \sim P_0$.
2. The statistical model $\M$ such that $P_0 \in \M$.
3. The statistical target parameter $\Psi$ and estimand $\Psi(P_0)$.
4. The estimator $\hat{\Psi}$ and estimate $\hat{\Psi}(P_n)$.
5. A measure of uncertainty for the estimate $\hat{\Psi}(P_n)$.

### (1) Data: A random variable with a probability distribution, $O \sim P_0$ {-}

The dataset we are confronted with is the collection of the results of a
scientific (or natural) experiment. We can view the data as a _random variable_
-- that is, if the same experiment were to be repeated, we should expect to
see a different realization of the data generated by the same underlying law
governing the experiment in question.  In particular, if the experiment were
repeated many times, the underlying probability distribution generating the
data, $P_0$, would be revealed.  The observed data on a single unit, $O$,
may be thought of as being drawn from this probability distribution $P_0$. Most
often, we observe $n$ _independent identically distributed_ (i.i.d.)
observations of the random variable $O$. Then, the observed data is the collection
$O_1, \ldots, O_n$, where the subscripts denote the individual observational
units. While not all data are i.i.d., this is certainly the most common case in
applied data analysis; moreover, there are a number of techniques for handling
non-i.i.d. data, such as establishing conditional independence, stratifying to
create distinct sets of independent data, and inferential corrections for
repeated or clustered observations, to name but a few.

It is crucial that the domain scientist (i.e., the researcher) have absolute
clarity about what is _actually known_ about the data-generating distribution
$P_0$ for a given scientific problem. This knowledge is rarely ground truth
itself --- but instead comes in the form of scientific conventions, accepted
hypotheses, and operational assumptions.  Just as critical is that this
scientific information be communicated to the statistician, whose role it is to
incorporate such knowledge into and guide any assumptions encoded within the
choice of statistical model. Unfortunately, communication between statisticians
and non-statistician researchers is often fraught with misinterpretation. This
is to be expected -- each have their own expertise! -- but proper communication
about the underlying science and the motivating study can help to ensure each
have appropriate context for a given statistical data analysis.  The roadmap
provides a mechanism by which to ensure clear communication between the
researcher and the statistician -- as such, it is an invaluable tool for such
communication.

#### The empirical probability measure, $P_n$ {-}

With $n$ i.i.d. observations in hand, we can define an empirical probability
measure, $P_n$. The empirical probability measure is an approximation of the
true probability measure, $P_0$, allowing us to learn from the observed data.
For example, we can define the empirical probability measure of a set $X$ to be
the proportion of observations that belong in $X$. That is,
\begin{equation*}
  P_n(X) = \frac{1}{n}\sum_{i=1}^{n} \I(O_i \in X)
\end{equation*}

In order to start learning from the data, we next need to ask *"What do we know
about the probability distribution of the data?"* This brings us on to Step 2.

### (2) Defining the statistical model $\M$ such that $P_0 \in \M$ {-}

The statistical model $\M$ is defined by the question we asked at the end of
Step 1 (i.e., "What do we know?"). $\M$ is the set of all possible probability
distributions that could describe the process by which our observed data have
been generated, appropriately constrained by background scientific knowledge.
Often, $\M$ is necessarily very large (i.e., nonparametric), reflecting the fact
that statistical knowledge about the data-generating process is limited.

Alternatively, if the probability distribution of the data at hand is described
by a finite number of parameters, then the statistical model is referred to as
_parametric_. Such an assumption is made, for example, by the proposition that
the random variable of interest, $O$, has a normal distribution with mean $\mu$
and variance $\sigma^2$. More generally, a parametric model may be defined as

\begin{equation*}
  \M(\theta) = \{P_{\theta} : \theta \in \R^d \},
\end{equation*}
which describes a constrained statistical model consisting of all distributions
$P_{\theta}$, that is, all distributions indexed only by the parameter $\theta$.

The assumption that the data-generating distribution has a specific, parametric
form is made quite commonly. Unfortunately, this is even the case when such
assumptions are not supported by existing or established domain knowledge. This
practice of oversimplification in the current -- and traditional -- culture of
statistical data analysis typically complicates or thwarts entirely any attempts
to rigorously answer the scientific question at hand. Why, you ask? Consider how
much knowledge one must have to _know_ (beyond a shadow of a doubt) that the
data-generating distribution underlying a given dataset is, in fact, governed by
just two parameters, as is the case with the ubiquitously
relied upon Normal distribution. When one makes such unfounded assumptions without the
appropriate background knowledge, the chosen statistical model is said to be
misspecified --- and this possible _model misspecification_ introduces bias. 
The philosophy used to justify parametric assumptions is rooted in misinterpretations 
of the often-quoted saying of George Box, that "All models are wrong but some are useful,"
which has been irresponsibly used to encourage the data analyst to make arbitrary modeling
choices.

The result of such assumptions and oversimplifications is a practice of statistical data 
science in which starkly disparate answers to the same scientific problem emerge. Practically, 
this is owed to the application of distinct statistical techniques under differing modeling 
decisions and assumptions made (but not communicated well) by different data analysts.
Even in the nascent days of statistical data analysis, it was recognized that it is "far
better [to develop] an approximate answer to the right question...than an exact
answer to the wrong question, which can always be made precise"
[@tukey1962future], though traditional statistics failed to heed this advice for
a number of decades [@donoho2017fifty]. The Targeted Learning paradigm avoids
this bias by defining the statistical model through a representation of the true
data-generating distribution underlying the observed data. The ultimate goal is
to formulate the statistical estimation problem _precisely_ (up to the
constraints imposed by available scientific knowledge), so that one
can then tailor the estimation procedure to the motivating scientific problem.
Now, on to Step 3: *"What are we trying to learn from the data?"*

### (3) The statistical target parameter $\Psi$ and estimand $\Psi(P_0)$ {-}

The statistical target parameter, $\Psi$, is defined as a mapping from the
statistical model, $\M$, to the parameter space. Usually, the parameter
space is a real number (but not necessarily so), in which case we can 
formally define the target parameter as the mapping $\Psi: \M \rightarrow \R$. The
estimand may be seen as a representation of the quantity that we wish to learn
from the data, the answer to a well-specified -- often causal -- question of
interest. In contrast to ordinary statistical estimands, causal estimands
require an extra set of assumptions to allow for their _identification from the
observed data_. Based on causal models [@pearl2009causality; @hernan2022causal],
these identification assumptions are untestable and must be justified through
a combination of knowledge about the system under study or the process by which
the experiment was conducted. These assumptions are described in greater detail
in the section on [causal target parameters](#causal).

For a simple example, consider a dataset containing observations of a survival
time on every subject, for which our question of interest is "What's the
probability that someone lives longer than five years?" We have,

\begin{equation*}
  \Psi(P_0) = \E_{P_O}(O > 5) = \int_5^{\infty} dP_O(o).
\end{equation*}

This answer to this question is the statistical **estimand, $\Psi(P_0)$**, which is
the mathematical quantity we wish to learn from the data. Once we have defined
$O$, $\M$ and $\Psi(P_0)$, we have formally defined the statistical estimation
problem. Next comes Step 4: "_How do we statistically answer the question posed
by the study that gathered the data?_"

### (4) The estimator $\hat{\Psi}$ and estimate $\hat{\Psi}(P_n)$ {-}

We typically will focus on estimation in realistic (often nonparametric) models.
To obtain a good approximation of the estimand, we need an estimator, an _a
priori_-specified learning algorithm defined as a mapping from the set of
possible distributions $\M$ to the space of the parameter of interest. That is,
$\hat{\Psi} : \M \rightarrow \R$. The estimator is a function that takes as
input the observed data, a realization of $P_n$, and gives as output a value in
the parameter space, which is the **estimate, $\hat{\Psi}(P_n)$**.
<!--
nh: idk how i feel about the above; it's a bit repetitive and feels imprecise
nh: also, the use of \hat{} over \Psi feels redundant and misleading, since it
would seem that \hat{\Psi} implies an approximate mapping, i.e., \hat{\Psi} is
_not_ \Psi, which makes it sound as though we're answering an approximate
version of the question represented by the mapping \Psi
-->

Where the estimator may be seen as an operator that maps the observed data and
corresponding empirical distribution to a value in the parameter space, the
numerical output produced by such a function is the estimate. Thus, it is an
element of the parameter space as informed by the empirical probability
distribution $P_n$ of the observed data $O_1, \ldots, O_n$. If we plug in a
realization of $P_n$ (based on a sample size $n$ of the random variable $O$), we
get back an estimate $\hat{\Psi}(P_n)$ of the true parameter value $\Psi(P_0)$.

In order to quantify the uncertainty in our estimate of the target parameter,
part of the process of conducting statistical inference, an understanding of the
sampling distribution of our estimator will be necessary. This brings us to Step
5: "_How confident should we be in our statistical answer to the scientific
question?_"

### (5) A measure of uncertainty for the estimate $\hat{\Psi}(P_n)$ {-}

Since the estimator $\hat{\Psi}$ is a function of the empirical distribution
$P_n$, the estimator itself is a random variable with a sampling distribution.
So, if we repeat the experiment of drawing $n$ observations, we would every time
end up with a different realization of our estimate. The hypothetical
distribution of these estimates is the sampling distribution of our estimator.

A primary goal in the construction of estimators is to be able to derive their
asymptotic sampling distributions through a theoretical analysis of a given
estimator. In this regard, an important property of the estimators on which we
focus is their asymptotic linearity. In particular, asymptotic linearity states 
that the difference between the estimator and the target estimand (i.e., the truth) 
can be represented, asymptotically, as an average of i.i.d. random variables:

\begin{equation*}
  \hat{\Psi}(P_n) - \Psi(P_0) = \frac{1}{n} \sum_{i=1}^n IC(P_0)(O_i) +
    o_p(n^{-1/2}),
\end{equation*}
where the influence curve (IC) is a function of the underlying distribution
$P_0$, most often through a limited set of nuisance parameters. Based on this
asymptotic approximation, the central limit theorem (CLT) can be used to show

\begin{equation*}
  \sqrt{n} \left(\hat{\Psi}(P_n) - \Psi(P_0)\right) \sim N(0, \sigma^2_{IC}),
\end{equation*}
where $\sigma^2_{IC}$ is the variance of $IC(P_0)(O)$. Given an estimate of
$\sigma^2_{IC}$, it is then possible to construct classic, _asymptotically
accurate_ Wald-type confidence intervals (CIs) and hypothesis tests. For
example, a standard $(1 - \alpha)$ CI takes the form

\begin{equation*}
  \Psi(P_n) \pm Z_{1 - \frac{\alpha}{2}} \hat{\sigma_{IC}} / \sqrt{n} \ ,
\end{equation*}
where $Z_{1 - \frac{\alpha}{2}}$ is the $(1 - \frac{\alpha}{2})^\text{th}$
quantile of the standard Normal distribution. Following convention, we will
often be interested in constructing 95% confidence intervals (corresponding to
probability mass $\alpha/2 = 0.025$ in each tail of the limit distribution);
thus, we will take $Z_{1 - \frac{\alpha}{2}} \approx 1.96$ as the quantile.

## Summary of the Roadmap {#roadmap-summary}

Data collected across $n$ i.i.d. units, $O_1, \ldots, O_n$, may be viewed as a
collection of random variables, $O$, with all copies of $O$ arising from the
same underlying probability distribution $\P_0$. This is expressed by denoting
the collection of data as being generated as $O_1, \ldots, O_n \sim P_0$. We
leverage statistical knowledge available about the experiment generating the
data to limit the statistical model, $\M$, in a realistic manner. In particular, 
we what the true data-generating distribution, $P_0$ to be contained within $\M$, 
which is itself a collection of candidate probability distributions respecting the
data-generating experiment. Often, these models -- that is, the statistical
model $\M$ -- must be very large to appropriately reflect the fact that
statistical knowledge is very limited. Hence, these _realistic_ statistical
models are often termed _semi-_ or _non-parametric_, since they are too large to
be indexed by a finite-dimensional set of parameters. Necessarily, our
statistical query must begin with, "What are we trying to learn from the data?",
a question whose answer is captured by the statistical target parameter, $\Psi$,
which maps the true data-generating distribution $P_0$ into the statistical
estimand, $\Psi(P_0)$. At this stage, the statistical estimation problem is
formally defined, allowing for the use of statistical theory to guide the
construction of optimal estimators.

## Causal Target Parameters {#causal}

In many cases, we are interested in problems that ask questions regarding the
_causal effect_ of an intervention -- whether an assigned treatment (e.g., a
prescribed drug) or a "naturally occurring" exposure (e.g., pollution from
a nearby factory) -- on a future outcome of interest. These causal
effects may be defined as summaries of the population of interest (e.g.,
population mean of a particular outcome) under differing contrasts (e.g., comparing
the treated to the untreated condition). For example, a causal effect could be
defined as the mean difference of a disease outcome between two
_causal contrasts_, counterfactual cases in which the study population were set
to uniformly experience low pollution levels for some pollutant, and in which
the same population were set to uniformly experience high levels of the same pollutant.

There are different ways of operationalizing the theoretical experiments that
generate the counterfactual data necessary for describing such causal contrasts
of interest. We could simply assume that the counterfactual outcomes exist in
theory for all treatment contrasts of interest [@neyman1938contribution;
@rubin2005causal; @imbens2015causal], which may be encoded in so-called "science
tables". Alternatively, we could consider interventions on structural causal models (SCMs)
[@pearl1995causal; @pearl2009causality], which may be represented by directed
acyclic graphs (DAGs). Both frameworks allow for the known or hypothesized set
of relationships between variables in the system under study to be encoded and
mathematically formalized.

### The Causal Model {-}

Throughout, we will focus on the use of DAGs and SCMs for the description of
causal parameters.  Estimators of statistical parameters that correspond, under
standard but untestable _identifiability_ assumptions, to these causal
parameters are introduced below. DAGs are a particularly useful tool for
visually expressing what we know about the causal relations among variables in
the system under study.  Ignoring exogenous $U$ terms (explained below), we
assume the following ordering of the variables that compose the observed data
$O$. We demonstrate the construction of a DAG below using `DAGitty`
[@textor2011dagitty]:

```{r simple-DAG, out.width = "60%"}
library(dagitty)
library(ggdag)

# make DAG by specifying dependence structure
dag <- dagitty(
  "dag {
    W -> A
    W -> Y
    A -> Y
    W -> A -> Y
  }"
)
exposures(dag) <- c("A")
outcomes(dag) <- c("Y")
tidy_dag <- tidy_dagitty(dag)

# visualize DAG
ggdag(tidy_dag) +
  theme_dag()
```

While DAGs like the above provide a convenient means by which to express the
causal relations between variables, these same causal relations can be
equivalently represented by an SCM:
\begin{align*}
  W &= f_W(U_W) \\
  A &= f_A(W, U_A) \\
  Y &= f_Y(W, A, U_Y),
\end{align*}
where the $f$'s are unspecified deterministic functions that generate the
corresponding random variables as a function of the variable's "parents" (i.e.,
upstream nodes with arrows into the given random variable) in the DAG, and the
unobserved, exogenous error terms (i.e., the $U$'s). An SCM may be thought of as
a representation of the algorithm that produces the data, $O$, in the population
of interest.  Much of statistics and data science is devoted to discovering
properties of this system of equations (e.g., estimation of the functional form
$f_Y$ governing the outcome variable $Y$).

<!--
where $U_W$, $U_A$, and $U_Y$ represent the unmeasured exogenous background
characteristics that influence the value of each variable. In the NPSEM, $f_W$,
$f_A$ and $f_Y$ denote that each variable (for $W$, $A$ and $Y$, respectively)
is a function of its parents and unmeasured background characteristics, but
note that there is no imposition of any particular functional constraints(e.g.,
linear, logit-linear, only one interaction, etc.). For this reason, they are
called non-parametric structural equation models (NPSEMs). The DAG and set of
nonparametric structural equations represent exactly the same information and
so may be used interchangeably.
-->

The first hypothetical experiment we will consider is assigning exposure to the
entire population and observing the outcome, and then withholding exposure to
the same population and observing the outcome. This corresponds to a comparison
of the outcome distribution in the population under two distinct interventions:

1. $A$ is set to $1$ for all individuals, and
2. $A$ is set to $0$ for all individuals.

These interventions may be thought of as operations that imply changes
to the structural equations in the system under study. For the case $A = 1$, we
have
\begin{align*}
  W &= f_W(U_W) \\
  A &= 1 \\
  Y(1) &= f_Y(W, 1, U_Y) \ ,
\end{align*}
while, for the case $A=0$,
\begin{align*}
  W &= f_W(U_W) \\
  A &= 0 \\
  Y(0) &= f_Y(W, 0, U_Y) \ .
\end{align*}

In these equations, $A$ is no longer a function of $W$ because the intervention
on the system set $A$ deterministically
to one of the values $1$ or $0$ consistent with the intervention performed.  The
new symbols $Y(1)$ and $Y(0)$ indicate the values the outcome variable would
take in the population of interest when it is generated by removing the
contribution of $A$ to $f_Y$ and instead setting $A$ to the values $1$ and $0$,
respectively. The variables $Y(1)$ and $Y(0)$ are often called counterfactuals
(since they arise from interventions that run contrary to fact) and are, in
other frameworks, called the _potential outcomes_ of $Y$
[@neyman1938contribution; rubin2005causal; imbens2015causal].  The difference in
the counterfactual means of the outcome under these two interventions defines a
well known causal parameter that is most often called the "average treatment
effect" (ATE) and is denoted

\begin{equation}
  ATE = \E_X(Y(1) - Y(0)),
  (\#eq:ate)
\end{equation}
where $\E_X$ is the mean under the theoretical (unobservable) complete data $X= (W, Y(1), Y(0))$. 
Note that the complete data structure $X$ is, by its very
definition, unobservable since one can never observe both of $Y(1)$ and $Y(0)$
for the same observational unit.

Note, we can define much more complicated interventions on SCMs, such as
interventions based upon dynamic rules (which assign particular interventions
based on a function of the covariates $W$), stochastic rules (which can even account for the
natural value of $A$, observed in the absence of the intervention), and much
more.  Each results in a different target parameter and entails different
identifiability assumptions discussed below.

### Identifiability {-}

Since we can never observe both $Y(0)$ (the counterfactual outcome when $A=0$)
and $Y(1)$ (similarly, the counterfactual outcome when $A=1$), we cannot
estimate the quantity in Equation \@ref(eq:ate) directly. This is called the
_Fundamental Problem of Causal Inference_ [@holland1986statistics]. Thus, one of
the primary activities in causal inference is to _identify_ the assumptions
necessary to express causal quantities of interest as functions of the
data-generating distribution of the observed data. To do this, we must make
assumptions under which such quantities may be estimated from the observed data
$O \sim P_0$ and its corresponding data-generating distribution $P_0$.
Fortunately, given the causal model specified in the SCM above, we can, with a
handful of untestable assumptions, estimate the ATE from observational data.
These assumptions may be summarized as follows.

::: {#consist-ass .definition name="Consistency"}
The outcome for unit $i$ is $Y_i(a)$ whenever $A_i = a$, which may be thought of
as "no other versions of treatment" or "no side effects of treatment."
:::

::: {#interf-ass .definition name="No Interference"}
The outcome for unit $i$, $Y_i$, cannot be affected by the exposure of unit $j$,
$A_j$, for all $i \neq j$.
:::

::: {#noconf-ass .definition name="No Unmeasured Confounding"}
$A \perp Y(a) \mid W$ for all $a \in \mathcal{A}$, which states that the
potential outcomes $(Y(a) : a \in \mathcal{A})$ arise independently from
exposure status $A$, conditional on the observed covariates $W$. This is the
analog of the _randomization_ assumption in data arising from natural
experiments, ensuring that the effect of $A$ on $Y$ can be disentangled from
that of $W$ on $Y$, even though $W$ affects both.
:::

::: {#posit-ass .definition name="Positivity (or Overlap)"}
All observed units, across strata defined by $W$, must have a bounded
(non-deterministic) probability of receiving treatment -- that is, $0 < \P(A = a
\mid W) < 1$ for all $a$ and $W$).
:::

Technically speaking, only the latter two of these assumptions are necessary
when working within the SCM framework, as the first two are implied properties
of an SCM for i.i.d. data (if you're really curious, see this commentary of
@pearl2010brief for an extended philosophical discussion). We introduce all four
identification assumptions because they are most often considered together, and
all four are necessary when working within the potential outcomes framework.

Given these assumptions, the ATE may be re-written as a function of $P_0$ --
specifically

\begin{align}
  \psi_{\text{ATE} &= \E_0(Y(1) - Y(0)) \\ \nonumber
                   &= \E_0 \left(\E_0[Y \mid A = 1, W] -
                      \E_0[Y \mid A = 0, W]\right).
  (\#eq:estimand)
\end{align}
In words, the ATE is the mean difference in the predicted outcome values for
each subject, under the contrast of treatment conditions ($A = 0$ versus $A =
1$), in the population (when averaged over all observations). Thus, a parameter
of a theoretical complete (or "full") data distribution can be represented as an
estimand of the observed data distribution. Significantly, there is nothing
about the representation in Equation \@ref(eq:estimand) that requires
parameteric assumptions; thus, the regression functions on the right hand side
may be estimated without restrictive assumptions about their underlying
functional forms.  With different parameters, there will be potentially
different identifiability assumptions and the resulting estimands can be
functions of different components of $P_0$. We discuss several more complex
estimands in subsequent sections.
