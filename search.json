[{"path":"index.html","id":"about-this-book","chapter":"About this book","heading":"About this book","text":"Targeted Learning R: Causal Data Science tlverse Software\nEcosystem open source, reproducible electronic handbook applying \nTargeted Learning methodology practice using tlverse software\necosystem. work currently early draft\nphase available facilitate input community. view \ncontribute available content, consider visiting GitHub\nrepository.\n","code":""},{"path":"index.html","id":"outline","chapter":"About this book","heading":"0.1 Outline","text":"contents handbook meant serve reference guide \napplied research well materials can taught series short\ncourses focused applications Targeted Learning. section\nintroduces set distinct causal questions, motivated case study,\nalongside statistical methodology software assessing causal claim \ninterest. (evolving) set materials includesMotivation: need statistical\nrevolutionThe Roadmap introductory case study: WASH Beneifits dataIntroduction tlverse software\necosystemCross-validation origami\npackage (work progress)Ensemble machine learning \nsl3 packageTargeted learning causal inference \ntmle3 packageOptimal treatments regimes \ntmle3mopttx packageStochastic treatment regimes \ntmle3shift packageCausal mediation analysis \ntmle3mediate package\n(work progress)Coda: need statistical\nrevolution","code":""},{"path":"index.html","id":"what-this-book-is-not","chapter":"About this book","heading":"What this book is not","text":"focus work providing -depth technical descriptions\ncurrent statistical methodology recent advancements. Instead, goal \nconvey key details state---art techniques manner \nclear complete, without burdening reader extraneous information.\nhope presentations herein serve references researchers\n– methodologists domain specialists alike – empower deploy\ncentral tools Targeted Learning efficient manner. technical\ndetails -depth descriptions classical theory recent advances\nfield Targeted Learning, interested reader invited consult\nvan der Laan Rose (2011) /van der Laan Rose (2018) appropriate. primary literature\nstatistical causal inference, machine learning, non/semiparametric theory\ninclude many recent advances Targeted Learning related areas.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"About this book","heading":"About the authors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"About this book","heading":"Mark van der Laan","text":"Mark van der Laan, PhD, Professor Biostatistics Statistics UC\nBerkeley. research interests include statistical methods computational\nbiology, survival analysis, censored data, adaptive designs, targeted maximum\nlikelihood estimation, causal inference, data-adaptive loss-based learning, \nmultiple testing. research group developed loss-based super learning \nsemiparametric models, based cross-validation, generic optimal tool \nestimation infinite-dimensional parameters, nonparametric density\nestimation prediction censored uncensored data. Building \nwork, research group developed targeted maximum likelihood estimation\ntarget parameter data-generating distribution arbitrary\nsemiparametric nonparametric models, generic optimal methodology \nstatistical causal inference. recently, Mark’s group focused \npart development centralized, principled set software tools \ntargeted learning, tlverse.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"About this book","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"About this book","heading":"Nima Hejazi","text":"Nima Hejazi PhD candidate biostatistics, working collaborative\ndirection Mark van der Laan Alan Hubbard. Nima affiliated UC\nBerkeley’s Center Computational Biology NIH Biomedical Big Data training\nprogram, well Fred Hutchinson Cancer Research Center. Previously,\nearned MA Biostatistics BA (majors Molecular Cell\nBiology, Psychology, Public Health), UC Berkeley. research\ninterests fall intersection causal inference machine learning,\ndrawing ideas non/semi-parametric estimation large, flexible\nstatistical models develop efficient robust statistical procedures \nevaluating complex target estimands observational randomized studies.\nParticular areas current emphasis include mediation/path analysis,\noutcome-dependent sampling designs, targeted loss-based estimation, vaccine\nefficacy trials. Nima also passionate statistical computing open\nsource software development applied statistics.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"About this book","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"About this book","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology \nChemistry minor BA Mathematics Spanish minor. Rachael’s research\nfocuses narrowing gap theory application modern\nstatistics real-world data science. Specifically, Rachael motivated \nissues arising healthcare, leverages strategies rooted causal\ninference nonparametric estimation build clinician-tailored,\nmachine-driven solutions. Rachael also passionate free, online-mediated\neducation corresponding pedagogy.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"About this book","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics, former head Division \nBiostatistics UC Berkeley, head data analytics core UC Berkeley’s\nSuperFund research program. current research interests include causal\ninference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"repro","chapter":"About this book","heading":"0.2 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.0.2 (2020-06-22),\npandoc version 2.2, \nfollowing packages:","code":""},{"path":"index.html","id":"learn","chapter":"About this book","heading":"0.3 Learning resources","text":"effectively utilize handbook, reader need fully trained\nstatistician begin understanding applying methods. However, \nhighly recommended reader understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.learning R programming language recommend following (free)\nintroductory resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGarret Grolemund Hadley Wickham’s R Data\nScienceFor general introduction causal inference, recommendMiguel . Hernán James M. Robins’ Causal Inference, forthcoming\n2020Jason . Roy’s Crash Course Causality: Inferring Causal Effects \nObservational Data \nCoursera","code":""},{"path":"index.html","id":"setup","chapter":"About this book","heading":"0.4 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"About this book","heading":"0.4.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"About this book","heading":"0.4.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"About this book","heading":"0.4.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"About this book","heading":"0.4.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"motivation.html","id":"motivation","chapter":"Motivation","heading":"Motivation","text":"“One enemy robust science humanity — appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Editorial (2015b)Scientific research unique point history. need improve rigor\nreproducibility field greater ever; corroboration moves\nscience forward, yet growing alarm results \nreproduced report false discoveries (Baker 2016). Consequences \nmeeting need result decline rate scientific\nprogression, reputation sciences, public’s trust \nfindings (Munafò et al. 2017; Editorial 2015a).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state culture data analysis statistics\nactually enables human bias improper model selection. hypothesis\ntests estimators derived statistical models, obtain valid\nestimates inference critical statistical model contains \nprocess generated data. Perhaps treatment randomized \ndepended small number baseline covariates; knowledge \ncan incorporated model. Alternatively, maybe data \nobservational, knowledge data-generating process (DGP).\ncase, statistical model contain data\ndistributions. practice; however, models selected based knowledge\nDGP, instead models often selected based (1) p-values \nyield, (2) convenience implementation, /(3) analysts loyalty\nparticular model. practice “cargo-cult statistics — \nritualistic miming statistics rather conscientious practice,”\n(Stark Saltelli 2018) characterized arbitrary modeling choices, even though\nchoices often result different answers research question.\n, “increasingly often, [statistics] used instead aid \nabet weak science, role can perform well used mechanically \nritually,” opposed original purpose safeguarding weak\nscience (Stark Saltelli 2018). presents fundamental drive behind epidemic\nfalse findings scientific research suffering (van der Laan Starmans 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite\" education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.\"— Szucs Ioannidis (2017)team University California, Berkeley, uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nspreading rapidly many students colleagues greatly\nenriched field, aptly named “Targeted Learning” methodology targets \nscientific question hand counter current culture \n“convenience statistics” opens door biased estimation, misleading\nresults, false discoveries. Targeted Learning restores fundamentals \nformalized field statistics, facts statistical\nmodel represents real knowledge experiment generated data,\ntarget parameter represents seeking learn data \nfeature distribution generated (van der Laan Starmans 2014). way,\nTargeted Learning defines truth establishes principled standard \nestimation, thereby inhibiting --human biases (e.g., hindsight bias,\nconfirmation bias, outcome bias) infiltrating analysis.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)objective handbook provide training students, researchers,\nindustry professionals, faculty science, public health, statistics, \nfields empower necessary knowledge skills utilize \nsound methodology Targeted Learning — technique provides tailored\npre-specified machines answering queries, data analysis \ncompletely reproducible, estimators efficient, minimally biased, \nprovide formal statistical inference.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives, remains critical acknowledge \nrole robust software plays allowing practitioners direct access \npublished results. recall “article…scientific publication \nscholarship , merely advertising scholarship. \nactual scholarship complete software development environment \ncomplete set instructions generated figures,” thus making \navailability adoption robust statistical software key enhancing \ntransparency inherent aspect science (Buckheit Donoho 1995).statistical methodology readily accessible practice, \ncrucial accompanied robust user-friendly software\n(Pullenayegum et al. 2016; Stromberg others 2004). tlverse software\necosystem developed fulfill need Targeted Learning\nmethodology. software facilitate computationally reproducible\nefficient analyses, also tool Targeted Learning education since\nworkflow mirrors methodology. particular, tlverse\nparadigm focus implementing specific estimator small set \nrelated estimators. Instead, focus exposing statistical framework\nTargeted Learning — R packages tlverse ecosystem\ndirectly model key objects defined mathematical theoretical\nframework Targeted Learning. ’s , tlverse R packages share \ncore set design principles centered extensibility, allowing \nused conjunction built upon one cohesive\nfashion.handbook, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuitive explanation readers build toolbox applying Targeted\nLearning statistical methodology, translate real-world causal\ninference analyses. preliminaries required prior learning\nendeavor – made available list recommended learning\nresources.","code":""},{"path":"intro.html","id":"intro","chapter":"1 The Roadmap for Targeted Learning","heading":"1 The Roadmap for Targeted Learning","text":"","code":""},{"path":"intro.html","id":"learning-objectives","chapter":"1 The Roadmap for Targeted Learning","heading":"Learning Objectives","text":"end chapter able :Translate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following causal statistical assumptions \nimplications: ..d., consistency, interference, positivity, SUTVA.","code":""},{"path":"intro.html","id":"introduction","chapter":"1 The Roadmap for Targeted Learning","heading":"Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"roadmap","chapter":"1 The Roadmap for Targeted Learning","heading":"1.1 The Roadmap","text":"Following roadmap process five stages.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\mathcal{M}\\) \\(P_0 \\\\mathcal{M}\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"1 The Roadmap for Targeted Learning","heading":"(1) Data: A random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set ’re confronted result experiment can\nview data random variable, \\(O\\), repeat experiment\ndifferent realization experiment. particular, \nrepeat experiment many times learn probability distribution,\n\\(P_0\\), data. , observed data \\(O\\) probability distribution\n\\(P_0\\) \\(n\\) independent identically distributed (..d.) observations \nrandom variable \\(O; O_1, \\ldots, O_n\\). Note data ..d.,\nways handle non-..d. data, establishing conditional\nindependence, stratifying data create sets identically distributed data,\netc. crucial researchers absolutely clear actually\nknow data-generating distribution given problem interest.\nUnfortunately, communication statisticians researchers often\nfraught misinterpretation. roadmap provides mechanism \nensure clear communication research statistician – truly helps\ncommunication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"1 The Roadmap for Targeted Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure \\(P_0\\), allowing us learn data. \nexample, can define empirical probability measure set, \\(\\), \nproportion observations end \\(\\). ,\n\\[\\begin{equation*}\n  P_n() = \\frac{1}{n}\\sum_{=1}^{n} \\mathbb{}(O_i \\)\n\\end{equation*}\\]order start learning something, need ask “know \nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"the-statistical-model-mathcalm-such-that-p_0-in-mathcalm","chapter":"1 The Roadmap for Targeted Learning","heading":"(2) The statistical model \\(\\mathcal{M}\\) such that \\(P_0 \\in \\mathcal{M}\\)","text":"statistical model \\(\\mathcal{M}\\) defined question asked \nend Step 1. defined set possible probability\ndistributions observed data. Often \\(\\mathcal{M}\\) large (possibly\ninfinite-dimensional), reflect fact statistical knowledge \nlimited. case \\(\\mathcal{M}\\) infinite-dimensional, deem \nnonparametric statistical model.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model parametric. \ncase, prescribe belief random variable \\(O\\) \nobserved , e.g., normal distribution mean \\(\\mu\\) variance\n\\(\\sigma^2\\). formally, parametric model may defined\n\\[\\begin{equation*}\n  \\mathcal{M} = \\{P_{\\theta} : \\theta \\\\mathcal{R}^d \\}\n\\end{equation*}\\]Sadly, assumption data-generating distribution specific,\nparametric forms --common, even leap faith. \npractice oversimplification current culture data analysis\ntypically derails attempt trying answer scientific question \nhand; alas, statements ever-popular quip Box “models \nwrong useful,” encourage data analyst make arbitrary choices\neven often force significant differences answers \nestimation problem. Targeted Learning paradigm suffer \nbias since defines statistical model representation true\ndata-generating distribution corresponding observed data.Now, Step 3: “trying learn data?”","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"1 The Roadmap for Targeted Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\mathcal{M}\\), parameter space (.e., real number)\n\\(\\mathcal{R}\\). , \\(\\Psi: \\mathcal{M}\\rightarrow\\mathbb{R}\\). estimand\nmay seen representation quantity wish learn \ndata, answer well-specified (often causal) question interest. \ncontrast purely statistical estimands, causal estimands require\nidentification observed data, based causal models include\nseveral untestable assumptions, described detail section \ncausal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\n\\[\\begin{equation*}\n  \\Psi(P_0) = \\mathbb{P}(O > 5)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity ’re trying learn data. defined \\(O\\),\n\\(\\mathcal{M}\\) \\(\\Psi(P_0)\\) formally defined statistical\nestimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"1 The Roadmap for Targeted Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"obtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\mathcal{M}_{NP}\\) (\\(P_n \\\\mathcal{M}_{NP}\\)), parameter space\nparameter interest. , \\(\\hat{\\Psi} : \\mathcal{M}_{NP} \\rightarrow \\mathbb{R}^d\\). estimator function takes input\nobserved data, realization \\(P_n\\), gives output value \nparameter space, estimate, \\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"1 The Roadmap for Targeted Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical\ndistribution \\(P_n\\), estimator random variable sampling\ndistribution. , repeat experiment drawing \\(n\\) observations \nevery time end different realization estimate \nestimator sampling distribution. sampling distribution estimators\ncan theoretically validated approximately normally distributed \nCentral Limit Theorem (CLT).class Central Limit Theorems (CLTs) statements regarding \nconvergence sampling distribution estimator normal\ndistribution. general, construct estimators whose limit sampling\ndistributions may shown approximately normal distributed sample size\nincreases. large enough \\(n\\) ,\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\sim N \\left(\\Psi(P_0), \\frac{\\sigma^2}{n}\\right),\n\\end{equation*}\\]\npermitting statistical inference. Now, can proceed quantify \nuncertainty chosen estimator construction hypothesis tests \nconfidence intervals. example, may construct confidence interval \nlevel \\((1 - \\alpha)\\) estimand, \\(\\Psi(P_0)\\):\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\left(\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\end{equation*}\\]\n\\(z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\)\nquantile standard normal distribution. Often, interested \nconstructing 95% confidence intervals, corresponding mass \\(\\alpha = 0.05\\) \neither tail limit distribution; thus, typically take\n\\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).Note: typically estimate standard error,\n\\(\\frac{\\sigma}{\\sqrt{n}}\\).95% confidence interval means take 100 different samples\nsize \\(n\\) compute 95% confidence interval sample, \napproximately 95 100 confidence intervals contain estimand,\n\\(\\Psi(P_0)\\). practically, means 95% probability\n(95% confidence) confidence interval procedure contain \ntrue estimand. However, single estimated confidence interval either \ncontain true estimand .","code":""},{"path":"intro.html","id":"roadmap-summary","chapter":"1 The Roadmap for Targeted Learning","heading":"1.2 Summary of the Roadmap","text":"Data, \\(O\\), viewed random variable probability distribution.\noften \\(n\\) units independent identically distributed units \nprobability distribution \\(P_0\\), \\(O_1, \\ldots, O_n \\sim P_0\\). \nstatistical knowledge experiment generated data. \nwords, make statement true data distribution \\(P_0\\) falls \ncertain set called statistical model, \\(\\mathcal{M}\\). Often sets \nlarge statistical knowledge limited - hence, statistical models\noften infinite dimensional models. statistical query , “\ntrying learn data?” denoted statistical target parameter,\n\\(\\Psi\\), maps \\(P_0\\) estimand, \\(\\Psi(P_0)\\). point \nstatistical estimation problem formally defined now need\nstatistical theory guide us construction estimators. ’s lot\nstatistical theory review course , particular, relies\nCentral Limit Theorem, allowing us come estimators \napproximately normally distributed also allowing us come statistical\ninference (.e., confidence intervals hypothesis tests).","code":""},{"path":"intro.html","id":"causal","chapter":"1 The Roadmap for Targeted Learning","heading":"1.3 Causal Target Parameters","text":"many cases, interested problems ask questions regarding \neffect intervention future outcome interest. questions can\nrepresented causal estimands.","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"1 The Roadmap for Targeted Learning","heading":"The Causal Model","text":"formalizing data statistical model, can define causal\nmodel express causal parameters interest. Directed acyclic graphs (DAGs)\none useful tool express know causal relations among\nvariables. Ignoring exogenous \\(U\\) terms (explained ), assume \nfollowing ordering variables observed data \\(O\\). \nusing DAGitty (Textor, Hardt, Knüppel 2011):directed acyclic graphs (DAGs) like provide convenient means \nvisualize causal relations variables, causal relations\namong variables can represented via set structural equations, \ndefine non-parametric structural equation model (NPSEM):\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(U_W\\), \\(U_A\\), \\(U_Y\\) represent unmeasured exogenous background\ncharacteristics influence value variable. NPSEM, \\(f_W\\),\n\\(f_A\\) \\(f_Y\\) denote variable (\\(W\\), \\(\\) \\(Y\\), respectively)\nfunction parents unmeasured background characteristics, note\nimposition particular functional constraints(e.g.,\nlinear, logit-linear, one interaction, etc.). reason, \ncalled non-parametric structural equation models (NPSEMs). DAG set \nnonparametric structural equations represent exactly information \nmay used interchangeably.first hypothetical experiment consider assigning exposure \nwhole population observing outcome, assigning exposure \nwhole population observing outcome. nonparametric structural\nequations, corresponds comparison outcome distribution \npopulation two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new nonparametric structural equation models. \ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\ncase \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervened system, setting \\(\\) deterministically either values\n\\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome variable \npopulation generated respective NPSEMs ; \noften called counterfactuals (since run contrary--fact). difference\nmeans outcome two interventions defines \nparameter often called “average treatment effect” (ATE), denoted\n\\[\\begin{equation}\\label{eqn:ate}\n  ATE = \\mathbb{E}_X(Y(1)-Y(0)),\n\\end{equation}\\]\n\\(\\mathbb{E}_X\\) mean theoretical (unobserved) full data\n\\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":""},{"path":"intro.html","id":"identifiability","chapter":"1 The Roadmap for Targeted Learning","heading":"Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate  directly. Instead, make assumptions \nquantity may estimated observed data \\(O \\sim P_0\\) \ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified NPSEM , can, handful untestable assumptions,\nestimate ATE, even observational data. assumptions may \nsummarized follows.causal graph implies \\(Y() \\perp \\) \\(\\\\mathcal{}\\), \nrandomization assumption. case observational data, \nanalogous assumption strong ignorability unmeasured confounding\n\\(Y() \\perp \\mid W\\) \\(\\\\mathcal{}\\);Although represented causal graph, also required assumption\ninterference units, , outcome unit \\(\\) \\(Y_i\\) \naffected exposure unit \\(j\\) \\(A_j\\) unless \\(=j\\);Consistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”;also necessary observed units, across strata defined \\(W\\),\nbounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\mathbb{P}(= \\mid W) < 1\\) \\(\\) \\(W\\)). assumption\nreferred positivity overlap.Remark: Together, (2) (3), assumptions interference \nconsistency, respectively, jointly referred stable unit\ntreatment value assumption (SUTVA).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\n\\[\\begin{equation}\\label{eqn:estimand}\n  ATE = \\mathbb{E}_0(Y(1) - Y(0)) = \\mathbb{E}_0\n    \\left(\\mathbb{E}_0[Y \\mid = 1, W] - \\mathbb{E}_0[Y \\mid = 0, W]\\right).\n\\end{equation}\\]\nwords, ATE difference predicted outcome values subject, \ncontrast treatment conditions (\\(= 0\\) vs. \\(= 1\\)), population,\naveraged observations. Thus, parameter theoretical “full” data\ndistribution can represented estimand observed data\ndistribution. Significantly, nothing representation \n requires parameteric assumptions; thus, regressions\nright hand side may estimated freely machine learning. \ndifferent parameters, potentially different identifiability\nassumptions resulting estimands can functions different components\n\\(P_0\\). discuss several complex estimands later sections \nhandbook.","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"2 Welcome to the tlverse","heading":"2 Welcome to the tlverse","text":"","code":""},{"path":"tlverse.html","id":"learning-objectives-1","chapter":"2 Welcome to the tlverse","heading":"Learning Objectives","text":"Understand tlverse ecosystem conceptuallyIdentify core components tlverseInstall tlverse R packagesUnderstand Targeted Learning roadmapLearn WASH Benefits example data","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"2 Welcome to the tlverse","heading":"What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"anatomy-of-the-tlverse","chapter":"2 Welcome to the tlverse","heading":"Anatomy of the tlverse","text":"main packages represent core tlverse:sl3: Modern Super Learning Pipelines\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.addition engines drive development tlverse, \nsupporting packages – particular, two…origami: Generalized Framework \nCross-Validation\n? generalized framework flexible cross-validation\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.\n? generalized framework flexible cross-validationWhy? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.\n? framework delayed computations (futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.key principle tlverse extensibility. , want support\nnew Targeted Learning estimators developed. model \nnew estimators implemented additional packages using core packages\n. currently two featured examples :tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule\n? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome ruleWhy? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Shift Interventions \ntlverse\n? Shift interventions continuous treatments\n? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.\n? Shift interventions continuous treatmentsWhy? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"2 Welcome to the tlverse","heading":"2.1 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use usethis package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token.Create Personal Access Token simply clicking “Generate token” \nbottom page.Copy Personal Access Token, long string lowercase letters \nnumbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio. able \naccess .Renviron file command, try inputting\nSys.setenv(GITHUB_PAT = ) Personal Access Token inserted \nstring equals symbol; error, skip \nstep 8..Renviron file, type GITHUB_PAT= paste Personal\nAccess Token equals symbol space..Renviron file, press enter key ensure .Renviron\nends newline.Save .Renviron file.Restart R changes take effect. can restart R via drop-\nmenu “Session” tab. “Session” tab top RStudio\ninterface.following steps, able successfully install \npackage threw error .","code":"Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`."},{"path":"data.html","id":"data","chapter":"3 Datasets","heading":"3 Datasets","text":"","code":""},{"path":"data.html","id":"wash","chapter":"3 Datasets","heading":"3.1 WASH Benefits Example Dataset","text":"data come study effect water quality, sanitation, hand\nwashing, nutritional interventions child development rural Bangladesh\n(WASH Benefits Bangladesh): cluster randomized controlled trial\n(Tofail et al. 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block randomized,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;hand-washing soap;combined water, sanitation, hand washing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.handbook, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nNCT01590095.purposes handbook, start treating data independent \nidentically distributed (..d.) random draws large target\npopulation. , available options, account clustering \ndata (within sampled geographic units), , simplification, avoid \ndetails handbook, although modifications methodology biased\nsamples, repeated measures, etc., available.28 variables measured, 1 variable set outcome \ninterest. outcome, \\(Y\\), weight--height Z-score (whz dat);\ntreatment interest, \\(\\), randomized treatment group (tr \ndat); adjustment set, \\(W\\), consists simply everything else. \nresults observed data structure \\(n\\) ..d. copies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables given just , complete\nsmall visualization describing marginal characteristics \ncovariate. Note asset variables reflect socio-economic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls); , course, design.","code":"#> # A tibble: 6 x 28\n#>     whz tr    fracode month  aged sex   momage momedu momheight hfiacat Nlt18\n#>   <dbl> <chr> <chr>   <dbl> <dbl> <chr>  <dbl> <chr>      <dbl> <chr>   <dbl>\n#> 1  0    Cont… N05265      9   268 male      30 Prima…      146. Food S…     3\n#> 2 -1.16 Cont… N05265      9   286 male      25 Prima…      149. Modera…     2\n#> 3 -1.05 Cont… N08002      9   264 male      25 Prima…      152. Food S…     1\n#> 4 -1.26 Cont… N08002      9   252 fema…     28 Prima…      140. Food S…     3\n#> 5 -0.59 Cont… N06531      9   336 fema…     19 Secon…      151. Food S…     2\n#> # … with 1 more row, and 17 more variables: Ncomp <dbl>, watmin <dbl>,\n#> #   elec <dbl>, floor <dbl>, walls <dbl>, roof <dbl>, asset_wardrobe <dbl>,\n#> #   asset_table <dbl>, asset_chair <dbl>, asset_khat <dbl>, asset_chouki <dbl>,\n#> #   asset_tv <dbl>, asset_refrig <dbl>, asset_bike <dbl>, asset_moto <dbl>,\n#> #   asset_sewmach <dbl>, asset_mobile <dbl>"},{"path":"data.html","id":"ist","chapter":"3 Datasets","heading":"3.2 International Stroke Trial Example Dataset","text":"International Stroke Trial database contains individual patient data \nInternational Stroke Trial (IST), multi-national randomized trial\nconducted 1991 1996 (pilot phase 1991 1993) aimed\nassess whether early administration aspirin, heparin, aspirin \nheparin, neither influenced clinical course acute ischaemic stroke\n(Sandercock et al. 1997). IST dataset includes data 19,435 patients\nacute stroke, 99% complete follow-. De-identified data \navailable download https://datashare..ed.ac.uk/handle/10283/128. \nstudy described detail Sandercock, Niewada, Członkowska (2011). example\ndata handbook considers sample 5,000 patients binary\noutcome recurrent ischemic stroke within 14 days randomization. Also\nexample data, ensure subjects missing outcome.26 variables measured, outcome interest, \\(Y\\), indicates recurrent\nischemic stroke within 14 days randomization (DRSISC ist); \ntreatment interest, \\(\\), randomized aspirin vs. aspirin treatment\nallocation (RXASP ist); adjustment set, \\(W\\), consists \nvariables measured baseline. data, outcome occasionally\nmissing, need create variable indicating missingness\n(\\(\\Delta\\)) analyses tlverse, since automatically\ndetected NA present outcome. observed data structure can\ndenoted \\(n\\) ..d. copies \\(O_i = (W_i, A_i, \\Delta_i, \\Delta Y_i)\\), \n\\(= 1, \\ldots, n\\), \\(\\Delta\\) denotes binary indicator outcome\nobserved.Like , can summarize variables measured IST sample data set\nskimr:(#tab:skim_ist_data)Data summaryVariable type: characterVariable type: numeric","code":"#> # A tibble: 6 x 26\n#>   RDELAY RCONSC SEX     AGE RSLEEP RATRIAL RCT   RVISINF RHEP24 RASP3  RSBP\n#>    <dbl> <chr>  <chr> <dbl> <chr>  <chr>   <chr> <chr>   <chr>  <chr> <dbl>\n#> 1     46 F      F        85 N      N       N     N       Y      N       150\n#> 2     33 F      M        71 Y      Y       Y     Y       N      Y       180\n#> 3      6 D      M        88 N      Y       N     N       N      N       140\n#> 4      8 F      F        68 Y      N       Y     Y       N      N       118\n#> 5     13 F      M        60 N      N       Y     N       N      N       140\n#> # … with 1 more row, and 15 more variables: RDEF1 <chr>, RDEF2 <chr>,\n#> #   RDEF3 <chr>, RDEF4 <chr>, RDEF5 <chr>, RDEF6 <chr>, RDEF7 <chr>,\n#> #   RDEF8 <chr>, STYPE <chr>, RXHEP <chr>, REGION <chr>,\n#> #   MISSING_RATRIAL_RASP3 <dbl>, MISSING_RHEP24 <dbl>, RXASP <dbl>,\n#> #   DRSISC <dbl>"},{"path":"data.html","id":"NHEFS","chapter":"3 Datasets","heading":"3.3 NHANES I Epidemiologic Follow-up Study (NHEFS)","text":"data National Health Nutrition Examination Survey (NHANES)\nData Epidemiologic Follow-Study. coming soon.snapshot data set shown :(#tab:skim_nhefs_data)Data summaryVariable type: numeric","code":"#> # A tibble: 6 x 64\n#>    seqn  qsmk death yrdth modth dadth   sbp   dbp   sex   age  race income\n#>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n#> 1   233     0     0    NA    NA    NA   175    96     0    42     1     19\n#> 2   235     0     0    NA    NA    NA   123    80     0    36     0     18\n#> 3   244     0     0    NA    NA    NA   115    75     1    56     1     15\n#> 4   245     0     1    85     2    14   148    78     0    68     1     15\n#> 5   252     0     0    NA    NA    NA   118    77     0    40     0     18\n#> # … with 1 more row, and 52 more variables: marital <dbl>, school <dbl>,\n#> #   education <dbl>, ht <dbl>, wt71 <dbl>, wt82 <dbl>, wt82_71 <dbl>,\n#> #   birthplace <dbl>, smokeintensity <dbl>, smkintensity82_71 <dbl>,\n#> #   smokeyrs <dbl>, asthma <dbl>, bronch <dbl>, tb <dbl>, hf <dbl>, hbp <dbl>,\n#> #   pepticulcer <dbl>, colitis <dbl>, hepatitis <dbl>, chroniccough <dbl>,\n#> #   hayfever <dbl>, diabetes <dbl>, polio <dbl>, tumor <dbl>,\n#> #   nervousbreak <dbl>, alcoholpy <dbl>, alcoholfreq <dbl>, alcoholtype <dbl>,\n#> #   alcoholhowmuch <dbl>, pica <dbl>, headache <dbl>, otherpain <dbl>,\n#> #   weakheart <dbl>, allergies <dbl>, nerves <dbl>, lackpep <dbl>,\n#> #   hbpmed <dbl>, boweltrouble <dbl>, wtloss <dbl>, infection <dbl>,\n#> #   active <dbl>, exercise <dbl>, birthcontrol <dbl>, pregnancies <dbl>,\n#> #   cholesterol <dbl>, hightax82 <dbl>, price71 <dbl>, price82 <dbl>,\n#> #   tax71 <dbl>, tax82 <dbl>, price71_82 <dbl>, tax71_82 <dbl>"},{"path":"tmle3.html","id":"tmle3","chapter":"4 The TMLE Framework","heading":"4 The TMLE Framework","text":"Jeremy CoyleBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learning-objectives-2","chapter":"4 The TMLE Framework","heading":"4.1 Learning Objectives","text":"Use tmle3 estimate Average Treatment Effect (ATE)Understand tmle3 “Specs”Fit tmle3 custom set parametersUse delta method estimate transformations parameters","code":""},{"path":"tmle3.html","id":"introduction-1","chapter":"4 The TMLE Framework","heading":"4.2 Introduction","text":"first step estimation procedure initial estimate \ndata-generating distribution, relevant part distribution \nneeded evaluate target parameter. initial estimation, use \nsuper learner (van der Laan, Polley, Hubbard 2007), described previous section.initial estimate relevant parts data-generating distribution\nnecessary evaluate target parameter, ready construct TMLE!","code":""},{"path":"tmle3.html","id":"substitution-estimators","chapter":"4 The TMLE Framework","heading":"4.2.1 Substitution Estimators","text":"Beyond fit prediction function, one might also want estimate \ntargeted parameters specific certain scientific questions.approach plug estimand interest estimates \nrelevant distributions.Sometimes, can use simple empirical distributions, averaging \nfunction observations (e.g., giving weight \\(1/n\\) \nobservations).parts distribution, like conditional means probabilities, \nestimate require sort smoothing due curse \ndimensionality.give one example using example average treatment effect (see\n):\\(\\Psi(P_0) = \\Psi(Q_0) = \\mathbb{E}_0 \\big[\\mathbb{E}_0[Y \\mid = 1, W] - \\mathbb{E}_0[Y \\mid = 0, W]\\big]\\), \\(Q_0\\) represents \ndistribution \\(Y \\mid ,W\\) distribution \\(W\\).Let \\(\\bar{Q}_0(,W) \\equiv \\mathbb{E}_0(Y \\mid ,W)\\) \\(Q_{0,W}(w) = P_0 (W=w)\\), \n\\[\n\\Psi(Q_0) = \\sum_w \\{ \\bar{Q}_0(1,w)-\\bar{Q}_0(0,w)\\} Q_{0,W}(w)\n\\]Substitution Estimator plugs empirical distribution (weight\n\\(1/n\\) observation) \\(Q_{0,W}(W_i)\\), estimate \nregression \\(Y\\) \\((,W)\\) (say SL fit):\n\\[\n \\Psi(Q_n) = \\frac{1}{n} \\sum_{=1}^n  \\{ \\bar{Q}_n(1,W_i)-\\bar{Q}_n(0,W_i)\\}\n \\]Thus, becomes average differences predictions fit\nkeeping observed \\(W\\), first replacing \\(=1\\) \n\\(=0\\).","code":""},{"path":"tmle3.html","id":"tmle","chapter":"4 The TMLE Framework","heading":"4.2.2 TMLE","text":"Though using SL arbitrary parametric regression improvement,\n’s sufficient properties estimator one needs \nrigorous inference.variance-bias trade-SL focused prediction\nmodel, can, instance, -fit portions distributions \ncritical estimating parameter interest, \\(\\Psi(P_0)\\).TMLE keeps benefits substitution estimators (one), augments\noriginal estimates correct issue also results \nasymptotically linear (thus normally-distributed) estimator \nconsistent Wald-style confidence intervals.Produces well-defined, unbiased, efficient substitution estimator target\nparameters data-generating distribution.Updates initial (super learner) estimate relevant part \ndata-generating distribution possibly using estimate nuisance\nparameter (like model intervention given covariates).Removes asymptotic residual bias initial estimator target\nparameter, uses consistent estimator \\(g_0\\).initial estimator consistent target parameter, additional\nfitting data targeting step may remove finite sample bias, \npreserves consistency property initial estimator.initial estimator estimator \\(g_0\\) consistent, \nalso asymptotically efficient according semi-parametric statistical\nmodel efficiency theory.Thus, every effort made achieve minimal bias asymptotic\nsemi-parametric efficiency bound variance.different types TMLE, sometimes set parameters,\nexample algorithm estimating ATE.case, one can present estimator :\\[\n \\Psi(Q^{\\star}_n) = \\frac{1}{n} \\sum_{=1}^n \\{ \\bar{Q}^{\\star}_n(1,W_i) -\n \\bar{Q}^{\\star}_n(0,W_i)\\}\n \\]\n\\(\\bar{Q}^{\\star}_n(,W)\\) TMLE augmented estimate.\n\\(f(\\bar{Q}^{\\star}_n(,W)) = f(\\bar{Q}_n(,W)) + \\epsilon_n \\cdot h_n(,W)\\),\n\\(f(\\cdot)\\) appropriate link function (e.g., logit), \\(\\epsilon_n\\)\nestimated coefficient \\(h_n(,W)\\) “clever covariate”.case, \\(h_n(,W) = \\frac{}{g_n(W)}-\\frac{1-}{1-g_n(W)}\\), \\(g_n(W) = \\mathbb{P}(=1 \\mid W)\\) estimated (also SL) propensity score,\nestimator depends initial SL fit outcome regression\n(\\(\\bar{Q}_0\\)) SL fit propensity score (\\(g_n\\)).robust augmentations used tlverse, \nadded layer cross-validation avoid -fitting bias (CV-TMLE), \ncalled methods can robustly estimated several parameters\nsimultaneously (e.g., points survival curve).","code":""},{"path":"tmle3.html","id":"inference","chapter":"4 The TMLE Framework","heading":"4.2.3 Inference","text":"estimators discuss asymptotically linear, meaning \ndifference estimate \\(\\Psi(P_n)\\) true parameter (\\(\\Psi(P_0)\\))\ncan represented first order ..d. sum:\n\\[\\begin{equation}\\label{eqn:IC}\n  \\Psi(P_n) - \\Psi(P_0) = \\frac{1}{n} \\sum_{=1}^n IC(O_i; \\nu) +\n  o_p(1/\\sqrt{n})\n\\end{equation}\\]\n\\(IC(O_i; \\nu)\\) (influence curve function) function \ndata possibly nuisance parameters \\(\\nu\\). Importantly, \nestimators mean-zero Gaussian limiting distributions; thus, \nunivariate case, one \n\\[\\begin{equation}\\label{eqn:limit_dist}\n  \\sqrt{n}(\\Psi(P_n) - \\Psi(P_0)) \\xrightarrow[]{D}N(0,\\mathbb{V}\n  IC(O_i; \\nu)),\n\\end{equation}\\]\ninference estimator interest may obtained terms \ninfluence function. simple case, 95% confidence interval may\nderived :\n\\[\\begin{equation}\\label{eqn:CI}\n  \\Psi(P^{\\star}_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n  \\sqrt{\\frac{\\hat{\\sigma}^2}{n}},\n\\end{equation}\\]\n\\(SE=\\sqrt{\\frac{\\hat{\\sigma}^2}{n}}\\) \\(\\hat{\\sigma}^2\\) sample\nvariance estimated IC’s: \\(IC(O; \\hat{\\nu})\\). One can use functional\ndelta method derive influence curve parameter interest may \nwritten function asymptotically linear estimators.Thus, can derive robust inference parameters estimated \nfitting complex, machine learning algorithms methods \ncomputationally quick (rely re-sampling based methods like \nbootstrap).","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"4 The TMLE Framework","heading":"4.3 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating Average Treatment Effect (ATE).reminder, ATE identified following statistical parameter\n(assumptions): \\(ATE = \\mathbb{E}_0(Y(1)-Y(0)) = \\mathbb{E}_0 \\left(\\mathbb{E}_0[Y \\mid =1,W] - \\mathbb{E}_0[Y \\mid =0,W] \\right),\\)","code":""},{"path":"tmle3.html","id":"load-the-data","chapter":"4 The TMLE Framework","heading":"4.3.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":""},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"4 The TMLE Framework","heading":"4.3.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\) (outcome)\ndata structure. tmle3 needs know variables dataset correspond\nroles. use list character vectors tell . call\n“Node List” corresponds nodes Directed Acyclic Graph\n(DAG), way displaying causal relationships variables.","code":""},{"path":"tmle3.html","id":"handle-missingness","chapter":"4 The TMLE Framework","heading":"4.3.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median (continuous) mode (discrete)\nimputed, additional covariates indicating imputation generatedObservations missing either treatment outcome variables excluded.implemented IPCW-TMLE efficiently handle missingness outcome\nvariable, plan implement IPCW-TMLE handle missingness \ntreatment variable well.steps implemented process_missing function tmle3:","code":""},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"4 The TMLE Framework","heading":"4.3.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, end-users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set ofcomponents specification\n, minimal additional detail, can run end-user.’ll start using one specs, work way \ninternals tmle3.","code":""},{"path":"tmle3.html","id":"define-the-learners","chapter":"4 The TMLE Framework","heading":"4.3.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":""},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"4 The TMLE Framework","heading":"4.3.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"#> A tmle3_Fit that took 1 step(s)\n#>    type                                    param   init_est tmle_est      se\n#> 1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0035831 0.010337 0.05064\n#>        lower   upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.088915 0.10959        0.010337         -0.088915           0.10959"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"4 The TMLE Framework","heading":"4.3.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"#> [1] 0.010337"},{"path":"tmle3.html","id":"tmle3-components","chapter":"4 The TMLE Framework","heading":"4.4 tmle3 Components","text":"Now ’ve successfully used spec obtain TML estimate, let’s look\nhood components. spec number functions \ngenerate objects necessary define fit TMLE.","code":""},{"path":"tmle3.html","id":"tmle3_task","chapter":"4 The TMLE Framework","heading":"4.4.1 tmle3_task","text":"First , tmle3_Task, analogous sl3_Task, containing data ’re\nfitting TMLE , well NP-SEM generated node_list\ndefined , describing variables relationships.","code":"#> $W\n#> tmle3_Node: W\n#>  Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight\n#>  Parents: \n#> \n#> $A\n#> tmle3_Node: A\n#>  Variables: tr\n#>  Parents: W\n#> \n#> $Y\n#> tmle3_Node: Y\n#>  Variables: whz\n#>  Parents: A, W"},{"path":"tmle3.html","id":"initial-likelihood","chapter":"4 The TMLE Framework","heading":"4.4.2 Initial Likelihood","text":"Next, object representing likelihood, factorized according \nNPSEM described :components likelihood indicate factors estimated: \nmarginal distribution \\(W\\) estimated using NP-MLE, conditional\ndistributions \\(\\) \\(Y\\) estimated using sl3 fits (defined \nlearner_list) .can use tandem tmle_task object obtain likelihood\nestimates observation:","code":"#> W: Lf_emp\n#> A: LF_fit\n#> Y: LF_fit#>                W       A        Y\n#>    1: 0.00021299 0.35119 -0.35564\n#>    2: 0.00021299 0.36392 -0.92990\n#>    3: 0.00021299 0.34124 -0.80184\n#>    4: 0.00021299 0.34758 -0.91770\n#>    5: 0.00021299 0.34353 -0.61402\n#>   ---                            \n#> 4691: 0.00021299 0.23773 -0.57275\n#> 4692: 0.00021299 0.22197 -0.23114\n#> 4693: 0.00021299 0.22567 -0.80089\n#> 4694: 0.00021299 0.28334 -0.89036\n#> 4695: 0.00021299 0.19391 -1.06870"},{"path":"tmle3.html","id":"targeted-likelihood-updater","chapter":"4 The TMLE Framework","heading":"4.4.3 Targeted Likelihood (updater)","text":"also need define “Targeted Likelihood” object. special type\nlikelihood able updated using tmle3_Update object. \nobject defines update strategy (e.g. submodel, loss function, CV-TMLE \n, etc).constructing targeted likelihood, can specify different update\noptions. See documentation tmle3_Update details different\noptions. example, can disable CV-TMLE (default tmle3) \nfollows:","code":""},{"path":"tmle3.html","id":"parameter-mapping","chapter":"4 The TMLE Framework","heading":"4.4.4 Parameter Mapping","text":"Finally, need define parameters interest. , spec defines \nsingle parameter, ATE. next section, ’ll see add additional\nparameters.","code":"#> [[1]]\n#> Param_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]"},{"path":"tmle3.html","id":"putting-it-all-together","chapter":"4 The TMLE Framework","heading":"4.4.5 Putting it all together","text":"used spec manually generate components, can now\nmanually fit tmle3:result equivalent fitting using tmle3 function .","code":"#> A tmle3_Fit that took 1 step(s)\n#>    type                                    param   init_est tmle_est       se\n#> 1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0050134 0.013088 0.050723\n#>        lower  upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.086327 0.1125        0.013088         -0.086327            0.1125"},{"path":"tmle3.html","id":"fitting-tmle3-with-multiple-parameters","chapter":"4 The TMLE Framework","heading":"4.5 Fitting tmle3 with multiple parameters","text":", fit tmle3 just one parameter. tmle3 also supports fitting\nmultiple parameters simultaneously. illustrate , ’ll use \ntmle_TSM_all spec:spec generates Treatment Specific Mean (TSM) level \nexposure variable. Note must first generate new targeted likelihood,\nold one targeted ATE. However, can recycle initial\nlikelihood fit , saving us super learner step.","code":"#> [[1]]\n#> Param_TSM: E[Y_{A=Control}]\n#> \n#> [[2]]\n#> Param_TSM: E[Y_{A=Handwashing}]\n#> \n#> [[3]]\n#> Param_TSM: E[Y_{A=Nutrition}]\n#> \n#> [[4]]\n#> Param_TSM: E[Y_{A=Nutrition + WSH}]\n#> \n#> [[5]]\n#> Param_TSM: E[Y_{A=Sanitation}]\n#> \n#> [[6]]\n#> Param_TSM: E[Y_{A=WSH}]\n#> \n#> [[7]]\n#> Param_TSM: E[Y_{A=Water}]"},{"path":"tmle3.html","id":"delta-method","chapter":"4 The TMLE Framework","heading":"4.5.1 Delta Method","text":"can also define parameters based Delta Method Transformations \nparameters. instance, can estimate ATE using delta method two\nTSM parameters:can similarly used estimate derived parameters like Relative\nRisks, Population Attributable Risks","code":"#> Param_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]"},{"path":"tmle3.html","id":"fit","chapter":"4 The TMLE Framework","heading":"4.5.2 Fit","text":"can now fit TMLE simultaneously TSM parameters, well \ndefined ATE parameter","code":"#> A tmle3_Fit that took 1 step(s)\n#>    type                                       param   init_est tmle_est\n#> 1:  TSM                            E[Y_{A=Control}] -0.5961253 -0.62053\n#> 2:  TSM                        E[Y_{A=Handwashing}] -0.6168261 -0.65898\n#> 3:  TSM                          E[Y_{A=Nutrition}] -0.6109573 -0.60538\n#> 4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.6011387 -0.60735\n#> 5:  TSM                         E[Y_{A=Sanitation}] -0.5877457 -0.58053\n#> 6:  TSM                                E[Y_{A=WSH}] -0.5195720 -0.44788\n#> 7:  TSM                              E[Y_{A=Water}] -0.5652293 -0.53685\n#> 8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.0050134  0.01318\n#>          se     lower    upper psi_transformed lower_transformed\n#> 1: 0.029842 -0.679020 -0.56204        -0.62053         -0.679020\n#> 2: 0.041960 -0.741218 -0.57674        -0.65898         -0.741218\n#> 3: 0.042067 -0.687834 -0.52293        -0.60538         -0.687834\n#> 4: 0.041236 -0.688173 -0.52653        -0.60735         -0.688173\n#> 5: 0.042322 -0.663485 -0.49758        -0.58053         -0.663485\n#> 6: 0.045677 -0.537408 -0.35836        -0.44788         -0.537408\n#> 7: 0.039059 -0.613405 -0.46030        -0.53685         -0.613405\n#> 8: 0.050718 -0.086226  0.11259         0.01318         -0.086226\n#>    upper_transformed\n#> 1:          -0.56204\n#> 2:          -0.57674\n#> 3:          -0.52293\n#> 4:          -0.52653\n#> 5:          -0.49758\n#> 6:          -0.35836\n#> 7:          -0.46030\n#> 8:           0.11259"},{"path":"tmle3.html","id":"exercises","chapter":"4 The TMLE Framework","heading":"4.6 Exercises","text":"","code":""},{"path":"tmle3.html","id":"tmle3-ex1","chapter":"4 The TMLE Framework","heading":"4.6.1 Estimation of the ATE with tmle3","text":"Follow steps estimate average treatment effect using data \nCollaborative Perinatal Project (CPP), available sl3 package. \nsimplify example, define binary intervention variable, parity01 –\nindicator one children current child \nbinary outcome, haz01 – indicator average height \nage.Define variable roles \\((W,,Y)\\) creating list nodes.\nInclude following baseline covariates \\(W\\): apgar1, apgar5,\ngagebrth, mage, meducyrs, sexn. \\(\\) \\(Y\\) specified\n.Define tmle3_Spec object ATE, tmle_ATE().Using base learning libraries defined , specify sl3 base\nlearners estimation \\(Q = E(Y \\mid ,Y)\\) \\(g=P(\\mid W)\\).Define metalearner like .Define metalearner like .Define one super learner estimating \\(Q\\) another estimating \\(g\\).\nUse metalearner \\(Q\\) \\(g\\) super learners.Create list two super learners defined Step 5 call \nobject learner_list. list names (defining super\nlearner estimating \\(g\\)) Y (defining super learner \nestimating \\(Q\\)).Fit tmle tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimating \\(g\\)\n\\(Q\\), defined Step 6. Note: Like , need \nmake data copy deal data.table weirdness\n(cpp2 <- data.table::copy(cpp)) use cpp2 data.Fit tmle tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimating \\(g\\)\n\\(Q\\), defined Step 6. Note: Like , need \nmake data copy deal data.table weirdness\n(cpp2 <- data.table::copy(cpp)) use cpp2 data.","code":""},{"path":"tmle3.html","id":"tmle3-ex2","chapter":"4 The TMLE Framework","heading":"4.6.2 Estimation of Strata-Specific ATEs with tmle3","text":"exercise, work random sample 5,000 patients \nparticipated International Stroke Trial (IST). data described \nChapter 3.2 tlverse handbook. included data \nsummarized description relevant exercise.outcome, \\(Y\\), indicates recurrent ischemic stroke within 14 days \nrandomization (DRSISC); treatment interest, \\(\\), randomized\naspirin vs. aspirin treatment allocation (RXASP ist); \nadjustment set, \\(W\\), consists simply variables measured baseline. \ndata, outcome occasionally missing, need create \nvariable indicating missingness (\\(\\Delta\\)) analyses \ntlverse, since missingness automatically detected NA present\noutcome. Covariates missing values (RATRIAL, RASP3 RHEP24)\nalready imputed. Additional covariates created\n(MISSING_RATRIAL_RASP3 MISSING_RHEP24), indicate whether \ncovariate imputed. missingness identical RATRIAL \nRASP3, one covariate indicating imputation two\ncovariates created.Estimate average effect randomized asprin treatment (RXASP = 1) \nrecurrent ischemic stroke. Even though missingness mechanism \\(Y\\),\n\\(\\Delta\\), need specified node list, still need\naccounted TMLE. words, estimation problem,\n\\(\\Delta\\) relevant factor likelihood addition \\(Q\\), \\(g\\).\nThus, defining list sl3 learners likelihood factor, \nsure include list learners estimation \\(\\Delta\\), say\nsl_Delta, specify something like\nlearner_list <- list(= sl_A, delta_Y = sl_Delta, Y = sl_Y).Recall RCT conducted internationally. Suposse concern\ndose asprin may varied across geographical regions, \naverage across geographical regions may warranted. Calculate \nstrata specific ATEs according geographical region (REGION).","code":""},{"path":"tmle3.html","id":"summary","chapter":"4 The TMLE Framework","heading":"4.7 Summary","text":"tmle3 general purpose framework generating TML estimates. \neasiest way use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. However, digging\nhood allows users specify wide range TMLEs. next\nsections, ’ll see framework can used estimate advanced\nparameters optimal treatments shift interventions.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"optimal-individualized-treatment-regimes","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5 Optimal Individualized Treatment Regimes","text":"Ivana MalenicaBased tmle3mopttx R package\nIvana Malenica, Jeremy Coyle, Mark van der Laan.Updated: 2021-02-07","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"learning-objectives-3","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.1 Learning Objectives","text":"Differentiate dynamic optimal dynamic treatment regimes static\ninterventions.Understand benefits challenges associated using\noptimal individualized treatment regimes practice.Contrast impact implementing optimal individualized treatment\npopulation static dynamic regimes.Estimate causal effects optimal individualized treatment regimes \ntmle3mopttx R package.Contrast population impact implementing optimal individualized\ntreatment based sub-optimal rules.Construct realistic optimal individualized treatments respect real data\nsubject-matter knowledge limitations interventions.Understand implement variable importance analysis defined \nterms optimal individualized treatment interventions.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"introduction-to-optimal-individualized-interventions","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.2 Introduction to Optimal Individualized Interventions","text":"Identifying intervention effective patient based \nlifestyle, genetic environmental factors common goal precision\nmedicine. put context, Abacavir Tenofovir commonly prescribed\npart antiretroviral therapy Human Immunodeficiency Virus (HIV)\npatients. However, individuals benefit two medications equally.\nparticular, patients renal dysfunction might deteriorate \nprescribed Tenofovir, due high nephrotoxicity caused medication.\nTenofovir still highly effective treatment option HIV patients, \norder maximize patient’s well-, beneficial prescribe\nTenofovir individuals healthy kidney function. Along \nlines, one might seek improve retention HIV care. randomized clinical\ntrial, several interventions show efficacy- including appointment reminders\ntext messages, small cash incentives time clinic visits, peer\nhealth workers. Ideally, want improve effectiveness assigning \npatient intervention likely benefit , well \nimprove efficiency allocating resources individuals need\n, benefit .\nFIGURE 5.1: Illustration Dynamic Treatment Regime Clinical Setting\nOne opts administer intervention individuals profit ,\ninstead assigning treatment population level. know \nintervention works patient? aim motivates different type \nintervention, opposed static exposures might used . \nparticular, chapter learn dynamic individualized interventions\ntailor treatment decision based collected covariates. Formally,\ndynamic treatments represent interventions treatment-decision stage\nallowed respond currently available treatment covariate\nhistory.statistics community treatment strategy termed \nindividualized treatment regime (ITR), (counterfactual) population\nmean outcome ITR value ITR (“Application Probability Theory Agricultural Experiments. Essay Principles. Section 9.” 1990; Robins 1986; Pearl 2009b). Even , suppose one wishes maximize population mean \noutcome, individual access set measured\ncovariates. means, example, can learn individual\ncharacteristics assigning treatment increases probability beneficial\noutcome individual. ITR maximal value referred \noptimal ITR optimal individualized treatment. Consequently, value\noptimal ITR termed optimal value, mean optimal\nindividualized treatment.problem estimating optimal individualized treatment received much\nattention statistics literature years, especially \nadvancement precision medicine; see Murphy (2003), Robins (2004), Zhang et al. (2016),\nZhao et al. (2012), Chakraborty Moodie (2013) Robins Rotnitzky (2014) name . However, much \nearly work depends parametric assumptions. , even randomized\ntrial, statistical inference optimal individualized treatment relies\nassumptions generally believed false, can lead biased\nresults.chapter, consider estimation mean outcome optimal\nindividualized treatment candidate rules restricted depend \nuser-supplied subset baseline covariates. estimation problem \naddressed statistical model data distribution \nnonparametric, places restrictions probability patient\nreceiving treatment given covariates (randomized trial). , \ndon’t need make assumptions relationship outcome \ntreatment covariates, relationship treatment \ncovariates. , provide Targeted Maximum Likelihood Estimator \nmean optimal individualized treatment allows us generate valid\ninference parameter, without parametric assumptions. \ntechnical presentation algorithm, interested reader invited \nconsult van der Laan Luedtke (2015) Luedtke van der Laan (2016).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"data-structure-and-notation","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.3 Data Structure and Notation","text":"Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\). denote \\(\\) categorical treatment, \\(Y\\)\nfinal outcome. particular, define \\(\\\\mathcal{}\\) \n\\(\\mathcal{} \\equiv \\{a_1, \\cdots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories (possibly two, binary setup).\nNote treat \\(W\\) vector-valued, representing collected\nbaseline covariates. Therefore, single random individual \\(\\), \nobserved data \\(O_i\\): corresponding baseline covariates \\(W_i\\),\ntreatment \\(A_i\\), final outcome \\(Y_i\\). say \\(O \\sim P_0\\), \ndata drawn probability distribution \\(P_0\\). emphasize \nmake assumptions distribution \\(P_0\\), \\(P_0 \\\\mathcal{M}\\), \\(\\mathcal{M}\\) fully nonparametric model. \npreviously mentioned, means make assumptions relationship\n\\(Y\\) \\(\\) \\(W\\), might able say something \nrelationship \\(\\) \\(W\\), case randomized trial. can assume\nnonparametric structural equation model (NPSEM) describe generation \\(O\\),\ndescribed Pearl (2009a). Specifically, :\n\\[\\begin{align*}\\label{eqn:npsem}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y),\n\\end{align*}\\]\ncollection \\(f=(f_W,f_A,f_Y)\\) denotes unspecified partially\nspecified functions. particular, NPSEM parameterizes \\(P_0\\) terms \ndistribution random variables \\(O\\) \\(U\\), \\(U=(U_W,U_A,U_Y)\\) \nexogenous random variables. can define counterfactuals \\(Y_{d(W)}\\) defined \nmodified system equation \\(\\) replaced rule \\(d(W)\\),\ndependent covariates \\(W\\).likelihood data admits factorization, implied time ordering\n\\(O\\). denote density \\(O\\) \\(p_0\\), corresponding distribution\n\\(P_0\\) dominating measure \\(\\mu\\).\n\\[\\begin{equation*}\\label{eqn:likelihood_factorization}\n  p_0(O) = p_{Y,0}(Y|,W) p_{,0}(|W) p_{W,0}(W) = q_{Y,0}(Y|,W) q_{,0}(|W)\n    q_{W,0}(W),\n\\end{equation*}\\]\n\\(p_{Y,0}(Y|,W)\\) conditional density \\(Y\\) given \\((, W)\\) \nrespect dominating measure \\(\\mu_Y\\), \\(p_{,0}\\) conditional density\n\\(\\) given \\(W\\) respect dominating measure \\(\\mu_A\\), \\(p_{W,0}\\) \ndensity \\(W\\) respect dominating measure \\(\\mu_W\\). Consequently, \ndefine \\(P_{Y,0}(Y|,W)=Q_{Y,0}(Y|,W)\\), \\(P_{,0}(|W)=g_0(|W)\\) \n\\(P_{W,0}(W)=Q_{W,0}(W)\\) corresponding conditional distributions \\(Y\\),\n\\(\\) \\(W\\). notational simplicity, define \\(\\bar{Q}_{Y,0}(,W) \\equiv E_0[Y|,W]\\) conditional expectation \\(Y\\) given \\((,W)\\).addition, denote \\(V\\) \\(V \\W\\), defining subset baseline\ncovariates optimal individualized rule depends . Note \\(V\\) \n\\(W\\), empty set, depending subject matter knowledge. \nparticular, researcher might want consider known effect modifiers available\ntime treatment decision possible \\(V\\) covariates. Defining \\(V\\)\nallows us consider possibly sub-optimal rules easier estimate,\nthereby allows statistical inference counterfactual mean outcome\nsub-optimal rule.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"defining-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4 Defining the Causal Effect of an Optimal Individualized Intervention","text":"Consider dynamic treatment rules \\(V \\rightarrow d(V) \\\\{a_1, \\cdots, a_{n_A} \\} \\times \\{1\\}\\), assigning treatment \\(\\) based \\(V \\W\\). \nmentioned previous section, causal effects defined terms \nhypothetical interventions NPSEM (). modified system\ntakes following form:\n\\[\\begin{align*}\\label{eqn:npsem_causal}\n  W &= f_W(U_W) \\\\ &= d(V) \\\\ Y_{d(V)} &= f_Y(d(V), W, U_Y),\n\\end{align*}\\]dynamic treatment regime may viewed intervention \n\\(\\) set equal value based hypothetical regime \\(d(V)\\), \n\\(Y_{d(V)}\\) corresponding outcome \\(d(V)\\). denote distribution\ncounterfactual quantities \\(P_{0,d(V)}\\).goal causal analysis motivated dynamic, optimal\nindividualized intervention, estimate parameter defined \ncounterfactual mean outcome respect modified intervention\ndistribution (either dynamic optimal dynamic). primarily interested \nvalue individualized rule, \\(E_0[Y_{d(V)}]\\). optimal rule \nrule maximal value: \\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\n\\mathcal{D}} E_0[Y_{d(V)}]\\] \\(\\mathcal{D}\\) represents set possible\nrules, \\(d\\), implied \\(V\\). note , case problem hand requires\nminimizing mean outcome, optimal individualized rule \nrule minimal value instead. Finally, target parameter can \nexpressed \n\\[\\psi_0 := E_0[Y_{d_{opt}(V)}].\\]optimal individualized rule, well value rule, causal\nparameters based unobserved counterfactuals. order causal\nquantities estimated observed data, need identified\nstatistical parameters. step roadmap requires make \nassumptions:Consistency: \\(Y^{d(v_i)}_i = Y_i\\) event \\(A_i = d(v_i)\\),\n\\(= 1, \\ldots, n\\).Stable unit value treatment assumption (SUTVA): \\(Y^{d(v_i)}_i\\) \ndepend \\(d(v_j)\\) \\(= 1, \\ldots, n\\) \\(j \\neq \\), lack\ninterference.Strong ignorability: \\(\\perp \\!\\!\\! \\perp Y^{d(v)} \\mid W\\), \\(\\\\mathcal{}\\).Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(|W) > 0)=1\\)causal assumptions, can identify \\(P_{0,d}\\) observed data\nusing G-computation formula:\\[P_{0,d_{opt}}(O) = Q_{Y,0}(Y|=d_{opt}(V),W)g_0(=d_{opt}(V)|W)Q_{W,0}(W).\\]\nvalue individualized rule can now expressed \\[E_0[Y_{d(V)}] = E_{0,W}[\\bar{Q}_{Y,0}(=d(V),W)],\\], causal assumptions, can interpreted mean outcome \n(possibly contrary fact), treatment assigned according rule.\nFinally, statistical counterpart causal parameter interest \ndefined \n\\[\\psi_0 = E_{0,W}[\\bar{Q}_{Y,0}(=d_{opt}(V),W)].\\]Inference optimal value shown difficult exceptional\nlaws, defined probability distributions treatment neither\nbeneficial harmful. Inference similarly difficult finite samples \ntreatment effect small strata, even though valid asymptotic\nestimators exist setting. mind, address estimation\nproblem assumption non-exceptional laws effect.Many methods learning optimal rule data developed\n(Murphy 2003, @robins2004, @laber2012, @kosorok2012, @moodie2013). \nchapter, focus methods discussed Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015). Note however, tmle3mopttx also supports widely\nused Q-learning approach, optimal individualized rule based \ninitial estimate \\(\\bar{Q}_{Y,0}(,W)\\) (Sutton, Barto, others 1998).follow methodology outlined Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015), learn optimal ITR using Super Learner\n(van der Laan, Polley, Hubbard 2007), estimate value cross-validated Targeted Minimum\nLoss-based Estimation (CV-TMLE) (Zheng van der Laan 2010). great generality, first\nneed estimate true individual treatment regime, \\(d_0(V)\\), \ncorresponds dynamic treatment rule (\\(d(V)\\)) takes subset covariates\n\\(V \\W\\) assigns treatment individual based observed\ncovariates \\(v\\). estimate true optimal ITR hand, can\nestimate corresponding value.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"binary-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.1 Binary treatment","text":"estimate optimal individualized treatment regime? case \nbinary treatment, key quantity optimal ITR blip function. One can\nshow optimal ITR assigns treatment individuals falling strata \nstratum specific average treatment effect, blip function, \npositive assign treatment individuals quantity \nnegative. Therefore binary treatment, causal assumptions, define\nblip function :\n\\[\\bar{Q}_0(V) \\equiv E_0[Y_1-Y_0|V] \\equiv E_0[\\bar{Q}_{Y,0}(1,W) -\n\\bar{Q}_{Y,0}(0,W) | V],\\]\naverage treatment effect within stratum \\(V\\). note \noptimal individualized rule can now derived \\(d_{opt}(V) = (\\bar{Q}_{0}(V) > 0)\\).package tmle3mopttx relies using Super Learner estimate blip\nfunction, easily extends general categorical treatment. \nmind, loss function utilized learning optimal individualized rule\ncorresponds conditional mean type losses. however worth mentioning \nLuedtke van der Laan (2016) present three different approaches learning optimal\nrule. Namely, focus :Super Learning Blip Function,Super Learning Blip Function,Super Learning Weighted Classification Problem,Super Learning Weighted Classification Problem,Joint Super Learner Blip Weighted Classification Problem.Joint Super Learner Blip Weighted Classification Problem.refer interested reader Luedtke van der Laan (2016) reference \nadvantages approach.Relying Targeted Maximum Likelihood (TML) estimator Super Learner\nestimate blip function, follow steps order obtain\nvalue ITR:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\) using sl3. denote \nestimates \\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(|W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW)\ntransform outcome, define:\n\\[D_{\\bar{Q}_Y,g,}(O) \\equiv \\frac{(=)}{g(|W)} (Y-\\bar{Q}_Y(,W)) +\n  \\bar{Q}_Y(=,W)\\]Note randomization positivity assumptions \n\\(E[D_{\\bar{Q}_Y,g,}(O) | V] = E[Y_a |V]\\). emphasize double robust nature\n-IPW transform- consistency \\(E[Y_a |V]\\) depend correct\nestimation either \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\). , \nrandomized trial, guaranteed consistent estimate \\(E[Y_a |V]\\) even \nget \\(\\bar{Q}_{Y,0}(,W)\\) wrong!Using transform, can define following contrast:\n\\(D_{\\bar{Q}_Y,g}(O) = D_{\\bar{Q}_Y,g,=1}(O) - D_{\\bar{Q}_Y,g,=0}(O)\\)estimate blip function, \\(\\bar{Q}_{0,}(V)\\), regressing\n\\(D_{\\bar{Q}_Y,g}(O)\\) \\(V\\) using specified sl3 library learners \nappropriate loss function.estimated rule \\(d(V) = \\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).obtain inference mean outcome estimated optimal rule\nusing CV-TMLE.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"categorical-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.2 Categorical treatment","text":"line approach considered binary treatment, extend blip\nfunction allow categorical treatment. denote blip function\nextensions pseudo-blips, new estimation targets \ncategorical setting. define pseudo-blips vector-valued entities \noutput given \\(V\\) vector length equal number treatment\ncategories, \\(n_A\\). , define :\n\\[\\bar{Q}_0^{pblip}(V) = \\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}\\]implement three different pseudo-blips tmle3mopttx.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case binary\ntreatment, strategy reduces approach described binary\nsetup.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case binary\ntreatment, strategy reduces approach described binary\nsetup.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a|V)\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a|V)\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=|V) |V)\\]Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=|V) |V)\\]Just like binary case, pseudo-blips estimated regressing contrasts\ncomposed using -IPW transform \\(V\\).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"note-on-inference","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.3 Note on Inference","text":"randomized trial, statistical inference relies second-order\ndifference estimator optimal individualized treatment \noptimal individualized treatment asymptotically negligible. \nreasonable condition consider rules depend small number \ncovariates, willing make smoothness assumptions. Alternatively,\ncan consider TMLEs statistical inference data-adaptive target\nparameters defined terms estimate optimal individualized\ntreatment. particular, instead trying estimate mean true\noptimal individualized treatment, aim estimate mean \nestimated optimal individualized treatment. , develop cross-validated\nTMLE approach provides asymptotic inference minimal conditions \nmean estimate optimal individualized treatment. \nparticular, considering data adaptive parameter allows us avoid\nconsistency rate condition fitted optimal rule, required \nasymptotic linearity TMLE mean actual, true optimal\nrule. Practically, estimated (data-adaptive) rule preferred, \npossibly sub-optimal rule one implemented population.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"why-cv-tmle","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.4 Why CV-TMLE?","text":"discussed van der Laan Luedtke (2015), CV-TMLE necessary \nnon-cross-validated TMLE biased upward mean outcome rule,\ntherefore overly optimistic. generally however, using CV-TMLE allows us\nfreedom estimation therefore greater data adaptivity, without\nsacrificing inference.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"interpreting-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5 Interpreting the Causal Effect of an Optimal Individualized Intervention","text":"summary, mean outcome optimal individualized treatment \ncounterfactual quantity interest representing mean outcome \neverybody, contrary fact, received treatment optimized\noutcome. optimal individualized treatment regime rule \noptimizes mean outcome dynamic treatment, candidate\nrules restricted respond user-supplied subset baseline\nintermediate covariates. essence, target parameter answers key\naim precision medicine: allocating available treatment tailoring \nindividual characteristics patient, goal optimizing \nfinal outcome.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"evaluating-the-causal-effect-of-an-oit-with-binary-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6 Evaluating the Causal Effect of an OIT with Binary Treatment","text":"Finally, demonstrate evaluate mean outcome optimal\nindividualized treatment using tmle3mopptx. start, let’s load packages\n’ll use set seed:","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.1 Simulated Data","text":"First, load simulated data. start general setup\ntreatment binary variable; later chapter consider\nanother data-generating distribution \\(\\) categorical. example,\ndata generating distribution following form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{3 \\times 3})\\\\\n  P(=1|W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  P(Y=1|,W) &= 0.5\\text{logit}^{-1}[-5I(=1)(W_1-0.5)+5I(=0)(W_1-0.5)] +\n     0.5\\text{logit}^{-1}(W_2W_3)\n\\end{align*}\\]composes observed data structure \\(O = (W, , Y)\\). Note \nmean true optimal rule \\(\\psi=0.578\\) data generating\ndistribution.formally express fact using tlverse grammar introduced \ntmle3 package, create single data object specify functional\nrelationships nodes directed acyclic graph (DAG) via\nnonparametric structural equation models (NPSEMs), reflected node list\nset :now observed data structure (data) specification role\nvariable data set plays nodes DAG.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.2 Constructing Optimal Stacked Regressions with sl3","text":"easily incorporate ensemble machine learning estimation procedure,\nrely facilities provided sl3 R\npackage. Using framework provided sl3\npackage, nuisance parameters TML estimator\nmay fit ensemble learning, using cross-validation framework \nSuper Learner algorithm van der Laan, Polley, Hubbard (2007).seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression (Q), propensity score\n(g), blip function (B). make explicit respect \nstandard notation bundling ensemble learners list object :learner_list object specifies role ensemble\nlearners ’ve generated play computing initial estimators. Recall \nneed initial estimators relevant parts likelihood order \nbuilding TMLE parameter interest. particular, learner_list\nmakes explicit fact Y used fitting outcome regression,\nused fitting treatment mechanism regression, finally B\nused fitting blip function.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere. specify argument\nV = c(\"W1\", \"W2\", \"W3\") initializing tmle3_Spec object order \ncommunicate ’re interested learning rule dependent V\ncovariates. Note don’t specify V- result rule\nbased collected covariates. also need specify type\npseudo-blip use estimation problem, list learners used\nestimate blip function, whether want maximize minimize final\noutcome, advanced features including searching less\ncomplex rule realistic interventions.seen , tmle3_mopttx_blip_revere specification object\n(like tmle3_Spec objects) store data \nspecific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally.elaborate initialization specifications. initializing \nspecification TMLE parameter interest, specified \nset covariates rule depends (V), type pseudo-blip use\n(type), learners used estimating relevant parts \nlikelihood blip function. addition, need specify whether \nwant maximize mean outcome rule (maximize), whether \nwant estimate rule covariates \\(V\\) provided user\n(complex). FALSE, tmle3mopttx instead consider possible\nrules smaller set covariates including static rules, optimize\nmean outcome subsets \\(V\\). , user might \nprovided full set collected covariates input \\(V\\), possible\ntrue rule depends subset set provided user. \ncase, returned mean optimal individualized rule based\nsmaller subset. addition, provide option search realistic\noptimal individualized interventions via realistic specification. \nTRUE, treatments supported data considered, therefore\nalleviating concerns regarding practical positivity issues. explore \nimportant extensions tmle3mopttx later sections.can see estimate \\(psi_0\\) \\(0.56\\), confidence\ninterval covers true mean true optimal individualized treatment.","code":"#> [22:14:24] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.42223  0.56606 0.027015 0.51311 0.61901\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.56606           0.51311           0.61901"},{"path":"optimal-individualized-treatment-regimes.html","id":"evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7 Evaluating the Causal Effect of an optimal ITR with Categorical Treatment","text":"section, consider evaluate mean outcome optimal\nindividualized treatment \\(\\) two categories. \nprocedure analogous previously described binary treatment, now need\npay attention type blip define estimation stage, well\nconstruct learners.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.1 Simulated Data","text":"First, load simulated data. , data generating distribution \nfollowing form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{4 \\times 4})\\\\\n  P(=|W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  P(Y=1|,W) = 0.5\\text{logit}^{-1}[15I(=1)(W_1-0.5) - 3I(=2)(2W_1+0.5) +\n    3I(=3)(3W_1-0.5)] +\\text{logit}^{-1}(W_2W_1)\n\\end{align*}\\]can just load data available part package follows:composes observed data structure \\(O = (W, , Y)\\). Note \nmean true optimal rule \\(\\psi=0.658\\), quantity aim\nestimate.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.2 Constructing Optimal Stacked Regressions with sl3","text":"seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression, propensity score, \nblip function. Note need estimate \\(g_0(|W)\\) categorical\n\\(\\) – therefore, use multinomial Super Learner option available within\nsl3 package learners can address multi-class classification\nproblems. order see learners can used estimate \\(g_0(|W)\\) \nsl3, run following:Note since corresponding blip vector valued, \ncolumn additional level treatment. , need create\nmultivariate learners helper function create_mv_learners takes \nlist initialized learners input.make explicit respect standard notation bundling \nensemble learners list object :","code":"#>  [1] \"Lrnr_bartMachine\"          \"Lrnr_bound\"               \n#>  [3] \"Lrnr_caret\"                \"Lrnr_cv_selector\"         \n#>  [5] \"Lrnr_dbarts\"               \"Lrnr_gam\"                 \n#>  [7] \"Lrnr_glmnet\"               \"Lrnr_grf\"                 \n#>  [9] \"Lrnr_h2o_glm\"              \"Lrnr_h2o_grid\"            \n#> [11] \"Lrnr_independent_binomial\" \"Lrnr_mean\"                \n#> [13] \"Lrnr_multivariate\"         \"Lrnr_nnet\"                \n#> [15] \"Lrnr_optim\"                \"Lrnr_polspline\"           \n#> [17] \"Lrnr_pooled_hazards\"       \"Lrnr_randomForest\"        \n#> [19] \"Lrnr_ranger\"               \"Lrnr_rpart\"               \n#> [21] \"Lrnr_screener_correlation\" \"Lrnr_solnp\"               \n#> [23] \"Lrnr_svm\"                  \"Lrnr_xgboost\""},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"can see estimate \\(psi_0\\) \\(0.60\\), confidence\ninterval covers true mean true optimal individualized treatment.","code":"#> [22:14:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:14:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:04] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:04] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:04] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:05] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:07] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:07] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:11] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:11] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:11] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:15:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]    0.551  0.61063 0.065332 0.48258 0.73867\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.61063           0.48258           0.73867"},{"path":"optimal-individualized-treatment-regimes.html","id":"extensions-to-causal-effect-of-an-oit","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8 Extensions to Causal Effect of an OIT","text":"section, consider two extensions procedure described \nestimating value OIT. First one considers setting user\nmight interested grid possible sub-optimal rules, corresponding \npotentially limited knowledge potential effect modifiers. second\nextension concerns implementation realistic optimal individual\ninterventions certain regimes might preferred, due practical \nglobal positivity restraints realistic implement.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simpler-rules","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8.1 Simpler Rules","text":"order consider ambitious fully \\(V\\)-optimal rule, \ndefine \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\). \nallows us consider sub-optimal rules easier estimate \npotentially provide realistic rules- , allow statistical\ninference counterfactual mean outcome sub-optimal rule.\nWithin tmle3mopttx paradigm, just need change complex\nparameter FALSE:Therefore even though user specified baseline covariates basis\nrule estimation, simpler rule based \\(W_2\\) \\(W_1\\) sufficient\nmaximize mean optimal individualized treatment.","code":"#> [22:16:07] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:24] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:29] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:16:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> A tmle3_Fit that took 1 step(s)\n#>    type             param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=W3,W2,W1}]  0.54309  0.61946 0.070471 0.48134 0.75758\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.61946           0.48134           0.75758"},{"path":"optimal-individualized-treatment-regimes.html","id":"realistic-optimal-individual-regimes","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8.2 Realistic Optimal Individual Regimes","text":"addition considering less complex rules, tmle3mopttx also provides \noption estimate mean realistic, implementable, optimal\nindividualized treatment. often case assigning particular regime\nmight ability fully maximize (minimize) desired outcome, \ndue global practical positivity constrains, treatment can never \nimplemented real life (highly unlikely). , specifying\nrealistic TRUE, consider possibly suboptimal treatments optimize\noutcome question supported data.","code":"#> [22:19:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:36] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:36] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:38] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:38] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:39] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:43] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:45] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:46] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:19:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.54847  0.65455 0.021603 0.61221 0.69689\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.65455           0.61221           0.69689\n#> \n#>   2   3 \n#> 507 493"},{"path":"optimal-individualized-treatment-regimes.html","id":"q-learning","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8.3 Q-learning","text":"Alternatively, estimate mean optimal individualized\ntreatment using Q-learning. optimal rule can learned fitting \nlikelihood, consequently estimating optimal rule fit \nlikelihood (Sutton, Barto, others 1998, @murphy2003).outline use tmle3mopttx package order estimate mean\nITR using Q-learning. demonstrated previous sections, \nfirst need initialize specification TMLE parameter \ninterest. opposed previous section however, now use\ntmle3_mopttx_Q instead tmle3_mopttx_blip_revere order indicate \nwant use Q-learning instead TMLE.","code":"#> [22:20:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:20:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:03] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:04] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:04] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:11] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:14] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:18] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:18] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:18] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:18] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:18] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:21] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:21] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:21] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:21] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:23] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:23] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:23] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:24] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [1] 0.47964"},{"path":"optimal-individualized-treatment-regimes.html","id":"variable-importance-analysis-with-oit","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.9 Variable Importance Analysis with OIT","text":"Suppose one wishes assess importance observed covariate, \nterms maximizing (minimizing) population mean outcome \noptimal individualized treatment regime. particular, covariate \nmaximizes (minimizes) population mean outcome optimal\nindividualized treatment considered covariates optimal\nassignment might considered important outcome. put \ncontext, perhaps optimal allocation treatment 1, denoted \\(A_1\\), results \nlarger mean outcome optimal allocation another treatment (\\(A_2\\)).\nTherefore, label \\(A_1\\) higher variable importance \nregard maximizing (minimizing) mean outcome optimal\nindividualized treatment.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data-2","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.9.1 Simulated Data","text":"order run tmle3mopttx variable importance measure, need consider\ncovariates categorical variables. illustration purpose, bin\nbaseline covariates corresponding data-generating distribution described\nsection 5.7.1:Note node list now includes \\(W_1\\) treatments well! Don’t worry,\nstill properly adjust baseline covariates.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"variable-importance-using-targeted-estimation-of-the-value-of-the-itr","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.9.2 Variable Importance using Targeted Estimation of the value of the ITR","text":"previous sections seen obtain contrast mean\noptimal individualized rule mean observed outcome\nsingle covariate- now ready run variable importance analysis\nspecified covariates. order run variable importance\nanalysis, first need initialize specification TMLE \nparameter interest done . addition, need specify\ndata corresponding list nodes, well appropriate\nlearners outcome regression, propensity score, blip function.\nFinally, need specify whether adjust \ncovariates assessing variable importance . adjust \\(W\\)s\nanalysis, adjust_for_other_A=TRUE, also \\(\\) covariates\ntreated exposure variable importance loop.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle3_mopttx_vim. First, indicate method used learning optimal\nindividualized treatment specifying method argument \ntmle3_mopttx_vim. method=\"Q\", using Q-learning rule\nestimation, need specify V, type learners arguments\nspec, since important Q-learning. However, \nmethod=\"SL\", corresponds learning optimal individualized\ntreatment using outlined methodology, need specify type\npseudo-blip use estimation problem, whether want \nmaximize minimize outcome, complex realistic rules. Finally, \nmethod=\"SL\" also need communicate ’re interested learning \nrule dependent V covariates specifying V argument. \nmethod=\"Q\" method=\"SL\", need indicate whether want maximize\nminimize mean optimal individualized rule. Finally, also\nneed specify whether final comparison mean optimal\nindividualized rule mean observed outcome \nmultiplicative scale (risk ratio) linear (similar average treatment\neffect).final result tmle3_vim tmle3mopttx spec ordered list\nmean outcomes optimal individualized treatment categorical\ncovariates dataset.","code":"#> [22:21:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:34] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:35] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:36] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:37] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:39] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:39] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:39] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:42] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:43] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:45] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:46] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:21:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> [22:22:36] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:36] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:38] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:38] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:39] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:40] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:42] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:42] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:43] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:44] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:46] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:46] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:47] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:48] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:50] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:22:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:03] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:03] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> \n#> Error in xgboost::xgb.DMatrix(Xmat) : \n#>   xgb.DMatrix does not support construction from double\n#> Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#> Error in self$compute_step() : \n#>   Error in if (nrow(Xmat) > 0) { : argument is of length zero\n#>    type                  param    init_est  tmle_est       se     lower\n#> 1:   RR RR(E[Y_{A=NULL}]/E[Y])  0.00054055  0.090997 0.032992  0.026334\n#> 2:   RR RR(E[Y_{A=NULL}]/E[Y]) -0.02727623 -0.082020 0.049824 -0.179674\n#>       upper psi_transformed lower_transformed upper_transformed  A           W\n#> 1: 0.155660         1.09527           1.02668            1.1684  A W3,W4,W2,W1\n#> 2: 0.015633         0.92125           0.83554            1.0158 W1  W3,W4,W2,A\n#>     Z_stat      p_nz p_nz_corrected\n#> 1:  2.7582 0.0029063      0.0058125\n#> 2: -1.6462 0.0498606      0.0498606"},{"path":"optimal-individualized-treatment-regimes.html","id":"real-world-data-and-tmle3mopttx","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.10 Real World Data and tmle3mopttx","text":"Finally, cement everything learned far real data application.\nprevious sections, using WASH Benefits data,\ncorresponding Effect water quality, sanitation, hand washing, \nnutritional interventions child development rural Bangladesh trial.\nmain aim cluster-randomized controlled trial assess \nimpact six intervention groups, including:chlorinated drinking waterchlorinated drinking waterimproved sanitationimproved sanitationhandwashing soaphandwashing soapcombined water, sanitation, handwashingcombined water, sanitation, handwashingimproved nutrition counselling provision lipid-based nutrient\nsupplementsimproved nutrition counselling provision lipid-based nutrient\nsupplementscombined water, sanitation, handwashing, nutrition.combined water, sanitation, handwashing, nutrition.aim estimate optimal ITR corresponding value optimal\nITR main intervention WASH Benefits data.start, let’s load data, convert columns class numeric,\ntake quick look :, specify NPSEM via node_list object. outcome \ninterest weight--height Z-score seek maximize, whereas \ntreatment six intervention groups aimed improving living conditions.\ncollected baseline covariates correspond \\(W\\).pick potential effect modifiers, including mother’s education, current\nliving conditions (floor), possession material items including \nrefrigerator. concentrate covariates might indicative \nsocio-economic status individuals involved trial.","code":"#>      whz tr fracode month aged sex momage momedu momheight hfiacat Nlt18 Ncomp\n#> 1:  0.00  1       4     9  268   2     30      2    146.40       1     3    11\n#> 2: -1.16  1       4     9  286   2     25      2    148.75       3     2     4\n#> 3: -1.05  1      20     9  264   2     25      2    152.15       1     1    10\n#>    watmin elec floor walls roof asset_wardrobe asset_table asset_chair\n#> 1:      0    1     0     1    1              0           1           1\n#> 2:      0    1     0     1    1              0           1           0\n#> 3:      0    0     0     1    1              0           0           1\n#>    asset_khat asset_chouki asset_tv asset_refrig asset_bike asset_moto\n#> 1:          1            0        1            0          0          0\n#> 2:          1            1        0            0          0          0\n#> 3:          0            1        0            0          0          0\n#>    asset_sewmach asset_mobile\n#> 1:             0            1\n#> 2:             0            1\n#> 3:             0            1#> \n#>    1    2    3 \n#>  733 1441 2503\n#> \n#>    0    1 \n#> 4177  500\n#> \n#>    0    1 \n#> 4305  372\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  -4.670  -1.280  -0.600  -0.586   0.080   4.970#> [22:23:41] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:45] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:49] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:23:57] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:01] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:05] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> [22:24:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se    lower    upper\n#> 1:  TSM E[Y_{A=NULL}] -0.55674 -0.47855 0.045161 -0.56707 -0.39004\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:        -0.47855          -0.56707          -0.39004"},{"path":"optimal-individualized-treatment-regimes.html","id":"exercises-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.11 Exercises","text":"","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"review-of-key-concepts","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.11.1 Review of Key Concepts","text":"difference dynamic optimal individualized regimes?difference dynamic optimal individualized regimes?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?Look back results generated section 5.7.1, compare \nmean optimal individualized treatment section 5.6. \nthink estimate higher less complex rule? set \ncovariates picked tmle3mopttx compare baseline covariates \ntrue rule depends ?Look back results generated section 5.7.1, compare \nmean optimal individualized treatment section 5.6. \nthink estimate higher less complex rule? set \ncovariates picked tmle3mopttx compare baseline covariates \ntrue rule depends ?Compare distribution treatments assigned true optimal\nindividualized treatment (section 5.6) realistic optimal individualized\ntreatment (section 5.7.2). Referring back data-generating\ndistribution, think distribution allocated treatment\nchanged?Compare distribution treatments assigned true optimal\nindividualized treatment (section 5.6) realistic optimal individualized\ntreatment (section 5.7.2). Referring back data-generating\ndistribution, think distribution allocated treatment\nchanged?Using simulation, perform variable importance analysis using\nQ-learning. results change ?Using simulation, perform variable importance analysis using\nQ-learning. results change ?","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"the-ideas-in-action","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.11.2 The Ideas in Action","text":"Using WASH benefits data, extract optimal ITR exact individual.\nintervention dominant? think ?Using WASH benefits data, extract optimal ITR exact individual.\nintervention dominant? think ?Consider simpler rules WASH benefits data example. set rules\npicked?Consider simpler rules WASH benefits data example. set rules\npicked?Using WASH benefits data, estimate realistic optimal ITR \ncorresponding value realistic ITR. results change?Using WASH benefits data, estimate realistic optimal ITR \ncorresponding value realistic ITR. results change?Change treatment Mother’s education (momedu), estimate \nvalue ITR setting. results indicate? Can\nintervene variable?Change treatment Mother’s education (momedu), estimate \nvalue ITR setting. results indicate? Can\nintervene variable?","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"advanced-topics","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.11.3 Advanced Topics","text":"can extend current approach include exceptional laws?can extend current approach include exceptional laws?can extend current approach incorporate resource constraints?can extend current approach incorporate resource constraints?can extend current approach continuous interventions?can extend current approach continuous interventions?","code":""},{"path":"stochastic-treatment-regimes.html","id":"stochastic-treatment-regimes","chapter":"6 Stochastic Treatment Regimes","heading":"6 Stochastic Treatment Regimes","text":"Nima HejaziBased tmle3shift R package\nNima Hejazi, Jeremy Coyle, Mark van der Laan.Updated: 2021-02-07","code":""},{"path":"stochastic-treatment-regimes.html","id":"learning-objectives-4","chapter":"6 Stochastic Treatment Regimes","heading":"6.1 Learning Objectives","text":"Differentiate stochastic treatment regimes static, dynamic, optimal\ntreatment regimes.Describe estimating causal effects stochastic interventions informs \nreal-world data analysis.Contrast population level stochastic intervention policy modified\ntreatment policy.Estimate causal effects stochastic treatment regimes \ntmle3shift R package.Specify grid counterfactual shift interventions used defining\nset stochastic intervention policies.Interpret set effect estimates grid counterfactual shift\ninterventions.Construct marginal structural models measure variable importance terms\nstochastic interventions, using grid shift interventions.Implement shift intervention individual level, facilitate\nshifting individual value ’s supported data.Define novel shift intervention functions extend tmle3shift R\npackage.","code":""},{"path":"stochastic-treatment-regimes.html","id":"introduction-to-stochastic-interventions","chapter":"6 Stochastic Treatment Regimes","heading":"6.2 Introduction to Stochastic Interventions","text":"Stochastic treatment regimes present relatively simple, yet extremely flexible\nmanner realistic causal effects (contrasts thereof) may \ndefined. Importantly, stochastic treatment regimes may applied nearly\nmanner treatment variable – continuous, ordinal, categorical, binary –\nallowing rich set causal effects defined formalism.\nchapter, examine simple example stochastic treatment regimes \ncontext continuous treatment variable interest, defining \nintuitive causal effect examine stochastic interventions \ngenerally. later sections, introduce numerous extensions based \nbroad class interventions – stochastic interventions binary\ntreatment variables stochastic mediation effects data-adaptive inference\nstochastic intervention effects. first step using stochastic\ntreatment regimes practice, present tmle3shift R\npackage, features \nimplementation recently developed algorithm computing targeted minimum\nloss-based estimates causal effect based stochastic treatment regime\nshifts natural value treatment based shifting function\n\\(d(,W)\\). comprehensive technical presentation material \nchapter, interested reader invited consult Díaz van der Laan (2018).\nAdditional background field Targeted Learning, well prior work\nstochastic treatment regimes, available van der Laan Rose (2011),\nvan der Laan Rose (2018), Díaz van der Laan (2012).stochastic treatment regimes arguably general \nclasses interventions causal effects may defined, \ninterventions conceptually simple.","code":""},{"path":"stochastic-treatment-regimes.html","id":"data-structure-and-notation-1","chapter":"6 Stochastic Treatment Regimes","heading":"6.3 Data Structure and Notation","text":"Consider \\(n\\) observed units \\(O_1, \\ldots, O_n\\), random variable \\(O = (W, , Y)\\) corresponds single observational unit. Let \\(W\\) denote baseline\ncovariates (e.g., age, sex, education level), \\(\\) intervention variable \ninterest (e.g., nutritional supplements), \\(Y\\) outcome interest (e.g.,\ndisease status). Though need case, let \\(\\) continuous-valued,\n.e. \\(\\\\mathbb{R}\\). Let \\(O_i \\sim \\mathcal{P} \\\\mathcal{M}\\), \n\\(\\mathcal{M}\\) nonparametric statistical model defined set \ncontinuous densities \\(O\\) respect dominating measure. \nformalize definition stochastic interventions corresponding\ncausal effects, introduce nonparametric structural equation model (NPSEM),\nbased Pearl (2009a), define system changes posited\ninterventions:\n\\[\\begin{align*}\\label{eqn:npsem}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y),\n\\end{align*}\\]\nset structural equations provide mechanistic model \nobserved data \\(O\\) assumed generated. several standard\nassumptions embedded NPSEM – specifically, temporal ordering \nsupposes \\(Y\\) occurs \\(\\), occurs \\(W\\); variable\n(.e., \\(\\{W, , Y\\}\\)) assumed generated corresponding\ndeterministic function (.e., \\(\\{f_W, f_A, f_Y\\}\\)) observed variables\nprecede temporally, well exogenous variable, denoted \\(U\\);\nlastly, exogenous variable assumed contain unobserved causes \ncorresponding observed variable.likelihood data \\(O\\) admits factorization, wherein, \\(p_0^O\\),\ndensity \\(O\\) respect product measure, density evaluated\nparticular observation \\(o\\) may written\n\\[\\begin{equation*}\\label{eqn:likelihood_factorization}\n  p_0^O(x) = q^O_{0,Y}(y \\mid = , W = w) q^O_{0,}(\\mid W = w)\n  q^O_{0,W}(w),\n\\end{equation*}\\]\n\\(q_{0, Y}\\) conditional density \\(Y\\) given \\((, W)\\) respect\ndominating measure, \\(q_{0, }\\) conditional density \\(\\) given\n\\(W\\) respect dominating measure \\(\\mu\\), \\(q_{0, W}\\) density \n\\(W\\) respect dominating measure \\(\\nu\\). , ease notation,\nlet \\(Q(, W) = \\mathbb{E}[Y \\mid , W]\\), \\(g(\\mid W) = \\mathbb{P}(\\mid W)\\),\n\\(q_W\\) marginal distribution \\(W\\). components likelihood\nessential developing understanding manner \nstochastic treatment regimes pertrub system corresponding causal\neffect may evaluated. Importantly, NPSEM parameterizes \\(p_0^O\\) terms\ndistribution random variables \\((O, U)\\) modeled system \nequations. turn, implies model distribution counterfactual\nrandom variables generated interventions data-generating process.","code":""},{"path":"stochastic-treatment-regimes.html","id":"defining-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes","heading":"6.4 Defining the Causal Effect of a Stochastic Intervention","text":"causal effects defined terms hypothetical interventions \nNPSEM (), may consider stochastic interventions two\nequivalent ways: (1) equation \\(f_A\\), giving rise \\(\\), replaced\nprobabilistic mechanism \\(g_{\\delta}(\\mid W)\\) differs \noriginal \\(g(\\mid W)\\), (2) observed value \\(\\) replaced \nnew value \\(A_{d(,W)}\\) based applying user-defined function \\(d(,W)\\) \n\\(\\). former case, stochastically modified value treatment\n\\(A_{\\delta}\\) drawn user-specified distribution \\(g_\\delta(\\mid W)\\),\nmay depend original distribution \\(g(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically modified\nvalue treatment \\(A_{\\delta} \\sim g_{\\delta}(\\cdot \\mid W)\\).\nAlternatively, latter case, stochastic treatment regime may \nviewed intervention \\(\\) set equal value based \nhypothetical regime \\(d(, W)\\), regime \\(d\\) depends treatment level\n\\(\\) assigned absence regime well \ncovariates \\(W\\). either case, one may view stochastic intervention \ngenerating counterfactual random variable \\(Y_{d(,W)} := f_Y(d(,W), W, U_Y) \\equiv Y_{g_{\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), counterfactual\noutcome \\(Y_{d(,W)} \\sim \\mathcal{P}_0^{\\delta}\\).Stochastic interventions second variety may referred depending\nnatural value treatment modified treatment policies.\nHaneuse Rotnitzky (2013) Young, Hernán, Robins (2014) provide discussion \ncritical differences similarities identification interpretation\ntwo classes stochastic intervention. sequel, \nrestrict attention simple stochastic treatment regime \ncharacterized modified treatment policy (MTP). Letting \\(\\) denote \ncontinuous-valued treatment, taking nutritional supplements\n(e.g., number vitamin pills) assume distribution \\(\\)\nconditional \\(W = w\\) support interval \\((l(w), u(w))\\). , \nminimum observed number pills taken \\(\\) individual covariates\n\\(W = w\\) \\(l(w)\\); similarly, maximum \\(u(w)\\). , simple stochastic\nintervention, based shift \\(\\delta\\), may defined\n\\[\\begin{equation}\\label{eqn:shift}\n  d(, w) =\n  \\begin{cases}\n    - \\delta & \\text{} > l(w) + \\delta \\\\\n    & \\text{} \\leq l(w) + \\delta,\n  \\end{cases}\n\\end{equation}\\]\n\\(0 \\leq \\delta \\leq u(w)\\) arbitrary pre-specified value \ndefines degree observed value \\(\\) shifted, \npossible. stochastic treatment regime may interpreted result\nclinic policy encourages individuals consume \\(\\delta\\) vitamin\npills normally, .e., based baseline characteristics.\ninterpretation stochastic intervention may made interesting\nallowing modification \\(\\delta\\) engenders function \nbaseline covariates \\(W\\), thereby allowing number vitamin pills taken\nfunction covariates age, sex, comorbidities, etc. class\nstochastic interventions first introduced Díaz van der Laan (2012) \ndiscussed Haneuse Rotnitzky (2013), Díaz van der Laan (2018), \nHejazi et al. (2020). Note intervention may written manner\nconsistent first class stochastic treatment regimes discussed \nwell – , per Díaz van der Laan (2012), \\(\\mathbb{P}_{\\delta}(g_0)(= \\mid W) = g_0(- \\delta(W) \\mid W)\\).goal causal analysis motivated stochastic intervention \nestimate parameter defined counterfactual mean outcome \nrespect stochastically modified intervention distribution. \nparticular, target causal estimand analysis \\(\\psi_{0, \\delta} := \\mathbb{E}_{P_0^{\\delta}}\\{Y_{d(,W)}\\}\\), mean counterfactual\noutcome variable \\(Y_{d(, W)}\\). prior work, Díaz van der Laan (2012) showed \ncausal quantity interest \\(\\mathbb{E}_0 \\{Y_{d(, W)}\\}\\) identified \nfunctional distribution \\(O\\):\n\\[\\begin{align*}\\label{eqn:identification2012}\n  \\psi_{0,d} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\mathbb{E}_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\cdot \\\\ &q_{0, }^O(\\mid W = w) \\cdot\n   q_{0, W}^O(w) d\\mu()d\\nu(w).\n\\end{align*}\\]\nidentification conditions may assumed hold, statistical\nparameter  matches exactly counterfactual\noutcome \\(\\psi_{0, \\delta}\\) intervention, allowing causal\neffect learned observed data \\(O\\). Díaz van der Laan (2012) provide \nderivation based efficient influence function (EIF) nonparametric\nmodel \\(\\mathcal{M}\\) develop several estimators quantity, including\nsubstitution, inverse probability weighted (IPW), augmented inverse probability\nweighted (AIPW), targeted maximum likelihood (TML) estimators, allowing \nsemiparametric-efficient estimation inference quantity interest.\nper Díaz van der Laan (2018), statistical target parameter may also \ndenoted \\(\\Psi(P_0) = \\mathbb{E}_{P_0}{\\overline{Q}(d(, W), W)}\\), \n\\(\\overline{Q}(d(, W), W)\\) counterfactual outcome value given\nindividual stochastic intervention distribution.Although focus work neither establishment identification\nresults development theoretical details, review necessary\nidentification details counterfactual mean stochastic\nintervention , interest completeness. Paraphrasing \nDíaz van der Laan (2012) Díaz van der Laan (2018), four standard assumptions \nnecessary order establish identifiability causal parameter \nobserved data via statistical functional – areConsistency: \\(Y^{d(a_i, w_i)}_i = Y_i\\) event \\(A_i = d(a_i, w_i)\\),\n\\(= 1, \\ldots, n\\)Stable unit value treatment assumption (SUTVA): \\(Y^{d(a_i, w_i)}_i\\) \ndepend \\(d(a_j, w_j)\\) \\(= 1, \\ldots, n\\) \\(j \\neq \\), lack\ninterference (Rubin 1978, 1980).Strong ignorability: \\(A_i \\indep Y^{d(a_i, w_i)}_i \\mid W_i\\), \\(= 1, \\ldots, n\\).Positivity (overlap)_: \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes \nsupport \\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).identification assumptions satisfied, Díaz van der Laan (2012) \nDíaz van der Laan (2018) provide efficient influence function respect \nnonparametric model \\(\\mathcal{M}\\) \n\\[\\begin{equation*}\\label{eqn:eif}\n  D(P_0)(x) = H(, w)({y - \\overline{Q}(, w)}) +\n  \\overline{Q}(d(, w), w) - \\Psi(P_0),\n\\end{equation*}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation*}\\label{eqn:aux_covar_full}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_0(- \\delta \\mid w)}\n  {g_0(\\mid w)} + \\mathbb{}(+ \\delta \\geq u(w)),\n\\end{equation*}\\]\nmay reduced \n\\[\\begin{equation*}\\label{eqn:aux_covar_simple}\n  H(,w) = \\frac{g_0(- \\delta \\mid w)}{g_0(\\mid w)} + 1\n\\end{equation*}\\]\ncase treatment within limits arise conditioning\n\\(W\\), .e., \\(A_i \\(u(w) - \\delta, u(w))\\).efficient influence function allows construction \nsemiparametric-efficient estimators may constructed. sequel, focus\ntargeted maximum likelihood (TML) estimator, Díaz van der Laan (2018)\ngive recipe:Construct initial estimators \\(g_n\\) \\(g_0(, W)\\) \\(Q_n\\) \n\\(\\overline{Q}_0(, W)\\), perhaps using data-adaptive regression techniques.observation \\(\\), compute estimate \\(H_n(a_i, w_i)\\) \nauxiliary covariate \\(H(a_i,w_i)\\).Estimate parameter \\(\\epsilon\\) logistic regression model\n\\[ \\text{logit}\\overline{Q}_{\\epsilon, n}(, w) =\n\\text{logit}\\overline{Q}_n(, w) + \\epsilon H_n(, w),\\]\nalternative regression model incorporating weights.Compute TML estimator \\(\\Psi_n\\) target parameter, defining update\n\\(\\overline{Q}_n^{\\star}\\) initial estimate\n\\(\\overline{Q}_{n, \\epsilon_n}\\):\n\\[\\begin{equation*}\\label{eqn:tmle}\n  \\Psi_n = \\Psi(P_n^{\\star}) = \\frac{1}{n} \\sum_{= 1}^n\n  \\overline{Q}_n^{\\star}(d(A_i, W_i), W_i).\n\\end{equation*}\\]","code":""},{"path":"stochastic-treatment-regimes.html","id":"interpreting-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes","heading":"6.5 Interpreting the Causal Effect of a Stochastic Intervention","text":"\nFIGURE 5.1: Animation counterfactual outcome changes natural treatment distribution subjected simple stochastic intervention\n","code":""},{"path":"stochastic-treatment-regimes.html","id":"evaluating-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes","heading":"6.6 Evaluating the Causal Effect of a Stochastic Intervention","text":"start, let us load packages use set seed simulation:need estimate two components likelihood order construct \nTML estimator. first components outcome regression,\n\\(\\hat{Q}_n\\), simple regression form \\(\\mathbb{E}[Y \\mid ,W]\\).\nestimate quantity may constructed using Super Learner\nalgorithm. construct components sl3-style Super Learner \nregression , using small variety parametric nonparametric\nregression techniques:second estimate treatment mechanism, \\(\\hat{g}_n\\),\n.e., propensity score. case continuous intervention node\n\\(\\), quantity takes form \\(p(\\mid W)\\), conditional\ndensity. Generally speaking, conditional density estimation challenging\nproblem received much attention literature. estimate \ntreatment mechanism, must make use learning algorithms specifically suited\nconditional density estimation; list learners may extracted \nsl3 using sl3_list_learners():proceed, ’ll select two learners, Lrnr_haldensify using\nhighly adaptive lasso conditional density estimation, based \nalgorithm given Díaz van der Laan (2011) implemented Hejazi, Benkeser, van der Laan (2020), \nsemiparametric location-scale conditional density estimators implemented \nsl3 package. Super Learner may \nconstructed pooling estimates modified conditional density\nregression techniques.Finally, construct learner_list object use constructing TML\nestimator target parameter interest:learner_list object specifies role ensemble\nlearners generated play computing initial estimators \nused building TMLE parameter interest . particular, \nmakes explicit fact Q_learner used fitting outcome\nregression g_learner used estimating treatment mechanism.","code":"#> [1] \"Lrnr_density_discretize\"     \"Lrnr_density_hse\"           \n#> [3] \"Lrnr_density_semiparametric\" \"Lrnr_haldensify\"            \n#> [5] \"Lrnr_solnp_density\""},{"path":"stochastic-treatment-regimes.html","id":"example-with-simulated-data","chapter":"6 Stochastic Treatment Regimes","heading":"6.6.1 Example with Simulated Data","text":"composes observed data structure \\(O = (W, , Y)\\). formally\nexpress fact using tlverse grammar introduced tmle3 package,\ncreate single data object specify functional relationships \nnodes directed acyclic graph (DAG) via nonparametric structural\nequation models (NPSEMs), reflected node list set :now observed data structure (data) specification role\nvariable data set plays nodes DAG.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_val = 0.5 initializing \ntmle3_Spec object communicate ’re interested shift \\(0.5\\) \nscale treatment \\(\\) – , specify \\(\\delta = 0.5\\) (note \narbitrarily chosen value example).seen , tmle_shift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"#>    W1 W2         A Y\n#> 1:  1  1  0.271651 1\n#> 2:  0  0 -0.663368 1\n#> 3:  0  0  0.113366 0\n#> 4:  0  1 -0.732558 0\n#> 5:  1  1  0.388835 1\n#> 6:  0  0  0.043986 0"},{"path":"stochastic-treatment-regimes.html","id":"targeted-estimation-of-stochastic-interventions-effects","chapter":"6 Stochastic Treatment Regimes","heading":"6.6.2 Targeted Estimation of Stochastic Interventions Effects","text":"print method resultant tmle_fit object conveniently displays \nresults computing TML estimator.","code":"#> \n#> Iter: 1 fn: 534.2313  Pars:  0.43334 0.38683 0.17983\n#> Iter: 2 fn: 534.2312  Pars:  0.43334 0.38684 0.17982\n#> solnp--> Completed in 2 iterations\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.76199  0.76263 0.021966 0.71958 0.80568\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.76263           0.71958           0.80568"},{"path":"stochastic-treatment-regimes.html","id":"statistical-inference-for-targeted-maximum-likelihood-estimates","chapter":"6 Stochastic Treatment Regimes","heading":"6.6.3 Statistical Inference for Targeted Maximum Likelihood Estimates","text":"Recall asymptotic distribution TML estimators studied\nthoroughly:\n\\[\\psi_n - \\psi_0 = (P_n - P_0) \\cdot D(\\bar{Q}_n^{\\star}, g_n) +\nR(\\hat{P}^{\\star}, P_0),\\]\n, provided following two conditions,\\(D(\\bar{Q}_n^{\\star}, g_n)\\) converges \\(D(P_0)\\) \\(L_2(P_0)\\) norm, andthe size class functions considered estimation \n\\(\\bar{Q}_n^{\\star}\\) \\(g_n\\) bounded (technically, \\(\\exists \\mathcal{F}\\)\n\\(D(\\bar{Q}_n^{\\star}, g_n) \\\\mathcal{F}\\) whp, \n\\(\\mathcal{F}\\) Donsker class),\nreadily admits conclusion \n\\(\\psi_n - \\psi_0 = (P_n - P_0) \\cdot D(P_0) + R(\\hat{P}^{\\star}, P_0)\\).additional condition remainder term \\(R(\\hat{P}^{\\star}, P_0)\\)\ndecays \\(o_P \\left( \\frac{1}{\\sqrt{n}} \\right),\\) \n\\[\\psi_n - \\psi_0 = (P_n - P_0) \\cdot D(P_0) + o_P \\left( \\frac{1}{\\sqrt{n}}\n\\right),\\]\n, central limit theorem, establishes Gaussian limiting distribution\nestimator:\\[\\sqrt{n}(\\psi_n - \\psi) \\N(0, V(D(P_0))),\\]\n\\(V(D(P_0))\\) variance efficient influence curve (canonical\ngradient) \\(\\psi\\) admits asymptotically linear representation.implies \\(\\psi_n\\) \\(\\sqrt{n}\\)-consistent estimator \\(\\psi\\),\nasymptotically normal (given ), locally\nefficient. allows us build Wald-type confidence intervals \nstraightforward manner:\\[\\psi_n \\pm z_{\\alpha} \\cdot \\frac{\\sigma_n}{\\sqrt{n}},\\]\n\\(\\sigma_n^2\\) estimator \\(V(D(P_0))\\). estimator \\(\\sigma_n^2\\)\nmay obtained using bootstrap computed directly via following\\[\\sigma_n^2 = \\frac{1}{n} \\sum_{= 1}^{n} D^2(\\bar{Q}_n^*, g_n)(O_i)\\]now re-examined facts, let’s simply examine results \ncomputing TML estimator:","code":""},{"path":"stochastic-treatment-regimes.html","id":"extensions-variable-importance-analysis-with-stochastic-interventions","chapter":"6 Stochastic Treatment Regimes","heading":"6.7 Extensions: Variable Importance Analysis with Stochastic Interventions","text":"","code":""},{"path":"stochastic-treatment-regimes.html","id":"defining-a-grid-of-counterfactual-interventions","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.1 Defining a grid of counterfactual interventions","text":"order specify grid shifts \\(\\delta\\) used defining set \nstochastic intervention policies priori manner, let us consider \narbitrary scalar \\(\\delta\\) defines counterfactual outcome \\(\\psi_n = Q_n(d(, W), W)\\), , simplicity, let \\(d(, W) = + \\delta\\). \nsimplified expression auxiliary covariate TMLE \\(\\psi\\) \n\\(H_n = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)}\\), \\(g^{\\star}(\\mid w)\\)\ndefines treatment mechanism stochastic intervention implemented.\n, ascertain whether given choice shift \\(\\delta\\) admissable\n(sense avoiding violations positivity assumption), let \nbound \\(C(\\delta) = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)} < M\\), \n\\(g^{\\star}(\\mid w)\\) function \\(\\delta\\) part, \\(M\\) potentially\nuser-specified upper bound \\(C(\\delta)\\). , \\(C(\\delta)\\) measure \ninfluence given observation, thereby providing way limit \nmaximum influence given observation (way bound \\(M\\) placed \n\\(C(\\delta)\\)) choice shift \\(\\delta\\).formalize extend procedure determine acceptable set values\nshift \\(\\delta\\) sequel. Specifically, let shift \\(d(, W) = + \\delta(, W)\\), shift \\(\\delta(, W)\\) defined \n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & \\delta_{\\text{min}}(,w) \\leq \\delta \\leq\n        \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{max}}(,w), & \\delta \\geq \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{min}}(,w), & \\delta \\leq \\delta_{\\text{min}}(,w) \\\\\n    \\end{cases},\n\\end{equation}\\]\n\\[\\delta_{\\text{max}}(, w) = \\text{argmax}_{\\left\\{\\delta \\geq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}\\] \n\\[\\delta_{\\text{min}}(, w) = \\text{argmin}_{\\left\\{\\delta \\leq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}.\\]provides strategy implementing shift level given\nobservation \\((a_i, w_i)\\), thereby allowing observations shifted\nappropriate value – whether \\(\\delta_{\\text{min}}\\), \\(\\delta\\), \n\\(\\delta_{\\text{max}}\\).purpose using shift practice, present software\nprovides functions shift_additive_bounded \nshift_additive_bounded_inv, define variation shift:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n\\end{equation}\\]\ncorresponds intervention natural value treatment\ngiven observational unit shifted value \\(\\delta\\) case \nratio intervened density \\(g^{\\star}(\\mid w)\\) natural\ndensity \\(g(\\mid w)\\) (, \\(C(\\delta)\\)) exceed bound \\(M\\). \ncase ratio \\(C(\\delta)\\) exceeds bound \\(M\\), stochastic\nintervention policy apply given unit remain \nnatural value treatment \\(\\).","code":""},{"path":"stochastic-treatment-regimes.html","id":"initializing-vimshift-through-its-tmle3_spec","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.2 Initializing vimshift through its tmle3_Spec","text":"start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_grid = seq(-1, 1, = 1)\ninitializing tmle3_Spec object communicate ’re interested\nassessing mean counterfactual outcome grid shifts -1, 0, 1 scale treatment \\(\\) (note numerical\nchoice shift arbitrarily chosen set values example).seen , tmle_vimshift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":""},{"path":"stochastic-treatment-regimes.html","id":"targeted-estimation-of-stochastic-interventions-effects-1","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.3 Targeted Estimation of Stochastic Interventions Effects","text":"One may walk step--step procedure fitting TML estimator\nmean counterfactual outcome shift grid, using \nmachinery exposed tmle3 R package.One may invoke tmle3 wrapper function (user-facing convenience utility)\nfit series TML estimators (one parameter defined grid\ndelta) single function call:Remark: print method resultant tmle_fit object conveniently\ndisplays results computing TML estimator.","code":"#> \n#> Iter: 1 fn: 534.0196  Pars:  0.40783 0.35788 0.23429\n#> Iter: 2 fn: 534.0196  Pars:  0.40783 0.35787 0.23430\n#> solnp--> Completed in 2 iterations\n#> A tmle3_Fit that took 1 step(s)\n#>          type          param init_est tmle_est        se   lower   upper\n#> 1:        TSM  E[Y_{A=NULL}]  0.55351  0.56184 0.0226250 0.51749 0.60618\n#> 2:        TSM  E[Y_{A=NULL}]  0.69755  0.69748 0.0229975 0.65240 0.74255\n#> 3:        TSM  E[Y_{A=NULL}]  0.82085  0.80029 0.0183487 0.76433 0.83625\n#> 4: MSM_linear MSM(intercept)  0.69063  0.68653 0.0198658 0.64760 0.72547\n#> 5: MSM_linear     MSM(slope)  0.13367  0.11923 0.0091122 0.10137 0.13709\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.56184           0.51749           0.60618\n#> 2:         0.69748           0.65240           0.74255\n#> 3:         0.80029           0.76433           0.83625\n#> 4:         0.68653           0.64760           0.72547\n#> 5:         0.11923           0.10137           0.13709"},{"path":"stochastic-treatment-regimes.html","id":"inference-with-marginal-structural-models","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.4 Inference with Marginal Structural Models","text":"Since consider estimating mean counterfactual outcome \\(\\psi_n\\) \nseveral values intervention \\(\\delta\\), taken aforementioned\n\\(\\delta\\)-grid, one approach obtaining inference single summary measure\nestimated quantities involves leveraging working marginal structural\nmodels (MSMs). Summarizing estimates \\(\\psi_n\\) working MSM allows\ninference trend imposed \\(\\delta\\)-grid evaluated via \nsimple hypothesis test parameter working MSM. Letting\n\\(\\psi_{\\delta}(P_0)\\) mean outcome shift \\(\\delta\\) \ntreatment, \\(\\vec{\\psi}_{\\delta} = (\\psi_{\\delta}: \\delta)\\) \ncorresponding estimators \\(\\vec{\\psi}_{n, \\delta} = (\\psi_{n, \\delta}: \\delta)\\).\n, let \\(\\beta(\\vec{\\psi}_{\\delta}) = \\phi((\\psi_{\\delta}: \\delta))\\).given MSM \\(m_{\\beta}(\\delta)\\), \n\\[\\beta_0 = \\text{argmin}_{\\beta} \\sum_{\\delta}(\\psi_{\\delta}(P_0) -\nm_{\\beta}(\\delta))^2 h(\\delta),\\]\nsolution \n\\[u(\\beta, (\\psi_{\\delta}: \\delta)) = \\sum_{\\delta}h(\\delta)\n\\left(\\psi_{\\delta}(P_0) - m_{\\beta}(\\delta) \\right) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta) = 0.\\]\nleads following expansion\n\\[\\beta(\\vec{\\psi}_n) - \\beta(\\vec{\\psi}_0) \\approx -\\frac{d}{d\\beta} u(\\beta_0,\n\\vec{\\psi}_0)^{-1} \\frac{d}{d\\psi} u(\\beta_0, \\psi_0)(\\vec{\\psi}_n -\n\\vec{\\psi}_0),\\]\n\n\\[\\frac{d}{d\\beta} u(\\beta, \\psi) = -\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta)^t \\frac{d}{d\\beta} m_{\\beta}(\\delta)\n-\\sum_{\\delta} h(\\delta) m_{\\beta}(\\delta) \\frac{d^2}{d\\beta^2}\nm_{\\beta}(\\delta),\\]\n, case MSM linear model (since\n\\(\\frac{d^2}{d\\beta^2} m_{\\beta}(\\delta) = 0\\)), reduces simply \n\\[\\frac{d}{d\\beta} u(\\beta, \\psi) = -\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta)^t \\frac{d}{d\\beta} m_{\\beta}(\\delta),\\]\n\n\\[\\frac{d}{d\\psi}u(\\beta, \\psi)(\\psi_n - \\psi_0) = \\sum_{\\delta} h(\\delta)\n\\frac{d}{d\\beta} m_{\\beta}(\\delta) (\\psi_n - \\psi_0)(\\delta),\\]\nmay write terms efficient influence function (EIF) \\(\\psi\\)\nusing first order approximation \\((\\psi_n - \\psi_0)(\\delta) = \\frac{1}{n}\\sum_{= 1}^n \\text{EIF}_{\\psi_{\\delta}}(O_i)\\),\n\\(\\text{EIF}_{\\psi_{\\delta}}\\) efficient influence function (EIF) \n\\(\\vec{\\psi}\\).Now, say, \\(\\vec{\\psi} = (\\psi(\\delta): \\delta)\\) d-dimensional, may\nwrite efficient influence function MSM parameter \\(\\beta\\) follows\n\\[\\text{EIF}_{\\beta}(O) = \\left(\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)^t \\right)^{-1} \\cdot\n\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)\n\\text{EIF}_{\\psi_{\\delta}}(O),\\] first term dimension\n\\(d \\times d\\) second term dimension \\(d \\times 1\\). , \nassume linear working MSM; however, analogous procedure may applied \nworking MSMs based GLMs.Inference parameter MSM may obtained straightforward\napplication delta method (discussed previously) – , may\nwrite efficient influence function MSM parameter \\(\\beta\\) terms \nEIFs corresponding point estimates. Based , inference\nworking MSM rather straightforward. wit, limiting distribution\n\\(m_{\\beta}(\\delta)\\) may expressed \\[\\sqrt{n}(\\beta_n - \\beta_0) \\N(0,\n\\Sigma),\\] \\(\\Sigma\\) empirical covariance matrix \n\\(\\text{EIF}_{\\beta}(O)\\).","code":"#>          type          param init_est tmle_est        se   lower   upper\n#> 1: MSM_linear MSM(intercept)  0.69063  0.68653 0.0198658 0.64760 0.72547\n#> 2: MSM_linear     MSM(slope)  0.13367  0.11923 0.0091122 0.10137 0.13709\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.68653           0.64760           0.72547\n#> 2:         0.11923           0.10137           0.13709"},{"path":"stochastic-treatment-regimes.html","id":"directly-targeting-the-msm-parameter-beta","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.4.1 Directly Targeting the MSM Parameter \\(\\beta\\)","text":"Note , working MSM fit individual TML estimates \nmean counterfactual outcome given value shift \\(\\delta\\) \nsupplied grid. parameter interest \\(\\beta\\) MSM asymptotically\nlinear (, fact, TML estimator) consequence construction \nindividual TML estimators. smaller samples, may prudent perform \nTML estimation procedure targets parameter \\(\\beta\\) directly, opposed\nconstructing several independently targeted TML estimates. \napproach constructing estimator proposed sequel.Suppose simple working MSM \\(\\mathbb{E}Y_{g^0_{\\delta}} = \\beta_0 + \\beta_1 \\delta\\), TML estimator targeting \\(\\beta_0\\) \\(\\beta_1\\) may \nconstructed \n\\[\\overline{Q}_{n, \\epsilon}(,W) = \\overline{Q}_n(,W) + \\epsilon (H_1(g),\nH_2(g),\\] \\(\\delta\\), \\(H_1(g)\\) auxiliary covariate \n\\(\\beta_0\\) \\(H_2(g)\\) auxiliary covariate \\(\\beta_1\\).construct targeted maximum likelihood estimator directly targets \nparameters working marginal structural model, may use \ntmle_vimshift_msm Spec (instead tmle_vimshift_delta Spec \nappears ):","code":"#> \n#> Iter: 1 fn: 533.0247  Pars:  0.42823 0.44440 0.12736\n#> Iter: 2 fn: 533.0247  Pars:  0.42823 0.44440 0.12736\n#> solnp--> Completed in 2 iterations\n#> A tmle3_Fit that took 1 step(s)\n#>          type          param init_est tmle_est       se   lower   upper\n#> 1: MSM_linear MSM(intercept)  0.69052  0.69085 0.020071 0.65151 0.73019\n#> 2: MSM_linear     MSM(slope)  0.13300  0.13291 0.008957 0.11536 0.15047\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.69085           0.65151           0.73019\n#> 2:         0.13291           0.11536           0.15047"},{"path":"stochastic-treatment-regimes.html","id":"example-with-the-wash-benefits-data","chapter":"6 Stochastic Treatment Regimes","heading":"6.7.5 Example with the WASH Benefits Data","text":"complete walk , let’s turn using stochastic interventions \ninvestigate data WASH Benefits trial. start, let’s load \ndata, convert columns class numeric, take quick look itNext, specify NPSEM via node_list object. example analysis,\n’ll consider outcome weight--height Z-score (previous\nchapters), intervention interest mother’s age time \nchild’s birth, take covariates potential confounders.consider counterfactual weight--height Z-score shifts \nage mother child’s birth, interpret estimates \nparameter? simplify interpretation, consider shift just year \nmother’s age (.e., \\(\\delta = 1\\)); setting, stochastic\nintervention correspond policy advocating potential mothers\ndefer child single calendar year, possibly implemented \nencouragement design deployed clinical setting.example, ’ll use variable importance strategy considering \ngrid stochastic interventions evaluate weight--height Z-score \nshift mother’s age two years (\\(\\delta = -2\\)) two years\n(\\(\\delta = 2\\)). , simply initialize Spec tmle_vimshift_delta\njust previous example:Prior running analysis, ’ll modify learner_list object \ncreated density estimation procedure rely \nlocation-scale conditional density estimation procedure, nonparametric\nconditional density approach based highly adaptive lasso (Díaz van der Laan 2011; Benkeser van der Laan 2016; Coyle, Hejazi, van der Laan 2020; Hejazi, Coyle, van der Laan 2020; Hejazi, Benkeser, van der Laan 2020)\ncurrently unable accommodate larger datasets.made preparations, ’re now ready estimate \ncounterfactual mean weight--height Z-score small grid \nshifts mother’s age child’s birth. Just , \nsimple call tmle3 wrapper function:","code":"#>      whz tr fracode month aged sex momage momedu momheight hfiacat Nlt18 Ncomp\n#> 1:  0.00  1       4     9  268   2     30      2    146.40       1     3    11\n#> 2: -1.16  1       4     9  286   2     25      2    148.75       3     2     4\n#> 3: -1.05  1      20     9  264   2     25      2    152.15       1     1    10\n#>    watmin elec floor walls roof asset_wardrobe asset_table asset_chair\n#> 1:      0    1     0     1    1              0           1           1\n#> 2:      0    1     0     1    1              0           1           0\n#> 3:      0    0     0     1    1              0           0           1\n#>    asset_khat asset_chouki asset_tv asset_refrig asset_bike asset_moto\n#> 1:          1            0        1            0          0          0\n#> 2:          1            1        0            0          0          0\n#> 3:          0            1        0            0          0          0\n#>    asset_sewmach asset_mobile\n#> 1:             0            1\n#> 2:             0            1\n#> 3:             0            1"},{"path":"stochastic-treatment-regimes.html","id":"exercises-2","chapter":"6 Stochastic Treatment Regimes","heading":"6.8 Exercises","text":"","code":""},{"path":"stochastic-treatment-regimes.html","id":"the-ideas-in-action-1","chapter":"6 Stochastic Treatment Regimes","heading":"6.8.1 The Ideas in Action","text":"Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable chosen preceding question,\nsummarizing trend sequence shifts using marginal structural\nmodel.Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable chosen preceding question,\nsummarizing trend sequence shifts using marginal structural\nmodel.Repeat preceding analysis, using grid shifts, instead\ndirectly targeting parameters marginal structural model. Interpret\nresults – , slope marginal structural model\ntell us trend across chosen sequence shifts?Repeat preceding analysis, using grid shifts, instead\ndirectly targeting parameters marginal structural model. Interpret\nresults – , slope marginal structural model\ntell us trend across chosen sequence shifts?","code":""},{"path":"stochastic-treatment-regimes.html","id":"review-of-key-concepts-1","chapter":"6 Stochastic Treatment Regimes","heading":"6.8.2 Review of Key Concepts","text":"Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.marginal structural model used summarize trend along\nsequence shifts previously help contextualize estimated effect\nsingle shift? , access estimates across several\nshifts marginal structural model parameters allow us richly\ninterpret findings?marginal structural model used summarize trend along\nsequence shifts previously help contextualize estimated effect\nsingle shift? , access estimates across several\nshifts marginal structural model parameters allow us richly\ninterpret findings?advantages, , targeting directly parameters \nmarginal structural model?advantages, , targeting directly parameters \nmarginal structural model?","code":""},{"path":"r6.html","id":"r6","chapter":"7 A Primer on the R6 Class System","heading":"7 A Primer on the R6 Class System","text":"central goal Targeted Learning statistical paradigm estimate\nscientifically relevant parameters realistic (usually nonparametric) models.tlverse designed using basic OOP principles R6 OOP framework.\n’ve tried make easy use tlverse packages without worrying\nmuch OOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"r6.html","id":"classes-fields-and-methods","chapter":"7 A Primer on the R6 Class System","heading":"7.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R’s various OOP systems,\nsee http://adv-r..co.nz/OO-essentials.html, Hadley Wickham’s Advanced\nR (Wickham 2014).","code":""},{"path":"r6.html","id":"object-oriented-programming-python-and-r","chapter":"7 A Primer on the R6 Class System","heading":"7.2 Object Oriented Programming: Python and R","text":"OO concepts (classes inherentence) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Baker, Monya. 2016. “Reproducibility Crisis? Nature Survey Lifts Lid Researchers View Crisis Rocking Science Think Help.” Nature 533 (7604). Nature Publishing Group: 452–55.Benkeser, David, Mark J van der Laan. 2016. “Highly Adaptive Lasso Estimator.” 2016 IEEE International Conference Data Science Advanced Analytics (DSAA). IEEE. https://doi.org/10.1109/dsaa.2016.93.Buckheit, Jonathan B, David L Donoho. 1995. “Wavelab Reproducible Research.” Wavelets Statistics, 55–81. Springer.Chakraborty, Bibhas, Erica EM Moodie. 2013. Statistical Methods Dynamic Treatment Regimes: Reinforcement Learning, Causal Inference, Personalized Medicine (Statistics Biology Health). Springer.Coyle, Jeremy R, Nima S Hejazi, Mark J van der Laan. 2020. hal9001: Scalable Highly Adaptive Lasso. https://doi.org/10.5281/zenodo.3558313.Díaz, Iván, Mark J van der Laan. 2011. “Super Learner Based Conditional Density Estimation Application Marginal Structural Models.” International Journal Biostatistics 7 (1). De Gruyter: 1–20.———. 2012. “Population Intervention Causal Effects Based Stochastic Interventions.” Biometrics 68 (2). Wiley Online Library: 541–49.———. 2018. “Stochastic Treatment Regimes.” Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies, 167–80. Springer Science & Business Media.Editorial, Nature. 2015a. “Scientists Fool — Can Stop.” Nature 526 (7572). Springer Nature.———. 2015b. “Let’s Think Cognitive Bias.” Nature 526 (7572). Springer Nature.Haneuse, Sebastian, Andrea Rotnitzky. 2013. “Estimation Effect Interventions Modify Received Treatment.” Statistics Medicine 32 (30). Wiley Online Library: 5260–77.Hejazi, Nima S, David C Benkeser, Mark J van der Laan. 2020. haldensify: Highly Adaptive Lasso Conditional Density Estimation. https://github.com/nhejazi/haldensify. https://doi.org/10.5281/zenodo.3698329.Hejazi, Nima S, Jeremy R Coyle, Mark J van der Laan. 2020. “hal9001: Scalable Highly Adaptive Lasso Regression R.” Journal Open Source Software. Open Journal. https://doi.org/10.21105/joss.02526.Hejazi, Nima S, Mark J van der Laan, Holly E Janes, Peter B Gilbert, David C Benkeser. 2020. “Efficient Nonparametric Inference Effects Stochastic Interventions Two-Phase Sampling, Applications Vaccine Efficacy Trials.” Biometrics. Wiley Online Library. https://doi.org/10.1111/biom.13375.Luedtke, ., M. J van der Laan. 2016. “Super-Learning Optimal Dynamic Treatment Rule.” International Journal Biostatistics 12 (1): 305–32.Munafò, Marcus R, Brian Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, John PA Ioannidis. 2017. “Manifesto Reproducible Science.” Nature Human Behaviour 1 (1). Nature Publishing Group: 0021.Murphy, Susan . 2003. “Optimal Dynamic Treatment Regimes.” Journal Royal Statistical Society: Series B (Statistical Methodology) 65 (2). Wiley Online Library: 331–55.Nosek, Brian , Charles R Ebersole, Alexander C DeHaven, David T Mellor. 2018. “Preregistration Revolution.” Proceedings National Academy Sciences 115 (11). National Acad Sciences: 2600–2606.“Application Probability Theory Agricultural Experiments. Essay Principles. Section 9.” 1990. Statistical Science 5 (4). Institute Mathematical Statistics: 465–72. http://www.jstor.org/stable/2245382.Pearl, Judea. 2009a. Causality: Models, Reasoning, Inference. Cambridge University Press.———. 2009b. Causality: Models, Reasoning Inference. 2nd ed. New York, NY, USA: Cambridge University Press.Peng, Roger. 2015. “Reproducibility Crisis Science: Statistical Counterattack.” Significance 12 (3). Wiley Online Library: 30–32.Pullenayegum, Eleanor M, Robert W Platt, Melanie Barwick, Brian M Feldman, Martin Offringa, Lehana Thabane. 2016. “Knowledge Translation Biostatistics: Survey Current Practices, Preferences, Barriers Dissemination Uptake New Statistical Methods.” Statistics Medicine 35 (6). Wiley Online Library: 805–18.Robins, James. 1986. “New Approach Causal Inference Mortality Studies Sustained Exposure Period—Application Control Healthy Worker Survivor Effect.” Mathematical Modelling 7 (9): 1393–1512. https://doi.org/https://doi.org/10.1016/0270-0255(86)90088-6.Robins, James M. 2004. “Optimal Structural Nested Models Optimal Sequential Decisions.” Proceedings Second Seattle Symposium Biostatistics: Analysis Correlated Data, edited D. Y. Lin P. J. Heagerty, 189–326. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4419-9076-1_11.Robins, James, Andrea Rotnitzky. 2014. “Discussion ‘Dynamic Treatment Regimes: Technical Challenges Applications’.” Electron. J. Statist. 8 (1). Institute Mathematical Statistics; Bernoulli Society: 1273–89. https://doi.org/10.1214/14-EJS908.Rubin, Donald B. 1978. “Bayesian Inference Causal Effects: Role Randomization.” Annals Statistics. JSTOR, 34–58.———. 1980. “Randomization Analysis Experimental Data: Fisher Randomization Test Comment.” Journal American Statistical Association 75 (371). JSTOR: 591–93.Sandercock, P, R Collins, C Counsell, B Farrell, R Peto, J Slattery, C Warlow. 1997. “International Stroke Trial (Ist): Randomized Trial Aspirin, Subcutaneous Heparin, , Neither Among 19,435 Patients Acute Ischemic Stroke.” Lancet 349 (9065): 1569–81.Sandercock, Peter AG, Maciej Niewada, Anna Członkowska. 2011. “International Stroke Trial Database.” Trials 12 (1). BioMed Central: 101.Stark, Philip B, Andrea Saltelli. 2018. “Cargo-Cult Statistics Scientific Crisis.” Significance 15 (4). Wiley Online Library: 40–43.Stromberg, Arnold, others. 2004. “Write Statistical Software? Case Robust Statistical Methods.” Journal Statistical Software 10 (5): 1–8.Sutton, Richard S, Andrew G Barto, others. 1998. Introduction Reinforcement Learning. Vol. 135. MIT press Cambridge.Szucs, Denes, John Ioannidis. 2017. “Null Hypothesis Significance Testing Unsuitable Research: Reassessment.” Frontiers Human Neuroscience 11. Frontiers: 390.Textor, Johannes, Juliane Hardt, Sven Knüppel. 2011. “DAGitty: Graphical Tool Analyzing Causal Diagrams.” Epidemiology 22 (5). LWW: 745.Tofail, Fahmida, Lia CH Fernald, Kishor K Das, Mahbubur Rahman, Tahmeed Ahmed, Kaniz K Jannat, Leanne Unicomb, et al. 2018. “Effect Water Quality, Sanitation, Hand Washing, Nutritional Interventions Child Development Rural Bangladesh (Wash Benefits Bangladesh): Cluster-Randomised Controlled Trial.” Lancet Child & Adolescent Health 2 (4). Elsevier: 255–68.van der Laan, Mark J, Eric C Polley, Alan E Hubbard. 2007. “Super Learner.” Statistical Applications Genetics Molecular Biology 6 (1).van der Laan, Mark J, Sherri Rose. 2011. Targeted Learning: Causal Inference Observational Experimental Data. Springer Science & Business Media.———. 2018. Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies. Springer Science & Business Media.van der Laan, Mark J, Richard JCM Starmans. 2014. “Entering Era Data Science: Targeted Learning Integration Statistics Computational Data Analysis.” Advances Statistics 2014. Hindawi.van der Laan, M. J, . Luedtke. 2015. “Targeted Learning Mean Outcome Optimal Dynamic Treatment Rule.” Journal Causal Inference 3 (1): 61–95.Wickham, Hadley. 2014. Advanced R. Chapman; Hall/CRC.Young, Jessica G, Miguel Hernán, James M Robins. 2014. “Identification, Estimation Approximation Risk Interventions Depend Natural Value Treatment Using Observational Data.” Epidemiologic Methods 3 (1). De Gruyter: 1–19.Zhang, Baqun, Anastasios Tsiatis, Marie Davidian, Min Zhang, Eric Laber. 2016. “Estimating Optimal Treatment Regimes Classification Perspective.” Stat 5 (1): 278–78. https://doi.org/10.1002/sta4.124.Zhao, Yingqi, Donglin Zeng, John Rush, Michael R Kosorok. 2012. “Estimating Individualized Treatment Rules Using Outcome Weighted Learning.” Journal American Statistical Association 107 (499). Taylor & Francis: 1106–18. https://doi.org/10.1080/01621459.2012.695674.Zheng, W., M. J van der Laan. 2010. “Asymptotic Theory Cross-validated Targeted Maximum Likelihood Estimation.” U.C. Berkeley Division Biostatistics Working Paper Series.","code":""}]
