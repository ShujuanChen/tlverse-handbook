[{"path":"index.html","id":"about-this-book","chapter":"About this book","heading":"About this book","text":"Targeted Learning R: Causal Data Science tlverse Software\nEcosystem open source, reproducible electronic handbook applying \nTargeted Learning methodology practice using tlverse software\necosystem. work currently early draft\nphase available facilitate input community. view \ncontribute available content, consider visiting GitHub\nrepository.\n","code":""},{"path":"index.html","id":"outline","chapter":"About this book","heading":"0.1 Outline","text":"contents handbook meant serve reference guide \napplied research well materials can taught series short\ncourses focused applications Targeted Learning. section\nintroduces set distinct causal questions, motivated case study,\nalongside statistical methodology software assessing causal claim \ninterest. (evolving) set materials includesMotivation: need statistical\nrevolutionThe Roadmap introductory case study: WASH Beneifits dataIntroduction tlverse software\necosystemCross-validation origami\npackageEnsemble machine learning \nsl3 packageTargeted learning causal inference \ntmle3 packageOptimal treatments regimes \ntmle3mopttx packageStochastic treatment regimes \ntmle3shift packageCausal mediation analysis \ntmle3mediate packageCoda: need statistical\nrevolution","code":""},{"path":"index.html","id":"what-this-book-is-not","chapter":"About this book","heading":"What this book is not","text":"focus work providing -depth technical descriptions\ncurrent statistical methodology recent advancements. Instead, goal \nconvey key details state---art techniques manner \nclear complete, without burdening reader extraneous information.\nhope presentations herein serve references researchers\n– methodologists domain specialists alike – empower deploy\ncentral tools Targeted Learning efficient manner. technical\ndetails -depth descriptions classical theory recent advances\nfield Targeted Learning, interested reader invited consult\nvan der Laan Rose (2011) /van der Laan Rose (2018) appropriate. primary literature\nstatistical causal inference, machine learning, non/semiparametric theory\ninclude many recent advances Targeted Learning related areas.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"About this book","heading":"About the authors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"About this book","heading":"Mark van der Laan","text":"Mark van der Laan, PhD, Professor Biostatistics Statistics UC\nBerkeley. research interests include statistical methods computational\nbiology, survival analysis, censored data, adaptive designs, targeted maximum\nlikelihood estimation, causal inference, data-adaptive loss-based learning, \nmultiple testing. research group developed loss-based super learning \nsemiparametric models, based cross-validation, generic optimal tool \nestimation infinite-dimensional parameters, nonparametric density\nestimation prediction censored uncensored data. Building \nwork, research group developed targeted maximum likelihood estimation\ntarget parameter data-generating distribution arbitrary\nsemiparametric nonparametric models, generic optimal methodology \nstatistical causal inference. recently, Mark’s group focused \npart development centralized, principled set software tools \ntargeted learning, tlverse.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"About this book","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"About this book","heading":"Nima Hejazi","text":"Nima Hejazi NSF postdoctoral research fellow \nDivision Biostatistics Weill Cornell Medicine affiliate\npostdoctoral researcher Vaccine Infectious Disease Division \nFred Hutchinson Cancer Research Center. 2021, completed PhD \nbiostatistics supervision Mark van der Laan Alan Hubbard.\nBroadly, Nima’s research interests combine causal inference, machine learning,\ncomputational statistics, drawing ideas non/semi-parametric\nestimation develop robust statistical procedures efficiency guarantees.\nParticular methodological areas current emphasis include causal mediation\nanalysis, efficient inference two-phase sampling designs, sieve estimation\nmachine learning, targeted loss-based estimation. work usually\ninspired applications vaccine clinical trials, computational biology, \nepidemiology. also passionate statistical computing, open source\nsoftware design applied statistics, research software engineering. Nima\nmade significant contributions hal9001, sl3, tmle3, \norigami packages, primary author tmle3shift tmle3mediate\npackages, responsible package maintenance distribution\nacross tlverse ecosystem.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"About this book","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"About this book","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning causal inference, Rachael’s\nresearch focuses statistical estimation inference realistic\nstatistical models. current projects involve personalized online machine\nlearning EHR streaming data vital signs, automated learning \nhighly adaptive lasso, causal effect estimation community-level\ninterventions. also working FDA-funded project led Dr. Susan\nGruber, Targeted Learning Framework Causal Effect Estimation Using\nReal-World Data. Rachael active contributor hal9001 sl3\nR packages tlverse.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"About this book","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics, former head Division \nBiostatistics UC Berkeley, head data analytics core UC Berkeley’s\nSuperFund research program. current research interests include causal\ninference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"repro","chapter":"About this book","heading":"0.2 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.1.1 (2021-08-10),\npandoc version 2.7.3, \nfollowing packages:","code":""},{"path":"index.html","id":"learn","chapter":"About this book","heading":"0.3 Learning resources","text":"effectively utilize handbook, reader need fully trained\nstatistician begin understanding applying methods. However, \nhighly recommended reader understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.learning R programming language recommend following (free)\nintroductory resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGarret Grolemund Hadley Wickham’s R Data\nScienceFor general introduction causal inference, recommendMiguel . Hernán James M. Robins’ Causal Inference: ,\n2021Jason . Roy’s Crash Course Causality: Inferring Causal Effects \nObservational Data \nCoursera","code":""},{"path":"index.html","id":"setup","chapter":"About this book","heading":"0.4 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"About this book","heading":"0.4.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"About this book","heading":"0.4.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"About this book","heading":"0.4.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"About this book","heading":"0.4.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"robust.html","id":"robust","chapter":"1 Robust Statistics and Reproducible Science","heading":"1 Robust Statistics and Reproducible Science","text":"“One enemy robust science humanity – appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Nature Editorial (Anonymous) (2015b)Scientific research unique point history. need improve\nrigor reproducibility field greater ever; corroboration moves\nscience forward, yet growing alarm results reproduced \nvalidated, suggesting possibility many discoveries may false\n(Baker 2016). Consequences meeting need result \ndecline rate scientific progress, reputation sciences, \npublic’s trust scientific findings (Munafò et al. 2017; Nature Editorial (Anonymous) 2015a).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state, culture statistical data analysis\nenables, rather precludes, manner human bias may affect \nresults (ideally objective) data analytic efforts. significant degree \nhuman bias enters statistical analysis efforts form improper model\nselection. procedures estimation hypothesis testing derived\nbased choice statistical model; thus, obtaining valid estimates \nstatistical inference relies critically chosen statistical model\ncontaining accurate representation process generated data.\nConsider, example, hypothetical study treatment assigned \ngroup patients: treatment assigned randomly characteristics\nindividuals (.e., baseline covariates) used making treatment\ndecision? knowledge can incorporated statistical model.\nAlternatively, data observational study, \ncontrol treatment assignment mechanism. cases, available\nknowledge data-generating process (DGP) limited still. \ncase, statistical model contain possible\ndistributions data. practice, however, models selected based\nscientific knowledge available DGP; instead, models often\nselected based (1) philosophical leanings analyst, (2) \nrelative convenience implementation statistical methods admissible within\nchoice model, (3) results significance testing (.e.,\np-values) applied within choice model.practice “cargo-cult statistics — ritualistic miming statistics\nrather conscientious practice,” (Stark Saltelli 2018) characterized \narbitrary modeling choices, even though choices often result different\nanswers research question. , “increasingly often,\n[statistics] used instead aid abet weak science, role can perform\nwell used mechanically ritually,” opposed original purpose \nsafeguarding weak science providing formal techniques evaluating\nveracity claim using properly collected data (Stark Saltelli 2018). \npresents fundamental drive behind epidemic false findings \nscientific research suffering (van der Laan Starmans 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite\" education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.\"— Szucs Ioannidis (2017)team University California, Berkeley uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nspreading rapidly many students colleagues greatly\nenriched field, aptly named “Targeted Learning” methodology emphasizes \nfocus (.e., “targeting ”) scientific question hand, running counter\ncurrent culture problem “convenience statistics,” opens door\nbiased estimation, misleading analytic results, erroneous discoveries.\nTargeted Learning embraces fundamentals formalized field \nstatistics, notably including notions statistical model must\nrepresent real knowledge experiment generated data \ntarget parameter represents seeking learn data \nfeature distribution generated (van der Laan Starmans 2014). way,\nTargeted Learning defines truth establishes principled standard \nestimation, thereby curtailing --human biases (e.g., hindsight bias,\nconfirmation bias, outcome bias) infiltrating objective analytic\nefforts.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)handbook aims provide practical training students, researchers,\nindustry professionals, academicians sciences (whether biological,\nphysical, economic, social), public health, statistics, numerous \nfields, equip necessary knowledge skills utilize \nmethodological developments Targeted Learning — technique provides\ntailored pre-specified machines answering queries — taking advantage \nestimators efficient, minimally biased, provide formal\nstatistical inference — every data analysis incorporates\nstate---art statistical methodology, ensuring compatibility \nguiding principles computational reproducibility.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives, robust, well-tested software plays \ncritical role allowing practitioners direct access published results\ngiven scientific investigation. fact, “article…scientific\npublication scholarship , merely advertising \nscholarship. actual scholarship complete software development\nenvironment complete set instructions generated figures,”\nthus making availability adoption robust statistical software key \nenhancing transparency inherent (assumed) aspect \nscientific process (Buckheit Donoho 1995).statistical methodology readily accessible practice, \ncrucial accompanied user-friendly software\n(Pullenayegum et al. 2016; Stromberg others 2004). tlverse software\necosystem, composed set package R language environment \nstatistical computing (R Core Team 2021), developed fulfill need Targeted\nLearning methodological framework. suite software tools\nfacilitate computationally reproducible efficient analyses, also \ntool Targeted Learning education, since workflow mirrors central\naspects statistical methodology. particular, programming paradigm\ncentral tlverse ecosystem focus implementing specific\nestimator small set related estimators. Instead, focus \nexposing statistical framework Targeted Learning — software\npackages tlverse ecosystem directly model key objects defined \nmathematical theoretical framework Targeted Learning. ’s ,\ntlverse software packages share core set design principles centered\nextensibility, allowing used conjunction \neven used cohesively building blocks formulating sophisticated\nstatistical analyses. introduction Targeted Learning framework, \nrecommend recent review paper \nCoyle et al. (2021).handbook, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuition-building explanations, readers learn use toolbox \napplying Targeted Learning statistical methodology, translate \nreal-world causal inference analyses. preliminaries required prior \nlearning endeavor – made available list recommended learning\nresources.","code":""},{"path":"intro.html","id":"intro","chapter":"2 The Roadmap for Targeted Learning","heading":"2 The Roadmap for Targeted Learning","text":"Nima Hejazi Rachael PhillipsUpdated: 2021-12-22","code":""},{"path":"intro.html","id":"learning-objectives","chapter":"2 The Roadmap for Targeted Learning","heading":"Learning Objectives","text":"chapter, provide guidance toTranslate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following statistical causal assumptions \nimplications: ..d., consistency, unmeasured confounding, interference,\npositivity.","code":""},{"path":"intro.html","id":"introduction","chapter":"2 The Roadmap for Targeted Learning","heading":"Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.1 The Roadmap","text":"roadmap five-stage process defining following.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\M\\) \\(P_0 \\\\M\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(1) Data: A random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set confronted collection results \nexperiment, can view data random variable – , \nrepeat experiment, different realization data\ngenerated experiment question. particular, experiment \nrepeated many times, probability distribution generating data, \\(P_0\\),\nlearned. , observed data single unit, \\(O\\), may thought \ndrawn probability distribution \\(P_0\\). often, observe \\(n\\)\nindependent identically distributed (..d.) observations random\nvariable \\(O\\), observed data collection \\(O_1, \\ldots, O_n\\), \nsubscripts denote individual observational units. data\n..d., certainly common case applied data analysis;\nmoreover, number techniques handling non-..d. data, \nestablishing conditional independence, stratifying data create distinct sets\nidentically distributed data, inferential corrections repeated \nclustered observations, name .crucial domain scientist (.e., researcher) absolute clarity\nactually known data-generating distribution given\nproblem interest. Just critical scientific information \ncommunicated statistician, whose job use knowledge guide\nassumptions encoded choice statistical model. Unfortunately,\ncommunication statisticians researchers often fraught \nmisinterpretation. roadmap provides mechanism ensure clear\ncommunication researcher statistician – invaluable\ntool communication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"2 The Roadmap for Targeted Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations hand, can define empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure, \\(P_0\\), allowing us learn observed data.\nexample, can define empirical probability measure set \\(X\\) \nproportion observations belong \\(X\\). ,\n\\[\\begin{equation*}\n  P_n(X) = \\frac{1}{n}\\sum_{=1}^{n} \\(O_i \\X)\n\\end{equation*}\\]order start learning data, next need ask “know\nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"defining-the-statistical-model-m-such-that-p_0-in-m","chapter":"2 The Roadmap for Targeted Learning","heading":"(2) Defining the statistical model \\(\\M\\) such that \\(P_0 \\in \\M\\)","text":"statistical model \\(\\M\\) defined question asked end \nStep 1. set possible probability distributions \ndescribe observed data, appropriately constrained background\nscientific knowledge. Often \\(\\M\\) large (e.g., nonparametric),\nreflecting fact statistical knowledge \ndata-generating process limited.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model referred \nparametric. assumption made, example, proposition \nrandom variable interest, \\(O\\), normal distribution mean \\(\\mu\\)\nvariance \\(\\sigma^2\\). generally, parametric model may defined \\[\\begin{equation*}\n  \\M = \\{P_{\\theta} : \\theta \\\\R^d \\},\n\\end{equation*}\\]\ndescribes statistical model consisting distributions\n\\(P_{\\theta}\\), distributions indexed parameter \\(\\theta\\).assumption data-generating distribution specific, parametric\nform made quite commonly, even assumptions supported \nexisting knowledge. practice oversimplification current culture\ndata analysis typically complicates attempt trying answer \nscientific question hand, owing fact possible model\nmisspecification introduces bias unknown magnitude. philosophy used \njustify parametric assumptions captured quote George Box \n“models wrong useful,” encourages data analyst \nmake arbitrary modeling choices. result practice data science \noften yields starkly different answers scientific problem, due \ndiffering modeling decisions assumptions made different analysts.\nEven nascent days data analysis, recognized “far\nbetter [develop] approximate answer right question…exact\nanswer wrong question, can always made precise”\n(Tukey 1962), though traditional statistics failed heed advice \nnumber decades (Donoho 2017). Targeted Learning paradigm avoids\nbias defining statistical model representation true\ndata-generating distribution corresponding observed data. ultimate\ngoal formulate statistical estimation problem exactly, one\ncan set tailor problem best possible estimation\nprocedure.Now, Step 3: “trying learn data?”","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\M\\), parameter space (.e., real number) \\(\\R\\) –\n, target parameter mapping \\(\\Psi: \\M \\rightarrow \\R\\). \nestimand may seen representation quantity wish learn\ndata, answer well-specified (often causal) question \ninterest. contrast purely statistical estimands, causal estimands require\nidentification observed data, based causal models include\nseveral untestable assumptions, described greater detail section \ncausal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\\[\\begin{equation*}\n  \\Psi(P_0) = \\P_O(O > 5) = \\int_5^{\\infty} dP_O(o)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity wish learn data. defined \\(O\\), \\(\\M\\) \n\\(\\Psi(P_0)\\) formally defined statistical estimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Typically, focus estimation realistic, nonparametric models. \nobtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\M_{NP}\\) (\\(P_n \\\\M_{NP}\\)), parameter space parameter \ninterest. , \\(\\hat{\\Psi} : \\M_{NP} \\rightarrow \\R^d\\). estimator \nfunction takes input observed data, realization \\(P_n\\), \ngives output value parameter space, estimate,\n\\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical distribution\n\\(P_n\\), estimator random variable sampling distribution.\n, repeat experiment drawing \\(n\\) observations every time\nend different realization estimate estimator \nsampling distribution.primary goal construction estimators able derive \nasymptotic sampling distributions theoretical analysis given\nestimator. regard, important property estimators \nfocus asymptotic linearity, states difference \nestimator target estimand (.e., truth) can represented,\nasymptotically, average ..d. random variables:\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) - \\Psi(P_0) = \\frac{1}{n} \\sum_{=1}^n IC(O_i; \\nu) +\n    o_p(n^{-1/2}),\n\\end{equation*}\\]\n\\(\\nu\\) represents possible nuisance parameters influence curve\n(IC) depends. Based validity asymptotic approximation, one can\ninvoke central limit theorem (CLT) show\\[\\begin{equation*}\n  \\sqrt{n} \\left(\\hat{\\Psi}(P_n) - \\Psi(P_0)\\right) \\sim N(0, \\sigma^2_{IC}),\n\\end{equation*}\\]\n\\(\\sigma^2_{IC}\\) variance \\(IC(O_i; \\nu)\\). Given estimate \n\\(\\sigma^2_{IC}\\), possible construct classic, asymptotically\naccurate Wald-type confidence intervals (CIs) hypothesis tests. \nexample, standard \\((1 - \\alpha)\\) CI form\\[\\begin{equation*}\n  \\Psi(P_n) \\pm Z_{1 - \\frac{\\alpha}{2}} \\hat{\\sigma_{IC}} / \\sqrt{n},\n\\end{equation*}\\]\ncan constructed, \\(Z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\) quantile standard normal distribution.\nOften, interested constructing 95% confidence intervals,\ncorresponding mass \\(\\alpha = 0.05\\) either tail limit distribution;\nthus, typically take \\(Z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).","code":""},{"path":"intro.html","id":"roadmap-summary","chapter":"2 The Roadmap for Targeted Learning","heading":"2.2 Summary of the Roadmap","text":"Data collected across \\(n\\) ..d. units, \\(O_1, \\ldots, O_n\\), can viewed \ncollection random variables, \\(O\\), arising probability\ndistribution \\(\\P_0\\). collection data may expressed \\(O_1, \\ldots, O_n \\sim P_0\\), leverage statistical knowledge available \nexperiment generated data. support statement true data\ndistribution \\(P_0\\) falls statistical model, \\(\\M\\), \ncollection candidate probability distributions reflecting data-generating\nexperiment. Often sets – , statistical model \\(\\M\\) – must \nlarge, appropriately reflect fact statistical knowledge \nlimited. Hence, realistic statistical models often termed semi- \nnon-parametric, since large indexed finite-dimensional\nset parameters. Necessarily, statistical query must begin , “\ntrying learn data?”, question whose answer captured \nstatistical target parameter, \\(\\Psi\\), maps true data-generating\ndistribution \\(P_0\\) statistical estimand, \\(\\Psi(P_0)\\). point \nstatistical estimation problem formally defined, allowing use \nstatistical theory guide construction optimal estimators.","code":""},{"path":"intro.html","id":"causal","chapter":"2 The Roadmap for Targeted Learning","heading":"2.3 Causal Target Parameters","text":"many cases, interested problems ask questions regarding \ncausal effect intervention future outcome interest. causal\neffects may defined summaries population interest (e.g., \npopulation mean particular outcome) different conditions (e.g.,\ntreated versus untreated). example, causal effect defined \ndifference means disease outcome causal contrasts \npopulation experience low pollution levels (pollutant) \nmean population case high pollution levels \nexperienced. different ways operationalizing theoretical\nexperiments generate counterfactual data necessary describing \ncausal contrasts interest, including simply assuming counterfactual\noutcomes exist theory treatment contrasts interest\n(Neyman 1938; Rubin 2005; Imbens Rubin 2015) \nconsidering interventions directed acyclic graphs (DAGs) nonparametric\nstructural equation models (NPSEMs) (Pearl 1995, 2009),\nencode known hypothesized set relationships \nvariables system study.","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"2 The Roadmap for Targeted Learning","heading":"The Causal Model","text":"focus use DAGs NPSEMs description causal parameters.\nEstimators statistical parameters correspond, standard \nuntestable identifiability assumptions, causal parameters \nintroduced . DAGs particularly useful tool expressing \nknow causal relations among variables system study.\nIgnoring exogenous \\(U\\) terms (explained ), assume following ordering\nvariables observed data \\(O\\). demonstrate construction \nDAG using DAGitty (Textor, Hardt, Knüppel 2011):DAGs like provide convenient means visualize\ncausal relations variables, causal relations among variables\ncan equivalently represented NPSEM:\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(f\\)’s unspecified (non-parametric) functions generate \ncorresponding random variable function variable’s parents (.e.,\nnodes arrows variable) DAG unobserved, exogenous\nerror terms (.e., \\(U\\)’s). NPSEM may thought representation\nalgorithm produces data, \\(O\\), population interest.\nMuch statistics data science devoted discovering properties \nsystem equations (e.g., estimation prediction function \\(f_Y\\)).first hypothetical experiment consider assigning exposure \nentire population observing outcome, withholding exposure \npopulation observing outcome. corresponds comparison\noutcome distribution population two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new sets nonparametric structural equations\ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\n, case \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervention system set \\(\\) deterministically either \nvalues \\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome\nvariable population interest generated respective\nNPSEMs ; often called counterfactuals. difference \nmeans outcome two interventions defines parameter \noften called “average treatment effect” (ATE), denoted\\[\\begin{equation}\n  ATE = \\E_X(Y(1) - Y(0)),\n  \\tag{2.1}\n\\end{equation}\\]\n\\(\\E_X\\) mean theoretical (unobserved) full data \\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Y\n    A -> Y\n    W -> A -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"intro.html","id":"identifiability","chapter":"2 The Roadmap for Targeted Learning","heading":"Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate quantity Equation (2.1) directly. Thus, primary task\ncausal inference methods context identify assumptions\nnecessary express causal quantities interest functions \ndata-generating distribution. make assumptions \nquantity may estimated observed data \\(O \\sim P_0\\) \ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified NPSEM , can, handful untestable assumptions,\nestimate ATE observational data. assumptions may summarized \nfollows.unmeasured confounding: \\(\\perp Y() \\mid W\\) \\(\\\\mathcal{}\\), states potential outcomes \\((Y() : \\\\mathcal{})\\) arise independently exposure status \\(\\), conditional \nobserved covariates \\(W\\). analog randomization\nassumption data arising natural experiments, ensuring effect\n\\(\\) \\(Y\\) can disentangled \\(W\\) \\(Y\\), even though \\(W\\)\naffects .interference units: outcome unit \\(\\), \\(Y_i\\), \naffected exposure unit \\(j\\), \\(A_j\\), \\(\\neq j\\).\nConsistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”.Positivity overlap: observed units, across strata defined \\(W\\),\nmust bounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\P(= \\mid W) < 1\\) \\(\\) \\(W\\)).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\\[\\begin{equation}\n  ATE = \\E_0(Y(1) - Y(0)) = \\E_0\n    \\left(\\E_0[Y \\mid = 1, W] - \\E_0[Y \\mid = 0, W]\\right).\n  \\tag{2.2}\n\\end{equation}\\]\nwords, ATE difference predicted outcome values \nsubject, contrast treatment conditions (\\(= 0\\) versus \\(= 1\\)),\npopulation, averaged observations. Thus, parameter \ntheoretical “full” data distribution can represented estimand \nobserved data distribution. Significantly, nothing \nrepresentation Equation (2.2) requires parameteric\nassumptions; thus, regressions right hand side may estimated.\ndifferent parameters, potentially different identifiability\nassumptions resulting estimands can functions different components\n\\(P_0\\). discuss several complex estimands later sections.","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"3 Welcome to the tlverse","heading":"3 Welcome to the tlverse","text":"Updated: 2021-12-22","code":""},{"path":"tlverse.html","id":"learning-objectives-1","chapter":"3 Welcome to the tlverse","heading":"Learning Objectives","text":"chapter introduces tlverse software ecosystem, includingUnderstanding tlverse ecosystem conceptually.Identifying core components tlverse.Installing tlverse R packages.Understanding Targeted Learning roadmap.Learning WASH Benefits example data.","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"3 Welcome to the tlverse","heading":"What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"anatomy-of-the-tlverse","chapter":"3 Welcome to the tlverse","heading":"Anatomy of the tlverse","text":"Targeted Learning methods targeted maximum likelihood (minimum\nloss-based) estimators (TMLEs). construction Targeted Learning\nestimator proceeds two-stage process:Flexibly learning particular components data-generating distribution\nmacchine learning (e.g., Super Learning), resulting initial\nestimates nuisance parameters.Use parametric model-based update via maximum likelihood estimation\n(.e., MLE), incorporating initial estimates produced prior step.packages making core components tlverse software ecosystem,\nsl3 tmle3, address two goals, respectively. Together, \ngeneral functionality exposed allows one build specific TMLEs\ntailored exactly particular estimation problem.software packages make core tlverse aresl3: Modern Super Machine Learning\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.Beyond engines provide driving force behind tlverse, \nsupporting packages play important roles background:origami: Generalized Framework \nCross-Validation (Coyle Hejazi 2018)\n? generalized framework flexible cross-validation.\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction Targeted\nLearning estimators.\n? generalized framework flexible cross-validation.? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction Targeted\nLearning estimators.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (.e., futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.\n? framework delayed computations (.e., futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.key principle tlverse extensibility. , software\necosystem aims support development new Targeted Learning estimators \nreaching maturity. achieve degree flexibility, follow \nmodel implementing new classes estimators, distinct causal inference\nproblems, separate packages, use core machinery provided \nsl3 tmle3 packages currently three examples:tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule.\n? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome rule.? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Stochastic Shift\nInterventions tlverse\n? Stochastic shift interventions continuous-valued treatments.\n? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.\n? Stochastic shift interventions continuous-valued treatments.? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.tmle3mediate: Causal Mediation\nAnalysis tlverse\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.\n? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"3 Welcome to the tlverse","heading":"3.1 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use usethis package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Create PAT simply clicking “Generate token” bottom page.Create PAT simply clicking “Generate token” bottom page.Copy PAT, long string lowercase letters numbers.Copy PAT, long string lowercase letters numbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio.\n.Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8.\nType usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio..Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, press enter key ensure .Renviron\nends new line..Renviron file, press enter key ensure .Renviron\nends new line.Save .Renviron file. example shows syntax \nlook.Save .Renviron file. example shows syntax \nlook.Restart R. can restart R via drop-menu RStudio’s “Session”\ntab, located top RStudio interface. \nrestart R changes take effect!following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.\nGITHUB_PAT <- yourPAT"},{"path":"data.html","id":"data","chapter":"4 Meet the Data","heading":"4 Meet the Data","text":"","code":""},{"path":"data.html","id":"wash","chapter":"4 Meet the Data","heading":"4.1 WASH Benefits Example Dataset","text":"data come study effect water quality, sanitation, hand\nwashing, nutritional interventions child development rural Bangladesh\n(WASH Benefits Bangladesh): cluster randomized controlled trial\n(Tofail et al. 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block randomized,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;hand-washing soap;combined water, sanitation, hand washing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.handbook, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nNCT01590095.purposes handbook, start treating data independent\nidentically distributed (..d.) random draws large target\npopulation. , available options, account clustering \ndata (within sampled geographic units), , simplification, avoid \ndetails handbook, although modifications methodology biased\nsamples, repeated measures, related complications, available.28 variables measured, single variable set \noutcome interest. outcome, \\(Y\\), weight--height Z-score\n(whz dat); treatment interest, \\(\\), randomized treatment\ngroup (tr dat); adjustment set, \\(W\\), consists simply \neverything else. results observed data structure \\(n\\) ..d.\ncopies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables given just , complete\nsmall visualization describing marginal characteristics \ncovariate. Note asset variables reflect socio-economic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls); , course, design.","code":"\nlibrary(readr)\n# read in data via readr::read_csv\ndat <- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  )\n)"},{"path":"origami.html","id":"origami","chapter":"5 Cross-validation","heading":"5 Cross-validation","text":"Ivana MalenicaBased origami R package\nJeremy Coyle, Nima Hejazi, Ivana Malenica Rachael Phillips.Updated: 2021-12-22","code":""},{"path":"origami.html","id":"learning-objectives-2","chapter":"5 Cross-validation","heading":"Learning Objectives","text":"end chapter able :Differentiate training, validation test sets.Differentiate training, validation test sets.Understand concept loss function, risk cross-validation.Understand concept loss function, risk cross-validation.Select loss function appropriate functional parameter \nestimated.Select loss function appropriate functional parameter \nestimated.Understand contrast different cross-validation schemes ..d. data.Understand contrast different cross-validation schemes ..d. data.Understand contrast different cross-validation schemes time dependent\ndata.Understand contrast different cross-validation schemes time dependent\ndata.Setup proper fold structure, build custom fold-based function, \ncross-validate proposed function using origami R package.Setup proper fold structure, build custom fold-based function, \ncross-validate proposed function using origami R package.Setup proper cross-validation structure use Super Learner\nusing origami R package.Setup proper cross-validation structure use Super Learner\nusing origami R package.","code":""},{"path":"origami.html","id":"introduction-1","chapter":"5 Cross-validation","heading":"5.1 Introduction","text":"chapter, start elaborating estimation step outlined \nintroductory chapter, discussed Roadmap Targeted\nLearning. order generate initial estimate target\nparameter – focus following chapter Super\nLearning, first need translate, incorporate, knowledge\ndata generating process estimation procedure, decide \nevaluate estimation performance.performance, error, algorithm used estimation procedure\ndirectly relates generalizability independent data. proper\nassessment performance proposed algorithms extremely important; \nguides choice final learning method, gives us quantitative\nassessment good chosen algorithm . order assess \nperformance algorithm, introduce concept loss function,\nhelps us define risk, also referred expected\nprediction error.goal, specified next chapter, \nestimate true risk proposed statistical learning method. \ngoal(s) consist :Estimating performance different algorithms order choose \nbest one.chosen winner, estimate true risk proposed\nstatistical learning method.following, propose method using observed data \ncross-validation procedure using origami package (Coyle Hejazi 2018).","code":""},{"path":"origami.html","id":"background","chapter":"5 Cross-validation","heading":"5.2 Background","text":"Ideally, data-rich scenario, split dataset three parts:training set,validation set,test set.training set used fit algorithm(s) interest; evaluate \nperformance fit(s) validation set, can used estimate\nprediction error (e.g., tuning model selection). final error \nchosen algorithm(s) obtained using test set, kept separately,\ndoesn’t see data final evaluation. One might wonder, \ntraining data readily available, use training error evaluate \nproposed algorithm’s performance? Unfortunately, training error \ngood estimate true risk; consistently decreases model complexity,\nresulting possible overfit training data low generalizability.Since data often scarce, separating training, validation test\nset usually possible. absence large data set designated\ntest set, must resort methods estimate true risk efficient\nsample re-use. Re-sampling methods, great generality, involve repeatedly\nsampling training set fitting proposed algorithms new\nsamples. often computationally intensive, re-sampling methods \nparticularly useful model selection estimation true risk. \naddition, might provide insight variability robustness \nalgorithm fit fitting algorithm training data.","code":""},{"path":"origami.html","id":"introducing-cross-validation","chapter":"5 Cross-validation","heading":"5.2.1 Introducing: cross-validation","text":"chapter, focus cross-validation – essential tool \nevaluating given algorithm extends sample target\npopulation sample derived. seen widespread application\nfacets statistics, perhaps notably statistical machine learning.\ncross-validation procedure can used model selection, well \nestimation true risk associated statistical learning method \norder evaluate performance. particular, cross-validation directly\nestimates true risk estimate applied independent sample\njoint distribution predictors outcome. used model\nselection, cross-validation powerful optimality properties. asymptotic\noptimality results state cross-validated selector performs (terms \nrisk) asymptotically well optimal oracle selector based true,\nunknown data generating distribution. details theoretical\nresults, suggest van der Laan Dudoit (2003), van der Laan, Dudoit, Keles (2004), Dudoit van der Laan (2005) \nVan der Vaart, Dudoit, Laan (2006).great generality, cross-validation works partitioning sample \ncomplementary subsets, applying particular algorithm(s) subset (\ntraining set), evaluating method choice complementary subset\n(validation/test set). procedure repeated across multiple partitions\ndata. variety different partitioning schemes exist, depending \nproblem interest, data size, prevalence outcome, dependence\nstructure. origami package provides suite tools generalize \napplication cross-validation arbitrary data analytic procedures. \nfollowing, describe different types cross-validation schemes readily\navailable origami, introduce general structure origami\npackage, show use applied settings.","code":""},{"path":"origami.html","id":"estimation-roadmap-how-does-it-all-fit-together","chapter":"5 Cross-validation","heading":"5.3 Estimation Roadmap: how does it all fit together?","text":"Similarly defined Roadmap Targeted Learning, \ncan define Estimation Roadmap guide estimation process. \nparticular, developed unified loss-based cross-validation methodology\nestimator construction, selection, performance assessment series \narticles (e.g., see van der Laan Dudoit (2003), van der Laan, Dudoit, Keles (2004), Dudoit van der Laan (2005),\nVan der Vaart, Dudoit, Laan (2006), van der Laan, Polley, Hubbard (2007)) follow three main steps:loss funtion:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure.\nMap full data loss function observed data loss function, \nexpected value leading efficient estimator risk.loss funtion:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure.\nMap full data loss function observed data loss function, \nexpected value leading efficient estimator risk.algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.cross-validation scheme:\nApply appropriate cross-validation select optimal estimator among \ncandidates, assess overall performance resulting estimator.cross-validation scheme:\nApply appropriate cross-validation select optimal estimator among \ncandidates, assess overall performance resulting estimator.Step 1 Estimation Roadmap allows us unify broad range problems\ntraditionally treated separately statistical literature,\nincluding density estimation, prediction polychotomous continuous\noutcomes. example, interested estimating full joint\nconditional density, use negative log-likelihood loss. instead\ninterested conditional mean continuous outcome, one use\nsquared error loss; outcome binary, one resort \nindicator (0-1) loss. unified loss-based framework also reconciles censored\nfull data estimation methods, generalizing loss based learning \nfull data loss based learning general censored data.","code":""},{"path":"origami.html","id":"example-cross-validation-and-prediction","chapter":"5 Cross-validation","heading":"5.4 Example: cross-validation and prediction","text":"Now introduced Estimation Roadmap, can define objective \nmathematical notation, using prediction example. Let observed\ndata defined \\(X = (W,Y)\\), unit specific data can written \n\\(X_i = (W_i,Y_i)\\), \\(= 1, \\ldots, n\\). \\(n\\) samples, \ndenote \\(Y_i\\) outcome interest (polychotomous continuous), \\(W_i\\)\n\\(p\\)-dimensional set covariates. Let \\(\\psi_0(W)\\) denote target\nparameter interest want estimate; example, interested\nestimating conditional expectation outcome given covariates,\n\\(\\psi_0(W) = E(Y \\mid W)\\). Following Estimation Roadmap, chose \nappropriate loss function, \\(L\\), \\(\\psi_0(W) = \\text{argmin}_{\\psi} E[L(X,\\psi(W))]\\). know \\(\\psi\\) ? order pick\noptimal estimator among candidates, assess overall performance\nresulting estimator, use cross-validation – dividing available data\ntraining set validation set. Observations training set \nused fit (train) estimator, validation set used assess\nrisk (validate) .derive general representation cross-validation, define split\nvector, \\(B_n = (B_n(): = 1, \\ldots, n) \\\\{0,1\\}^n\\). Note split\nvector independent empirical distribution, \\(P_n\\). realization \n\\(B_n\\) defines random split data training validation set \n\\[B_n() = 0, \\ \\ \\text{sample training set}\\]\n\\[B_n() = 1, \\ \\ \\text{sample validation set.}\\]\ncan define \\(P_{n,B_n}^0\\) \\(P_{n,B_n}^1\\) empirical\ndistributions training validation sets, respectively. \\(n_0 = \\sum_i (1-B_n())\\) \\(n_1 = \\sum_i B_n()\\) denote number samples \nset. particular distribution split vector \\(B_n\\) defines type \ncross-validation scheme, tailored problem data set hand.","code":""},{"path":"origami.html","id":"cross-validation-schemes-in-origami","chapter":"5 Cross-validation","heading":"5.5 Cross-validation schemes in origami","text":"specified earlier, particular distribution split vector \\(B_n\\)\ndefines type cross-validation method. following, describe\ndifferent types cross-validation schemes available origami package, \nshow use sequel.","code":""},{"path":"origami.html","id":"wash-benefits-study-example","chapter":"5 Cross-validation","heading":"WASH Benefits Study Example","text":"order illustrate different cross-validation schemes, using \nWASH data. Detailed information WASH Benefits Example Dataset can \nfound Chapter 3. particular, interested predicting\nweight--height z-score whz using available covariate data. \nillustration, start treating data independent identically\ndistributed (..d.) random draws. see cross-validation scheme \n, subset data \\(n=30\\). Note row represents \n..d. sample, indexed row number.look first 30 data.","code":"\nlibrary(data.table)\nlibrary(origami)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"origami.html","id":"cross-validation-for-i.i.d.-data","chapter":"5 Cross-validation","heading":"5.5.1 Cross-validation for i.i.d. data","text":"","code":""},{"path":"origami.html","id":"re-substitution","chapter":"5 Cross-validation","heading":"5.5.1.1 Re-substitution","text":"re-substitution method simplest strategy estimating risk\nassociated fitting proposed algorithm set observations. , \nobserved data used training validation set.illustrate usage re-substitution method origami package\n; use function folds_resubstitution(n). order setup\nfolds_resubstitution(n), just need total number samples want \nallocate training validation sets; remember row data \nunique ..d. sample. Notice structure origami output:v: cross-validation foldtraining_set: indexes samples training setvalidation_set: indexes samples training set.structure origami output (aka, fold(s)) persist \ncross-validation schemes present chapter. , show fold\ngenerated re-substitution method:","code":"folds_resubstitution(nrow(washb_data))\n[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n$validation_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"holdout-method","chapter":"5 Cross-validation","heading":"5.5.1.2 Holdout method","text":"holdout method, validation set approach, consists randomly\ndividing available data training set validation set (holdout\nset). model fitted training set, evaluated \nobservations validation set. Typically, data split \n\\(60/40\\), \\(70/30\\), \\(80/20\\) \\(90/10\\) splits.holdout method intuitive, conceptually easy, computationally \ndemanding. However, repeat process randomly splitting data \ntraining validation set, might get different cross-validated\nemprical risk. particular, emprical mean loss validation\nsets might highly variable, depending samples included training/validation split. Overall, cross-validated emprical risk \nholdout method variabiable, since includes variability random\nsplit well - want. classification problems, \npossibility uneven distribution different classes training validation\nset unless data stratified. Finally, note using \ndata train evaluate performance proposed algorithm, might\nresult bias.","code":""},{"path":"origami.html","id":"leave-one-out","chapter":"5 Cross-validation","heading":"5.5.1.3 Leave-one-out","text":"leave-one-cross-validation scheme closely related holdout\nmethod. particular, also involves splitting data training \nvalidation set; however, instead partitioning observed data sets \nsimilar size, single observation used validation set. ,\nmajority units employed training (fitting) proposed\nalgorithm. Since one unit (example \\(x_1 = (w_1, y_1)\\)) used \nfitting process, leave-one-cross-validation results possibly less\nbiased estimate true risk; typically, leave-one-approach \noverestimate risk much holdout method. hand, since\nestimate risk based single sample, typically highly\nvariable estimate.can repeat process spiting data training validation set\nsamples part validation set point. example,\nnext iteration cross-validation might \\(x_2 = (w_2,y_2)\\) \nvalidation set rest \\(n-1\\) samples training set. Repeating\napproach \\(n\\) times results , example, \\(n\\) squared errors \\(MSE_1, MSE_2, \\ldots, MSE_n\\). estimate true risk average \n\\(n\\) squared errors. leave-one-cross-validation results less\nbiased (albeit, variable) estimate risk holdout method, \nexpensive implement \\(n\\) large.illustrate usage leave-one-cross-validation origami\npackage ; use function folds_loo(n). order setup\nfolds_loo(n), similarly re-substitution method, just need total\nnumber samples want cross-validate. show first two folds\ngenerated leave-one-cross-validation .","code":"folds <- folds_loo(nrow(washb_data))\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 1\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 2\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"v-fold","chapter":"5 Cross-validation","heading":"5.5.1.4 V-fold","text":"alternative leave-one-V-fold cross-validation. \ncross-validation scheme randomly divides data \\(v\\) sets (folds) equal\nsize; fold, number samples validation set .\nV-fold cross-validation, one folds treated validation set,\nwhereas proposed algorithm fit remaining \\(v-1\\) folds \ntraining set. loss, example MSE, computed samples \nvalidation set. proposed algorithm trained performance\nevaluated first fold, repeat process \\(v\\) times; time, \ndifferent group samples treated validation set. Note V-fold\ncross-validation effectively use data train evaluate \nproposed algorithm without overfitting training data. end, \nV-fold cross-validation results \\(v\\) estimates validation error. final\nV-fold CV estimate computed average validation losses.dataset \\(n\\) samples, V-fold cross-validation \\(v=n\\) just\nleave-one-; similarly, set \\(n=1\\), can get holdout method’s\nestimate algorithm’s performance. Despite obvious computational\nadvantages, V-fold cross-validation often gives accurate estimates \ntrue risk. reason comes bias-variance trade-comes\nemploying methods; leave-one-might less biased, \nhigher variance. difference becomes obvious \\(v<<n\\) (\nsmall, increase bias). V-fold cross-validation, end \naveraging output \\(v\\) fits typically less correlated \noutputs leave-one-fits. Since mean many highly correlated\nquantities higher variance, leave-one-estimate risk \nhigher variance estimate based V-fold cross-validation.Let’s see V-fold cross-validation origami action! next chapter\nstudy Super Learner — actual algorithm fit evaluate\nperformance. Super Learner relies V-fold cross-validation default cross-validation scheme. order\nset V-fold CV, need call function folds_vfold(n, V). Arguments\nfolds_vfold(n, V) require total number samples \ncross-validated, number folds want get.\\(V=2\\), get 2 folds \\(n/2\\) number samples training \nvalidation set.","code":"folds <- folds_vfold(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\n$validation_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\n$validation_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"monte-carlo","chapter":"5 Cross-validation","heading":"5.5.1.5 Monte Carlo","text":"Monte Carlo cross-validation, randomly select fraction data\n(without replacement) form training set; assign rest \nsamples validation set. , data repeatedly randomly\ndivided two sets, training set \\(n_0 = n \\cdot (1-p)\\) observations \nvalidation set \\(n_1 = n \\cdot p\\) observations. process \nrepeated multiple times, generating (random) new training validation\npartitions time.Since partitions independent across folds, sample can appear \nvalidation set multiple times – note stark difference\nMonte Carlo V-fold cross-validation. given \\(p\\), Monte Carlo\ncross-validation optimal done infinite times, \ncomputationally feasible. Monte Carlo\ncross-validation, one able explore many available partitions \nV-fold cross-validation – resulting possibly less variable estimate\nrisk, cost increase bias. many overlapping splits,\noften also need splits (thus computational time) achieve\nV-fold performance \\(V\\) splits.illustrate usage Monte Carlo cross-validation origami\npackage using function folds_montecarlo(n, V, pvalidation). order\nsetup folds_montecarlo(n, V, pvalidation), need:total number samples want cross-validate;number folds;proportion observations placed validation set.\\(V=2\\) \\(pvalidation=0.2\\), obtain 2 folds approximately \\(6\\) samples\nvalidation set per fold.","code":"folds <- folds_montecarlo(nrow(washb_data), V = 2, pvalidation = 0.2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1] 19 27 16 29 23 12  1  3 18 11  5  7  8  6  9 22 10 25 20 28 15  2 24 26\n\n$validation_set\n[1]  4 13 14 17 21 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 19 15 28 25 29 11 20 17 14  4  9 12 30  8 27 18 16 10 13  6 24  3 26  1\n\n$validation_set\n[1]  2  5  7 21 22 23\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"bootstrap","chapter":"5 Cross-validation","heading":"5.5.1.6 Bootstrap","text":"bootstrap cross-validation also consists randomly selecting samples, \nreplacement, training set. rest samples picked \ntraining set allocated validation set. process repeated\nmultiple times, generating (random) new training validation partitions\ntime. contract Monte Carlo cross-validation, total number \nsamples training validation size across folds constant. also\nsample replacement, hence samples can multiple training\nsets. proportion observations validation sets random\nvariable, expectation \\(\\sim 0.368\\).illustrate usage bootstrap cross-validation origami package\nusing function folds_bootstrap(n, V). order setup\nfolds_bootstrap(n, V), need:total number samples want cross-validate;number folds.\\(V=2\\), obtain \\(2\\) folds different number samples validation\nset across folds.","code":"folds <- folds_bootstrap(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  5 30  1 29 16 10 11  8 25 28  2 11  2 16 28 15 28  1 27  9 19 20 30 18\n[26] 11 13  2 18 12\n\n$validation_set\n [1]  3  4  6  7 14 17 21 22 23 24 26\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 12 16 10 29 22 15 27  9 27 16 12 28 10 28 26  1 14  6 23 14 21 16  5 20  8\n[26] 23 25  8 27  5\n\n$validation_set\n [1]  2  3  4  7 11 13 17 18 19 24 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"cross-validation-for-dependent-data","chapter":"5 Cross-validation","heading":"5.5.2 Cross-validation for dependent data","text":"origami package also supports numerous cross-validation schemes \ntime-series data, single multiple time-series arbitrary time\nnetwork dependence.","code":""},{"path":"origami.html","id":"airpassenger-example","chapter":"5 Cross-validation","heading":"AirPassenger Example","text":"order illustrate different cross-validation schemes time-series, \nusing AirPassenger data; widely used, freely available\ndataset. AirPassenger dataset R provides monthly totals \ninternational airline passengers 1949 1960. dataset already \ntime series class therefore class date manipulation required.Goal: want forecast number airline passengers time \\(h\\)\nhorizon using historical data 1949 1960.","code":"\nlibrary(ggfortify)\n\ndata(AirPassengers)\nAP <- AirPassengers\n\nautoplot(AP) +\n  labs(\n    x = \"Date\",\n    y = \"Passenger numbers (1000's)\",\n    title = \"Air Passengers from 1949 to 1961\"\n  )\n\nt <- length(AP)"},{"path":"origami.html","id":"rolling-origin","chapter":"5 Cross-validation","heading":"5.5.2.1 Rolling origin","text":"Rolling origin cross-validation scheme lends “online” algorithms,\nlarge streams data fit continually, final fit \nconstantly updated data acquired. general, rolling origin\nscheme defines initial training set, iteration size \ntraining set grows \\(m\\) observations reach time \\(t\\) particular\nfold. time points included training set always behind \nvalidation set time points; addition, might gap training\nvalidation times size \\(h\\).illustrate rolling origin cross-validation, show example\n3 folds. , first window size 15 time points, first\ntrain proposed algorithm. evaluate performance 10 time\npoints, gap size 5 training validation time points.\nfollowing fold, train algorithm longer stream data, 25\ntime points, including original 15 started . evaluate \nperformance 10 time points future.\nFIGURE 5.1: Rolling origin CV\nillustrate usage rolling origin cross-validation origami\npackage using function folds_rolling_origin(n, first_window, validation_size, gap, batch). order setup folds_rolling_origin(n, first_window, validation_size, gap, batch), need:total number time points want cross-validate;size first training set;size validation set;gap training validation set;size update training set per iteration CV.time-series \\(t=144\\) time points. Setting first_window \\(50\\),\nvalidation_size 10, gap 5 batch 20, get 4 time-series\nfolds; show first two .","code":"folds <- folds_rolling_origin(\n  t,\n  first_window = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-window","chapter":"5 Cross-validation","heading":"5.5.2.2 Rolling window","text":"Instead adding time points training set per iteration, \nrolling window cross-validation scheme “rolls” training sample forward \n\\(m\\) time units. rolling window scheme might considered parametric\nsettings one wishes guard moment parameter drift \ndifficult model explicitly; also efficient computationally\ndemanding settings streaming data, large amounts training\ndata stored. contrast rolling origin CV, training sample \niteration rolling window scheme always .illustrate rolling window cross-validation 3 time-series folds\n. first window size 15 time points, first train \nproposed algorithm. previous illustration, evaluate performance\n10 time points, gap size 5 training validation time\npoints. However, next fold, train algorithm time points\naway origin (, 10 time points). Note size \ntraining set new fold first fold (15 time points).\nsetup keeps training sets comparable time (fold) compared\nrolling origin CV. evaluate performance proposed\nalgorithm 10 time points future.\nFIGURE 5.2: Rolling window CV\nillustrate usage rolling window cross-validation origami\npackage using function folds_rolling_window(n, window_size, validation_size, gap, batch). order setup folds_rolling_window(n, window_size, validation_size, gap, batch), need:total number time points want cross-validate;size training sets;size validation set;gap training validation set;size update training set per iteration CV.Setting window_size \\(50\\), validation_size 10, gap 5 \nbatch 20, also get 4 time-series folds; show first two .","code":"folds <- folds_rolling_window(\n  t,\n  window_size = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-origin-with-v-fold","chapter":"5 Cross-validation","heading":"5.5.2.3 Rolling origin with V-fold","text":"variant rolling origin scheme accounts sample dependence \nrolling-origin-\\(V\\)-fold cross-validation. contrast canonical rolling\norigin CV, samples training validation set , \nvariant encompasses \\(V\\)-fold CV addition time-series setup. \npredictions evaluated future times time-series units seen\ntraining step, allowing dependence samples time. One\ncan use rolling-origin-\\(v\\)-fold cross-validation origami package\nusing function folds_vfold_rolling_origin_pooled(n, t, id, time, V, first_window, validation_size, gap, batch). figure , show \\(V=2\\)\n\\(V\\)-folds, 2 time-series CV folds.\nFIGURE 5.3: Rolling origin V-fold CV\n","code":""},{"path":"origami.html","id":"rolling-window-with-v-fold","chapter":"5 Cross-validation","heading":"5.5.2.4 Rolling window with v-fold","text":"Analogous previous section, can extend rolling window CV support\nmultiple time-series arbitrary sample dependence. One can use \nrolling-window-\\(V\\)-fold cross-validation origami package using \nfunction folds_vfold_rolling_window_pooled(n, t, id, time, V, window_size, validation_size, gap, batch). figure , show \\(V=2\\) \\(V\\)-folds, \n2 time-series CV folds.\nFIGURE 5.4: Rolling window V-fold CV\n","code":""},{"path":"origami.html","id":"general-workflow-of-origami","chapter":"5 Cross-validation","heading":"5.6 General workflow of origami","text":"dive details, let’s take moment review \nbasic functionality origami R package. main function origami\ncross_validate. start , user must define fold structure function\noperates fold. passed cross_validate, cross_validate\napply specified function fold, combine fold-specific results meaningful way. see action later sections; \nnow, provide specific details step process .","code":""},{"path":"origami.html","id":"define-folds","chapter":"5 Cross-validation","heading":"5.6.1 (1) Define folds","text":"folds object passed cross_validate list folds; lists can\ngenerated using make_folds function. fold consists list \ntraining index vector, validation index vector, fold_index (\norder list folds). function supports variety \ncross-validation schemes describe following section. make_folds\ncan balance across levels variable (strata_ids), can also keep\nobservations independent unit together (cluster).","code":""},{"path":"origami.html","id":"define-fold-function","chapter":"5 Cross-validation","heading":"5.6.2 (2) Define fold function","text":"cv_fun argument cross_validate function perform \noperation fold. first argument function must fold,\nreceive individual fold object operate . Additional arguments\ncan passed cv_fun using ... argument cross_validate. Within\nfunction, convenience functions training, validation \nfold_index can return various components fold object. training\nvalidation passed object, index sensible way.\ninstance, vector, index vector directly; \ndata.frame matrix, index rows. allows user easily\npartition data training validation sets. fold function must return\nnamed list results containing whatever fold-specific outputs generated.","code":""},{"path":"origami.html","id":"apply-cross_validate","chapter":"5 Cross-validation","heading":"5.6.3 (3) Apply cross_validate","text":"defining folds, cross_validate can used map cv_fun across\nfolds using future_lapply. means can easily parallelized\nspecifying parallelization scheme (.e., plan future\nparallelization framework R\n(Bengtsson 2021)). application cross_validate generates list\nresults. described , call cv_fun returns list \nresults, different elements type result care . main\nloop generates list individual lists results (sort \n“meta-list”). “meta-list” inverted one element\nper result type (list results fold). default,\ncombine_results used combine results type lists sensible\nmanner. results combined determined automatically examining \ndata types results first fold. can modified \nspecifying list arguments .combine_control.","code":""},{"path":"origami.html","id":"cross-validation-in-action","chapter":"5 Cross-validation","heading":"5.7 Cross-validation in action","text":"Let’s see origami action! following chapter learn use\ncross-validation Super Learner, can utilize power \ncross-validation build optimal ensembles algorithms, just use \nsingle statistical learning method.","code":""},{"path":"origami.html","id":"cross-validation-with-linear-regression","chapter":"5 Cross-validation","heading":"5.7.1 Cross-validation with linear regression","text":"First, load relevant R packages, set seed, load full\nWASH data . order illustrate cross-validation origami \nlinear regression, focus predicting weight--height Z-score\nwhz using available covariate data. stated previously, \nassume data independent identically distributed, ignoring cluster\nstructure imposed clinical trial design. sake illustration, \nwork subset data, remove samples missing data \ndataset; learn next chapter deal missingness.’s look data:can see covariates used prediction:Next, fit linear model full data, goal predicting \nweight--height Z-score whz using available covariate data. Let’s\ntry :can assess well model fits data comparing predictions \nlinear model true outcomes observed data set. well\nknown (standard) mean squared error. can extract lm model\nobject follows:mean squared error 0.86568. important problem arises\nassess model way - , trained linear\nregression model full data set assessed error full data\nset, using data. , course, generally interested \nwell model explains variation observed data; rather, \ninterested explanation provided model generalizes target\npopulation sample presumably derived. used \navailable data, honestly evaluate well model fits (thus\nexplains) variation population level.resolve issue, cross-validation allows particular procedure (e.g.,\nlinear regression) implemented subsets data, evaluating \nwell procedure fits testing (“validation”) set, thereby providing \nhonest evaluation error.can easily add cross-validation linear regression procedure using\norigami. First, let us define new function perform linear regression \nspecific partition data (called “fold”):cv_lm function rather simple: merely split available data \ntraining validation sets (using eponymous functions provided \norigami) fit linear model training set, evaluate model \nvalidation set. simple example origami considers \ncv_fun — functions using cross-validation perform particular routine\ninput data set. defined function, can simply generate \nset partitions using origami’s make_folds function, apply cv_lm\nfunction resultant folds object. , replicate \nre-substitution estimate error – “hand” – using\nfunctions make_folds cv_lm.(nearly) matches estimate error obtained .can honestly evaluate error V-fold cross-validation, \npartitions data \\(v\\) subsets, fitting model \\(v - 1\\) \nsubsets evaluating subset held testing. \nrepeated subset used validation. can easily apply \ncv_lm function using origami’s cross_validate (n.b., default \nperforms 10-fold cross-validation):performed 10-fold cross-validation, quickly notice previous\nestimate model error (resubstitution) bit optimistic. honest\nestimate error larger!","code":"\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# Remove missing data, then pick just the first 500 rows\nwashb_data <- washb_data %>%\n  drop_na() %>%\n  slice(1:500)\n\noutcome <- \"whz\"\ncovars <- colnames(washb_data)[-which(names(washb_data) == outcome)]outcome\n[1] \"whz\"\ncovars\n [1] \"tr\"             \"fracode\"        \"month\"          \"aged\"          \n [5] \"sex\"            \"momage\"         \"momedu\"         \"momheight\"     \n [9] \"hfiacat\"        \"Nlt18\"          \"Ncomp\"          \"watmin\"        \n[13] \"elec\"           \"floor\"          \"walls\"          \"roof\"          \n[17] \"asset_wardrobe\" \"asset_table\"    \"asset_chair\"    \"asset_khat\"    \n[21] \"asset_chouki\"   \"asset_tv\"       \"asset_refrig\"   \"asset_bike\"    \n[25] \"asset_moto\"     \"asset_sewmach\"  \"asset_mobile\"  lm_mod <- lm(whz ~ ., data = washb_data)\nsummary(lm_mod)\n\nCall:\nlm(formula = whz ~ ., data = washb_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8890 -0.6799 -0.0169  0.6595  3.1005 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                     -1.89006    1.72022   -1.10   0.2725   \ntrHandwashing                   -0.25276    0.17032   -1.48   0.1385   \ntrNutrition                     -0.09695    0.15696   -0.62   0.5371   \ntrNutrition + WSH               -0.09587    0.16528   -0.58   0.5622   \ntrSanitation                    -0.27702    0.15846   -1.75   0.0811 . \ntrWSH                           -0.02846    0.15967   -0.18   0.8586   \ntrWater                         -0.07148    0.15813   -0.45   0.6515   \nfracodeN05160                    0.62355    0.30719    2.03   0.0430 * \nfracodeN05265                    0.38762    0.31011    1.25   0.2120   \nfracodeN05359                    0.10187    0.31329    0.33   0.7452   \nfracodeN06229                    0.30933    0.29766    1.04   0.2993   \nfracodeN06453                    0.08066    0.30006    0.27   0.7882   \nfracodeN06458                    0.43707    0.29970    1.46   0.1454   \nfracodeN06473                    0.45406    0.30912    1.47   0.1426   \nfracodeN06479                    0.60994    0.31463    1.94   0.0532 . \nfracodeN06489                    0.25923    0.31901    0.81   0.4169   \nfracodeN06500                    0.07539    0.35794    0.21   0.8333   \nfracodeN06502                    0.36748    0.30504    1.20   0.2290   \nfracodeN06505                    0.20038    0.31560    0.63   0.5258   \nfracodeN06516                    0.55455    0.29807    1.86   0.0635 . \nfracodeN06524                    0.49429    0.31423    1.57   0.1164   \nfracodeN06528                    0.75966    0.31060    2.45   0.0148 * \nfracodeN06531                    0.36856    0.30155    1.22   0.2223   \nfracodeN06862                    0.56932    0.29293    1.94   0.0526 . \nfracodeN08002                    0.36779    0.26846    1.37   0.1714   \nmonth                            0.17161    0.10865    1.58   0.1149   \naged                            -0.00336    0.00112   -3.00   0.0029 **\nsexmale                          0.12352    0.09203    1.34   0.1802   \nmomage                          -0.01379    0.00973   -1.42   0.1570   \nmomeduPrimary (1-5y)            -0.13214    0.15225   -0.87   0.3859   \nmomeduSecondary (>5y)            0.12632    0.16041    0.79   0.4314   \nmomheight                        0.00512    0.00919    0.56   0.5776   \nhfiacatMildly Food Insecure      0.05804    0.19341    0.30   0.7643   \nhfiacatModerately Food Insecure -0.01362    0.12887   -0.11   0.9159   \nhfiacatSeverely Food Insecure   -0.13447    0.25418   -0.53   0.5970   \nNlt18                           -0.02557    0.04060   -0.63   0.5291   \nNcomp                            0.00179    0.00762    0.23   0.8145   \nwatmin                           0.01347    0.00861    1.57   0.1182   \nelec                             0.08906    0.10700    0.83   0.4057   \nfloor                           -0.17763    0.17734   -1.00   0.3171   \nwalls                           -0.03001    0.21445   -0.14   0.8888   \nroof                            -0.03716    0.49214   -0.08   0.9399   \nasset_wardrobe                  -0.05754    0.13736   -0.42   0.6755   \nasset_table                     -0.22079    0.12276   -1.80   0.0728 . \nasset_chair                      0.28012    0.13750    2.04   0.0422 * \nasset_khat                       0.02306    0.11766    0.20   0.8447   \nasset_chouki                    -0.13943    0.14084   -0.99   0.3227   \nasset_tv                         0.17723    0.12972    1.37   0.1726   \nasset_refrig                     0.12613    0.23162    0.54   0.5863   \nasset_bike                      -0.02568    0.10083   -0.25   0.7990   \nasset_moto                      -0.32094    0.19944   -1.61   0.1083   \nasset_sewmach                    0.05090    0.17795    0.29   0.7750   \nasset_mobile                     0.01420    0.14972    0.09   0.9245   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.984 on 447 degrees of freedom\nMultiple R-squared:  0.129, Adjusted R-squared:  0.0277 \nF-statistic: 1.27 on 52 and 447 DF,  p-value: 0.104(err <- mean(resid(lm_mod)^2))\n[1] 0.86568\ncv_lm <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # split up data into training and validation sets\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit linear model on training set and predict on validation set\n  mod <- lm(as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # capture results to be returned as output\n  out <- list(\n    coef = data.frame(t(coef(mod))),\n    SE = (preds - valid_data[, out_var_ind])^2\n  )\n  return(out)\n}# re-substitution estimate\nresub <- make_folds(washb_data, fold_fun = folds_resubstitution)[[1]]\nresub_results <- cv_lm(fold = resub, data = washb_data, reg_form = \"whz ~ .\")\nmean(resub_results$SE, na.rm = TRUE)\n[1] 0.86568# cross-validated estimate\nfolds <- make_folds(washb_data)\ncvlm_results <- cross_validate(\n  cv_fun = cv_lm, folds = folds, data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvlm_results$SE, na.rm = TRUE)\n[1] 1.35"},{"path":"origami.html","id":"cross-validation-with-random-forests","chapter":"5 Cross-validation","heading":"5.7.2 Cross-validation with random forests","text":"examine origami , let us return example analysis using \nWASH data set. , write new cv_fun type object. example, \nuse Breiman’s randomForest (Breiman 2001):, writing cv_rf function cross-validate randomForest, used\nprevious function cv_lm example. now, individual cv_fun must\nwritten hand; however, future releases, wrapper may available \nsupport auto-generating cv_funs used origami., use cross_validate apply new cv_rf function folds\nobject generated make_folds.Using 10-fold cross-validation (default), obtain honest estimate \nprediction error random forests. , gather use \norigami’s cross_validate procedure can generalized arbitrary estimation\ntechniques, given availability appropriate cv_fun function.","code":"\n# make sure to load the package!\nlibrary(randomForest)\n\ncv_rf <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # define training and validation sets based on input object of class \"folds\"\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit Random Forest regression on training set and predict on holdout set\n  mod <- randomForest(formula = as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # define output object to be returned as list (for flexibility)\n  out <- list(\n    coef = data.frame(mod$coefs),\n    SE = ((preds - valid_data[, out_var_ind])^2)\n  )\n  return(out)\n}# now, let's cross-validate...\nfolds <- make_folds(washb_data)\ncvrf_results <- cross_validate(\n  cv_fun = cv_rf, folds = folds, \n  data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvrf_results$SE)\n[1] 1.0271"},{"path":"origami.html","id":"cross-validation-with-arima","chapter":"5 Cross-validation","heading":"5.7.3 Cross-validation with arima","text":"Cross-validation can also used forecast model selection time series\nsetting. , partitioning scheme mirrors application \nforecasting model: ’ll train data past observations (either \navailable recent subset), use model fit predict next\nobservations. consider AirPassengers dataset , monthly time\nseries passenger air traffic thousands people.Suppose want pick two forecasting models different arima\nconfigurations. can evaluating forecasting performance.\nFirst, set appropriate cross-validation scheme time-series.default, folds_rolling_origin increase size training set \none time point fold. followed default option, 85\nfolds train! Luckily, can pass batch option \nfolds_rolling_origin tells increase size training set \n10 points iteration. Since want forecast immediate next point,\ngap argument remains default (0).arima model AR component seems better fit data.","code":"data(AirPassengers)\nprint(AirPassengers)\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432folds <- make_folds(AirPassengers,\n  fold_fun = folds_rolling_origin,\n  first_window = 36, validation_size = 24, batch = 10\n)\n\n# How many folds where generated?\nlength(folds)\n[1] 9\n\n# Examine the first 2 folds.\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n$validation_set\n [1] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n\n$validation_set\n [1] 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\nattr(,\"class\")\n[1] \"fold\"# make sure to load the package!\nlibrary(forecast)\n\n# function to calculate cross-validated squared error\ncv_forecasts <- function(fold, data) {\n  # Get training and validation data\n  train_data <- training(data)\n  valid_data <- validation(data)\n  valid_size <- length(valid_data)\n\n  train_ts <- ts(log10(train_data), frequency = 12)\n\n  # First arima model\n  arima_fit <- arima(train_ts, c(0, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred <- predict(arima_fit, n.ahead = valid_size)\n  arima_pred <- 10^raw_arima_pred$pred\n  arima_MSE <- mean((arima_pred - valid_data)^2)\n\n  # Second arima model\n  arima_fit2 <- arima(train_ts, c(5, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred2 <- predict(arima_fit2, n.ahead = valid_size)\n  arima_pred2 <- 10^raw_arima_pred2$pred\n  arima_MSE2 <- mean((arima_pred2 - valid_data)^2)\n\n  out <- list(mse = data.frame(\n    fold = fold_index(),\n    arima = arima_MSE, arima2 = arima_MSE2\n  ))\n  return(out)\n}\n\nmses <- cross_validate(\n  cv_fun = cv_forecasts, folds = folds, data = AirPassengers,\n  use_future = FALSE\n)\nmses$mse\n  fold   arima  arima2\n1    1   68.21  137.28\n2    2  319.68  313.15\n3    3  578.35  713.36\n4    4  428.69  505.31\n5    5  407.33  371.27\n6    6  281.82  250.99\n7    7  827.56  910.12\n8    8 2099.59 2213.15\n9    9  398.37  293.38\ncolMeans(mses$mse[, c(\"arima\", \"arima2\")])\n arima arima2 \n601.07 634.22 "},{"path":"origami.html","id":"exercises","chapter":"5 Cross-validation","heading":"5.8 Exercises","text":"","code":""},{"path":"origami.html","id":"review-of-key-concepts","chapter":"5 Cross-validation","heading":"5.8.1 Review of Key Concepts","text":"Compare contrast V-fold cross-validation resubstitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.Compare contrast V-fold cross-validation resubstitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.advantages disadvantages \\(v\\)-fold CV relative :\nholdout CV?\nleave-one-CV?\nadvantages disadvantages \\(v\\)-fold CV relative :holdout CV?leave-one-CV?can’t use V-fold cross-validation time-series data?can’t use V-fold cross-validation time-series data?use rolling window origin non-stationary time-series? ?use rolling window origin non-stationary time-series? ?","code":""},{"path":"origami.html","id":"the-ideas-in-action","chapter":"5 Cross-validation","heading":"5.8.2 The Ideas in Action","text":"Let \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\). kind \ncross-validation scheme used rare outcome? can \norigami package?Let \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\). kind \ncross-validation scheme used rare outcome? can \norigami package?Consider WASH benefits dataset presented chapter. can \ninclude cluster information cross-validation? can \norigami package?Consider WASH benefits dataset presented chapter. can \ninclude cluster information cross-validation? can \norigami package?","code":""},{"path":"origami.html","id":"advanced-topics","chapter":"5 Cross-validation","heading":"5.8.3 Advanced Topics","text":"Think dataset arbitrary spatial dependence, know\nextent dependence, groups formed dependence clear\nspillover effects. kind cross-validation can use?Think dataset arbitrary spatial dependence, know\nextent dependence, groups formed dependence clear\nspillover effects. kind cross-validation can use?Continuing last problem, kind procedure, cross-validation\nmethod, can use spatial dependence clearly defined \nprevious problem?Continuing last problem, kind procedure, cross-validation\nmethod, can use spatial dependence clearly defined \nprevious problem?Consider classification problem large number predictors. \nstatistician proposes following analysis:\nFirst screen predictors, leaving covariates strong\ncorrelation class labels.\nFit algorithm using subset highly correlated covariates.\nUse cross-validation estimate tuning parameters performance\nproposed algorithm.\ncorrect application cross-validation? ?Consider classification problem large number predictors. \nstatistician proposes following analysis:First screen predictors, leaving covariates strong\ncorrelation class labels.Fit algorithm using subset highly correlated covariates.Use cross-validation estimate tuning parameters performance\nproposed algorithm.correct application cross-validation? ?","code":""},{"path":"sl3.html","id":"sl3","chapter":"6 Super (Machine) Learning","heading":"6 Super (Machine) Learning","text":"Rachael PhillipsBased sl3 R package Jeremy\nCoyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Oleg Sofrygin.Updated: 2021-12-22","code":""},{"path":"sl3.html","id":"learning-objectives-3","chapter":"6 Super (Machine) Learning","heading":"Learning Objectives","text":"end chapter able :Select objective function () aligns intention \nanalysis (ii) optimized target parameter.Select objective function () aligns intention \nanalysis (ii) optimized target parameter.Assemble diverse library learners considered Super Learner\nensemble. particular, able :\nCustomize learner modifying ’s tuning parameters.\nCreate several different versions learner \nspecifying grid tuning parameters.\nCurate covariate screening pipelines order pass screener’s\noutput, subset covariates, input another learner \nuse subset covariates selected screener model data.\nAssemble diverse library learners considered Super Learner\nensemble. particular, able :Customize learner modifying ’s tuning parameters.Create several different versions learner \nspecifying grid tuning parameters.Curate covariate screening pipelines order pass screener’s\noutput, subset covariates, input another learner \nuse subset covariates selected screener model data.Specify learner ensembling (metalearner) corresponds\nobjective function.Specify learner ensembling (metalearner) corresponds\nobjective function.Fit Super Learner ensemble nested cross-validation obtain \nestimate performance ensemble --sample data.Fit Super Learner ensemble nested cross-validation obtain \nestimate performance ensemble --sample data.Obtain sl3 variable importance metrics.Obtain sl3 variable importance metrics.Interpret fit discrete continuous Super Learners’ \ncross-validated risk table coefficients.Interpret fit discrete continuous Super Learners’ \ncross-validated risk table coefficients.Justify base library machine learning algorithms ensembling\nlearner terms prediction problem, statistical model \\(\\M\\), data\nsparsity, dimensionality covariates.Justify base library machine learning algorithms ensembling\nlearner terms prediction problem, statistical model \\(\\M\\), data\nsparsity, dimensionality covariates.","code":""},{"path":"sl3.html","id":"motivation","chapter":"6 Super (Machine) Learning","heading":"Motivation","text":"common task data analysis prediction – using observed data (input\nvariables outcomes) learn function can map new input variables\npredicted outcome.\ndata, algorithms learn complex relationships variables\nnecessary adequately model data. data, main terms\nregression models might fit data quite well.generally impossible know priori algorithm best\ngiven data set prediction problem. ’s like picking winner \nGreat British Bake start first week!Super Learner solves issue algorithm selection creating \nensemble many algorithms, simplest (intercept-) \ncomplex (neural nets, tree-based methods, support vector machines, etc.).Super Learner works using cross-validation manner theoretically\n(large samples) guarantees resulting fit good possible,\ngiven algorithms provided.","code":""},{"path":"sl3.html","id":"introduction-2","chapter":"6 Super (Machine) Learning","heading":"6.1 Introduction","text":"Chapter 1, introduced Roadmap Targeted\nLearning general template translate real-world data\napplications formal statistical estimation problems. first steps \nroadmap define statistical estimation problem, establishThe data $O$ random variable, equivalently, realization \nparticular experiment/study, probability distribution \\(P_0\\).\nwritten \\(O \\sim P_0\\), \\(P_0\\) also commonly referred \ndata-generating process (DGP) also data-generating distribution\n(DGD). data structure \\(O\\) comprised variables, \nvector covariates \\(W\\), treatment exposure \\(\\), outcome \\(Y\\),\n\\(O=(W,,Y) \\sim P_0\\). often observe random variable \\(O\\) \\(n\\) times, \nrepeating common experiment \\(n\\) times. example, \\(O_1,\\ldots, O_n\\)\nrandom variables result random sample \\(n\\) subjects \npopulation, collecting baseline characteristics \\(W\\), randomly assigning\ntreatment \\(\\), later measuring outcome \\(Y\\).statistical model \\(\\M\\) set possible probability distributions\ngiven rise data. ’s essential \\(\\M\\) \nconstrained factual subject-matter knowledge order guarantee \\(P_0\\)\nresides statistical model, written \\(P_0 \\\\M\\). Continuing\nexample step 1, following restrictions placed \nstatistical model: \\(O_1, \\ldots, O_n\\) observations data \nindependent identically distributed (..d.), assignment \ntreatment \\(\\) random based covariates \\(W\\).translation scientific question interest function \n\\(P_0\\), target statistical estimand \\(\\Psi(P_0)\\). example, might\ninterested average difference mean outcomes treatment\n\\(=1\\) versus placebo \\(=0\\):\n\\(\\Psi(P_0)=E_{P_0}\\Big[E_{P_0}(Y|=1,W)−E_{P_0}(Y|=0,W)\\Big]\\). Note\n, scientific question causal, ’s translation \nproduce target causal estimand; another layer translation,\nidentifiability, required express target causal estimand \nfunction observed data distribution \\(P_0\\). See causal target\nparameters information causal quantities, causal models\nidentifiability.statistical estimation problem established, estimator\ncan constructed. Estimators (also referred algorithms learners)\nfunctions take input observed data, return output \nestimate \\(P_0\\) feature \\(P_0\\) (component target\nestimand). use Super Learner (SL) algorithm estimate \nprediction function, use SL estimator prediction\nfunction predict outcomes new input (e.g., covariate/predictor) data.\nOccasionally, prediction function target estimand; \ncommonly, prediction function component target estimand.Consider example need estimate prediction function \n\\(E_{P_0}(Y|,W)\\) (conditional mean outcome, given treatment \\(\\) \nbaseline covariates \\(W\\)), can predict outcomes \nhypothetical scenario subjects received treatment \\(=1\\). \norder estimator output predictions correspond outcomes \nworld subjects received treatment \\(=1\\), need supply\nestimator input data reflects ; specifically, baseline\ncovariate information \\(W\\) remain treatment \nobserved data, treatment \\(\\) set 1 individuals,\nregardless whether actually received . learning paradigm\ncorresponds estimation component target statistical estimand\nmentioned step 3 , \\(E_{P_0}(Y|=1,W)\\) component \\(\\Psi(P_0)\\).various strategies estimators can employ model relationships\nobserved data, “one fits ” algorithm realm\nreal-world data science. However, statistical performance algorithms’\n(e.g., mean squared error) can used compare . Therefore, algorithm\nselection driven criteria () proven optimize\nrelevant statistical properties (e.g., provide theoretical guarantees) (ii)\nshown reliable practice (e.g., complex real-world data). SL\nalgorithm equipped standard, cross-validation\ncriterion, ensures large samples SL perform atleast \nwell unknown best-performing candidate algorithm (van der Laan Dudoit 2003; Van der Vaart, Dudoit, Laan 2006; van der Laan, Polley, Hubbard 2007). Also, ensemble machine learning\nalgorithm, SL leverages information learned variety candidate\nalgorithms creating weighted combination \n(.e., metalearning). summary, SL represents practical approach \nprincipled machine learning. shown adaptive robust, even\nsmall samples (Polley van der Laan 2010).","code":""},{"path":"sl3.html","id":"candidate-learners-and-ensembling","chapter":"6 Super (Machine) Learning","heading":"6.1.1 Candidate Learners and Ensembling","text":"set algorithms considering SL (also referred “library”)\nconsist align ’s known DGP \nknown DGP. words, learners library \ntailored respect statistical model \\(\\mathcal{M}\\), terms ofthe restrictions placed \\(\\mathcal{M}\\), candidate algorithms\nrepresent functions align knowledge DGP, andthe vastness \\(\\mathcal{M}\\), library able adapt \ndiversity possible forms DGP, can acheived including\nvariety learning strategies (e.g., range parametric regression\nmodels multi-step algorithms involving screening covariates,\npenalizations, optimizing tuning parameters, etc.)","code":""},{"path":"sl3.html","id":"example-respecting-known-bounds-on-the-outcome","chapter":"6 Super (Machine) Learning","heading":"6.1.1.1 Example: Respecting known bounds on the outcome","text":"Suppose known outcome take certain values,\n(e.g., outcome always positive real number). statistical model\nconstrained reflect outcome bounds, order \nlearners library respect statistical model, learners \nconstructed predictions fall outside outcome\nbounds. learners allow link functions (e.g., generalized elastic-net\nregression models), different link functions can chosen match known\noutcome bounds. learner support link functions, \nbounding criteria model fitting, possibility \nlearner yield predictions fall outside known outcome bounds. \nscenario, predictions within known bounds \ntruncated, might fine learners seldomly produce predictions\nviolate known outcome bounds.general, use link function(s) sensible, since formulates \nmodel function respects statistical model, optimizes\nfit model. truncation option optimizes model ’s \nbig, losing information, corrects ad hoc. However, might \nlimiting include learners support desired link function\n(e.g., library several logistic regression models), since diversity \npossible DGPs might captured library.Recall SL weighted combination library candidate\nalgorithms, weighted combinations created equally. order \nensure SL bounds predictions candidates \nlibrary, SL’s weighted combination convex combination\n(.e., weights non-negative sum one). simple example \nconvex combination -called “discrete SL” \n“cross-validated selector”, uses metalearner assigns weight \none best candidate algorithm library, weight zero \nothers (best described step 4() step--step\noverview ). flexible metalearners,\nlike default metalearner sl3, allow multiple algorithms\nnonzero weights still enforce convex combination.Keep mindThe learners SL library justified terms prediction\nproblem, statistical model \\(\\M\\), data sparsity (e.g., number \nindependent samples, number events rare binary outcomes), number\ncovariates. metalearning strategy similarly justified.","code":""},{"path":"sl3.html","id":"sl3-steps","chapter":"6 Super (Machine) Learning","heading":"6.1.2 Fitting the Super Learner","text":"","code":""},{"path":"sl3.html","id":"cross-validation","chapter":"6 Super (Machine) Learning","heading":"6.1.2.1 Cross-validation","text":"many different cross-validation schemes, designed \naccommodate different study designs, data structures, prediction\nproblems. See cross-validation detail.\nfigure shows example \\(V\\)-fold cross-validation \\(V=10\\)\nfolds, default cross-validation structure sl3 R\npackage. darker boxes represent -called “validation data” \nlighter boxes represent -called “training data”. following details\nimportant notice:Across folds, \\(V\\) (10) copies dataset. \ndifference copy coloring, distinguishes subset\ndata ’s considered training data subset ’s\nconsidered validation data.Within fold 1/\\(V\\) (1/10) data validation data.Across folds, data considered validation data \nobservation included twice validation data. Therefore, \ntotal number validation data observations across folds \nequal total number observations data.","code":""},{"path":"sl3.html","id":"step-by-step-procedure-with-v-fold-cross-validation","chapter":"6 Super (Machine) Learning","heading":"6.1.2.2 Step-by-step procedure with V-fold Cross-validation","text":"Fit learner (say \\(K\\) learners) whole dataset. refer\nlearners trained whole dataset “full-fit”\nlearners.Break data evenly \\(V\\) disjoint subsets. Separately, create\n\\(V\\) copies data. copy \\(v\\), \\(v=1,\\ldots,V\\), create \n\\(V\\) folds labelling portion data included subset\n\\(v\\) validation sample, labelling ’s remaining data\ntraining sample.fold \\(v\\), \\(v=1,\\ldots,V\\), fit learner (say \\(K\\)\nlearners) training sample predict validation sample outcomes\nproviding fitted learner validation sample covariates \ninput. Notice learner fit \\(V\\) times. refer \nlearners trained across \\(V\\) cross-validation folds \n“cross-validated fit” learners.Combine validation sample predictions folds learners \ncreate -called \\(K\\) column matrix “cross-validated predictions”.\nmatrix also commonly referred \\(Z\\) matrix. Notice \ncontains, learner, --sample predictions \nobservations data.Train metalearner (e.g., non-negative least squares regression) \ndata predictors outcomes \\(Z\\) matrix observed data\noutcomes, respectively. metalearner — just like ordinary ML\nalgorithm — estimates parameters ’s model using training data\nafterwards, fitted model can used obtain predicted outcomes\nnew input data. ’s special metalearner ’s\nestimated model parameters (e.g., regression coefficients) correspond \n’s predictors, variables \\(Z\\) matrix, \\(K\\) learners’\npredictions. metalearner fit, can used obtain predicted\noutcomes new input data; , new \\(K\\) learners predictions’ can \nsupplied fitted metalearner order obtain predicted outcomes.fitted metalearner full-fit learners define weighted\ncombination \\(K\\) learners, finalizing Super Learner (SL) fit. \nobtain SL predictions full-fit learners’ predictions first obtained\nfed input fitted metalearner; metalearner’s output\nSL predictions.","code":""},{"path":"sl3.html","id":"sl3-theory","chapter":"6 Super (Machine) Learning","heading":"6.1.3 Theoretical foundations","text":"section construction.detail Super Learner algorithm refer reader \nPolley van der Laan (2010) van der Laan, Polley, Hubbard (2007). optimality results \ncross-validation selector among family algorithms established \nvan der Laan Dudoit (2003) extended Van der Vaart, Dudoit, Laan (2006).","code":""},{"path":"sl3.html","id":"sl3-microwave-dinner-implementation","chapter":"6 Super (Machine) Learning","heading":"sl3 “Microwave Dinner” Implementation","text":"begin illustrating core functionality SL algorithm \nimplemented sl3.sl3 implementation consists following steps:Load necessary libraries dataDefine machine learning taskMake SL creating library base learners metalearnerTrain SL machine learning taskObtain predicted values","code":""},{"path":"sl3.html","id":"wash-benefits-study-example-1","chapter":"6 Super (Machine) Learning","heading":"WASH Benefits Study Example","text":"Using WASH Benefits Bangladesh data, interested predicting\nweight--height z-score whz using available covariate data. \ninformation dataset, data work , \ndescribed chapter tlverse\nhandbook. Let’s begin!","code":""},{"path":"sl3.html","id":"load-the-necessary-libraries-and-data","chapter":"6 Super (Machine) Learning","heading":"0. Load the necessary libraries and data","text":"First, load relevant R packages, set seed, load data.","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(SuperLearner)\nlibrary(origami)\nlibrary(sl3)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nhead(washb_data) %>%\n  kable() %>%\n  kableExtra:::kable_styling(fixed_thead = T) %>%\n  scroll_box(width = \"100%\", height = \"300px\")"},{"path":"sl3.html","id":"define-the-machine-learning-task","chapter":"6 Super (Machine) Learning","heading":"1. Define the machine learning task","text":"define machine learning task (predict weight--height Z-score\nwhz using available covariate data), need create sl3_Task\nobject.sl3_Task keeps track roles variables play machine\nlearning problem, data, metadata (e.g., observational-level\nweights, IDs, offset).Also, missing outcomes, need set drop_missing_outcome = TRUE create task. next analysis, IST stroke trial\ndata, missing outcome. following chapter, need \nestimate missingness mechanism; conditional probably \noutcome observed, given history (.e., variables measured\nmissingness). Estimating missingness mechanism requires learning\nprediction function outputs predicted probability unit\nmissing, given history.warning important. task just imputed missing covariates us.\nSpecifically, covariate column missing values, sl3 uses \nmedian impute missing continuous covariates, mode impute binary\ncategorical covariates.Also, covariate column missing values, sl3 adds additional\ncolumn indicating whether value imputed, particularly\nhandy missingness data might informative.Also, notice specify number folds, loss function\ntask. default cross-validation scheme V-fold, \\(V=10\\) number\nfolds.Let’s visualize washb_task:can’t see print task, default cross-validation fold\nstructure (\\(V\\)-fold cross-validation \\(V\\)=10 folds) created \ndefined task.Tip: type washb_task$ press tab button (\nneed press tab twice ’re RStudio), can view \nactive public fields methods can accessed washb_task\nobject.","code":"# specify the outcome and covariates\noutcome <- \"whz\"\ncovars <- colnames(washb_data)[-which(names(washb_data) == outcome)]\n\n# create the sl3 task\nwashb_task <- make_sl3_Task(\n  data = washb_data,\n  covariates = covars,\n  outcome = outcome\n)\nWarning in process_data(data, nodes, column_names = column_names, flag = flag, :\nMissing covariate data detected: imputing covariates.washb_task\nA sl3 Task with 4695 obs and these nodes:\n$covariates\n [1] \"tr\"              \"fracode\"         \"month\"           \"aged\"           \n [5] \"sex\"             \"momage\"          \"momedu\"          \"momheight\"      \n [9] \"hfiacat\"         \"Nlt18\"           \"Ncomp\"           \"watmin\"         \n[13] \"elec\"            \"floor\"           \"walls\"           \"roof\"           \n[17] \"asset_wardrobe\"  \"asset_table\"     \"asset_chair\"     \"asset_khat\"     \n[21] \"asset_chouki\"    \"asset_tv\"        \"asset_refrig\"    \"asset_bike\"     \n[25] \"asset_moto\"      \"asset_sewmach\"   \"asset_mobile\"    \"delta_momage\"   \n[29] \"delta_momheight\"\n\n$outcome\n[1] \"whz\"\n\n$id\nNULL\n\n$weights\nNULL\n\n$offset\nNULL\n\n$time\nNULLlength(washb_task$folds) # how many folds?\n[1] 10\n\nhead(washb_task$folds[[1]]$training_set) # row indexes for fold 1 training\n[1] 1 2 3 4 5 6\nhead(washb_task$folds[[1]]$validation_set) # row indexes for fold 1 validation\n[1] 12 21 29 41 43 53\n\nany(\n  washb_task$folds[[1]]$training_set %in% washb_task$folds[[1]]$validation_set\n)\n[1] FALSE"},{"path":"sl3.html","id":"make-a-super-learner","chapter":"6 Super (Machine) Learning","heading":"2. Make a Super Learner","text":"Now defined machine learning problem sl3_Task, \nready make Super Learner (SL). requires specification ofA set candidate machine learning algorithms, also commonly referred \nlibrary learners. set include diversity algorithms\nbelieved consistent true data-generating distribution.metalearner, ensemble base learners.might also incorporateFeature selection, pass subset predictors algorithm.Hyperparameter specification, tune base learners.Learners properties indicate features support. may use\nsl3_list_properties() get list properties supported least\none learner.Since continuous outcome, may identify learners support\noutcome type sl3_list_learners().Now idea learners, can construct using \nmake_learner function new method.can customize learner hyperparameters incorporate diversity different\nsettings. Documentation learners hyperparameters can found\nsl3 Learners\nReference.can use Lrnr_define_interactions define interaction terms among\ncovariates. interactions supplied list character vectors,\nvector specifies interaction. example, specify\ninteractions (1) tr (whether subject received \nWASH intervention) elec (whether subject electricity); \n(2) tr hfiacat (subject’s level food security).just defined incomplete. order fit learners \ninteractions, need create Pipeline. Pipeline set learners\nfit sequentially, fit one learner used define \ntask next learner. need create Pipeline interaction\ndefining learner another learner incorporate terms fitting\nmodel. Let’s create learner pipeline fit linear model \ncombination main terms interactions terms, specified \nlrn_interaction.can also include learners SuperLearner R package.fun trick create customized learners grid parameters.see Lrnr_caret called sl3_list_learners(c(\"binomial\"))? \nneed specify use popular algorithm candidate SL \nalgorithm want tune, passed method caret::train().order assemble library learners, need Stack \ntogether.Stack special learner interface \nlearners. makes stack special combines multiple learners \ntraining simultaneously, predictions can either combined\ncompared.can also stack learners first creating vector, \ninstantiating stack. prefer method, since easily allows us \nmodify names learners.’re jumping ahead bit, let’s check something quickly. ’s\nstraightforward, just one step, set stack \nlearners train cross-validated manner.","code":"sl3_list_properties()\n [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n[13] \"timeseries\"    \"weights\"       \"wrapper\"      sl3_list_learners(\"continuous\")\n [1] \"Lrnr_arima\"                     \"Lrnr_bartMachine\"              \n [3] \"Lrnr_bayesglm\"                  \"Lrnr_bilstm\"                   \n [5] \"Lrnr_bound\"                     \"Lrnr_caret\"                    \n [7] \"Lrnr_cv_selector\"               \"Lrnr_dbarts\"                   \n [9] \"Lrnr_earth\"                     \"Lrnr_expSmooth\"                \n[11] \"Lrnr_gam\"                       \"Lrnr_gbm\"                      \n[13] \"Lrnr_glm\"                       \"Lrnr_glm_fast\"                 \n[15] \"Lrnr_glmnet\"                    \"Lrnr_grf\"                      \n[17] \"Lrnr_gru_keras\"                 \"Lrnr_gts\"                      \n[19] \"Lrnr_h2o_glm\"                   \"Lrnr_h2o_grid\"                 \n[21] \"Lrnr_hal9001\"                   \"Lrnr_HarmonicReg\"              \n[23] \"Lrnr_hts\"                       \"Lrnr_lightgbm\"                 \n[25] \"Lrnr_lstm_keras\"                \"Lrnr_mean\"                     \n[27] \"Lrnr_multiple_ts\"               \"Lrnr_nnet\"                     \n[29] \"Lrnr_nnls\"                      \"Lrnr_optim\"                    \n[31] \"Lrnr_pkg_SuperLearner\"          \"Lrnr_pkg_SuperLearner_method\"  \n[33] \"Lrnr_pkg_SuperLearner_screener\" \"Lrnr_polspline\"                \n[35] \"Lrnr_randomForest\"              \"Lrnr_ranger\"                   \n[37] \"Lrnr_rpart\"                     \"Lrnr_rugarch\"                  \n[39] \"Lrnr_screener_correlation\"      \"Lrnr_solnp\"                    \n[41] \"Lrnr_stratified\"                \"Lrnr_svm\"                      \n[43] \"Lrnr_tsDyn\"                     \"Lrnr_xgboost\"                  \n# choose base learners\nlrn_glm <- make_learner(Lrnr_glm)\nlrn_mean <- Lrnr_mean$new()\nlrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_enet.5 <- make_learner(Lrnr_glmnet, alpha = 0.5)\n\nlrn_polspline <- Lrnr_polspline$new()\n\nlrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)\n\nlrn_hal_faster <- Lrnr_hal9001$new(max_degree = 2, reduce_basis = 0.05)\n\nxgb_fast <- Lrnr_xgboost$new() # default with nrounds = 20 is pretty fast\nxgb_50 <- Lrnr_xgboost$new(nrounds = 50)\ninteractions <- list(c(\"elec\", \"tr\"), c(\"tr\", \"hfiacat\"))\n# main terms as well as the interactions above will be included\nlrn_interaction <- make_learner(Lrnr_define_interactions, interactions)# we already instantiated a linear model learner, no need to do that again\nlrn_glm_interaction <- make_learner(Pipeline, lrn_interaction, lrn_glm)\nlrn_glm_interaction\n[1] \"Lrnr_define_interactions_TRUE\"\n[1] \"Lrnr_glm_TRUE\"\nlrn_bayesglm <- Lrnr_pkg_SuperLearner$new(\"SL.bayesglm\")\n# I like to crock pot my SLs\ngrid_params <- list(\n  cost = c(0.01, 0.1, 1, 10, 100, 1000),\n  gamma = c(0.001, 0.01, 0.1, 1),\n  kernel = c(\"polynomial\", \"radial\", \"sigmoid\"),\n  degree = c(1, 2, 3)\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nsvm_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_svm$new, as.list(tuning_params))\n})grid_params <- list(\n  max_depth = c(2, 4, 6),\n  eta = c(0.001, 0.1, 0.3),\n  nrounds = 100\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\ngrid\n  max_depth   eta nrounds\n1         2 0.001     100\n2         4 0.001     100\n3         6 0.001     100\n4         2 0.100     100\n5         4 0.100     100\n6         6 0.100     100\n7         2 0.300     100\n8         4 0.300     100\n9         6 0.300     100\n\nxgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_xgboost$new, as.list(tuning_params))\n})\nxgb_learners\n[[1]]\n[1] \"Lrnr_xgboost_100_1_2_0.001\"\n\n[[2]]\n[1] \"Lrnr_xgboost_100_1_4_0.001\"\n\n[[3]]\n[1] \"Lrnr_xgboost_100_1_6_0.001\"\n\n[[4]]\n[1] \"Lrnr_xgboost_100_1_2_0.1\"\n\n[[5]]\n[1] \"Lrnr_xgboost_100_1_4_0.1\"\n\n[[6]]\n[1] \"Lrnr_xgboost_100_1_6_0.1\"\n\n[[7]]\n[1] \"Lrnr_xgboost_100_1_2_0.3\"\n\n[[8]]\n[1] \"Lrnr_xgboost_100_1_4_0.3\"\n\n[[9]]\n[1] \"Lrnr_xgboost_100_1_6_0.3\"\n# Unlike xgboost, I have no idea how to tune a neural net or BART machine, so\n# I let caret take the reins\nlrnr_caret_nnet <- make_learner(Lrnr_caret, algorithm = \"nnet\")\nlrnr_caret_bartMachine <- make_learner(Lrnr_caret,\n  algorithm = \"bartMachine\",\n  method = \"boot\", metric = \"Accuracy\",\n  tuneLength = 10\n)stack <- make_learner(\n  Stack, lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50\n)\nstack\n[1] \"Lrnr_glm_TRUE\"                                  \n[2] \"Lrnr_polspline_5\"                               \n[3] \"Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE\"\n[4] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE\"  \n[5] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE\"  \n[6] \"Lrnr_xgboost_50_1\"                              # named vector of learners first\nlearners <- c(\n  lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50\n)\nnames(learners) <- c(\n  \"glm\", \"polspline\", \"enet.5\", \"ridge\", \"lasso\", \"xgboost50\"\n)\n# next make the stack\nstack <- make_learner(Stack, learners)\n# now the names are pretty\nstack\n[1] \"glm\"       \"polspline\" \"enet.5\"    \"ridge\"     \"lasso\"     \"xgboost50\"cv_stack <- Lrnr_cv$new(stack)\ncv_stack\n[1] \"Lrnr_cv\"\n[1] \"glm\"       \"polspline\" \"enet.5\"    \"ridge\"     \"lasso\"     \"xgboost50\""},{"path":"sl3.html","id":"screening-algorithms-for-feature-selection","chapter":"6 Super (Machine) Learning","heading":"Screening Algorithms for Feature Selection","text":"can optionally select subset available covariates pass \nvariables modeling algorithm. current set learners \ncan used prescreening covariates included .Lrnr_screener_importance selects num_screen (default = 5) covariates\nbased variable importance ranking provided learner. \nlearner importance method can used Lrnr_screener_importance;\ncurrently includes Lrnr_ranger, Lrnr_randomForest, \nLrnr_xgboost.Lrnr_screener_coefs, provides screening covariates based \nmagnitude estimated coefficients (possibly regularized) GLM.\nthreshold (default = 1e-3) defines minimum absolute size \ncoefficients, thus covariates, kept. Also, max_retain argument\ncan optionally provided restrict number selected covariates \nmax_retain.Lrnr_screener_correlation provides covariate screening procedures \nrunning test correlation (Pearson default), selecting (1)\ntop ranked variables (default), (2) variables pvalue lower \npre-specified threshold.Lrnr_screener_augment augments set screened covariates additional\ncovariates included default, even screener \nselect . example use screener included .Let’s consider screening covariates based randomForest variable\nimportance ranking (ordered mean decrease accuracy). select top\n5 important covariates according ranking, can combine\nLrnr_screener_importance Lrnr_ranger (limiting number trees \nsetting ntree = 20).Hang ! think – confess: Bob Ross us know \n20 trees makes lonely forest, shouldn’t consider , \nsacrifices make chapter built time!example format Lrnr_screener_augment included \nclarity.Selecting covariates non-zero lasso coefficients quite common. Let’s\nconstruct Lrnr_screener_coefs screener just , test \n.pipe selected covariates modeling algorithm, need \nmake Pipeline, similar one built regression model \ninteraction terms.Now, learners preceded screening step.also consider original stack, compare feature selection\nmethods perform comparison methods without feature selection.Analogous seen , stack pipeline \noriginal stack together, may use base learners super\nlearner.use default\nmetalearner,\nuses\nLrnr_solnp \nprovide fitting procedures pairing loss\nfunction \nmetalearner\nfunction. \ndefault metalearner selects loss metalearner pairing based outcome\ntype. Note learner can used metalearner.Now made diverse stack base learners, ready make \nSL. SL algorithm fits metalearner validation set\npredictions/losses across folds.can also use Lrnr_cv build SL, cross-validate stack \nlearners compare performance learners stack, cross-validate\nsingle learner (see “Cross-validation” section sl3\nintroductory tutorial).Furthermore, can Define New sl3\nLearners can used\nplaces otherwise use sl3 learners, including\nPipelines, Stacks, SL.Recall discrete SL, cross-validated selector, metalearner \nassigns weight 1 learner lowest cross-validated empirical\nrisk, weight 0 learners. metalearner specification can\ninvoked Lrnr_cv_selector.","code":"miniforest <- Lrnr_ranger$new(\n  num.trees = 20, write.forest = FALSE,\n  importance = \"impurity_corrected\"\n)\n\n# learner must already be instantiated, we did this when we created miniforest\nscreen_rf <- Lrnr_screener_importance$new(learner = miniforest, num_screen = 5)\nscreen_rf\n[1] \"Lrnr_screener_importance_5\"\n\n# which covariates are selected on the full data?\nscreen_rf$train(washb_task)\n[1] \"Lrnr_screener_importance_5\"\n$selected\n[1] \"aged\"      \"month\"     \"tr\"        \"momheight\" \"momedu\"   keepme <- c(\"aged\", \"momage\")\n# screener must already be instantiated, we did this when we created screen_rf\nscreen_augment_rf <- Lrnr_screener_augment$new(\n  screener = screen_rf, default_covariates = keepme\n)\nscreen_augment_rf\n[1] \"Lrnr_screener_augment_c(\\\"aged\\\", \\\"momage\\\")\"# we already instantiated a lasso learner above, no need to do it again\nscreen_lasso <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)\nscreen_lasso\n[1] \"Lrnr_screener_coefs_0_NULL_2\"\nscreen_rf_pipe <- make_learner(Pipeline, screen_rf, stack)\nscreen_lasso_pipe <- make_learner(Pipeline, screen_lasso, stack)# pretty names again\nlearners2 <- c(learners, screen_rf_pipe, screen_lasso_pipe)\nnames(learners2) <- c(names(learners), \"randomforest_screen\", \"lasso_screen\")\n\nfancy_stack <- make_learner(Stack, learners2)\nfancy_stack\n[1] \"glm\"                 \"polspline\"           \"enet.5\"             \n[4] \"ridge\"               \"lasso\"               \"xgboost50\"          \n[7] \"randomforest_screen\" \"lasso_screen\"       \nsl <- make_learner(Lrnr_sl, learners = fancy_stack)\ndiscrete_sl_metalrn <- Lrnr_cv_selector$new()\ndiscrete_sl <- Lrnr_sl$new(\n  learners = fancy_stack,\n  metalearner = discrete_sl_metalrn\n)"},{"path":"sl3.html","id":"train-the-super-learner-on-the-machine-learning-task","chapter":"6 Super (Machine) Learning","heading":"3. Train the Super Learner on the machine learning task","text":"SL algorithm fits metalearner validation-set predictions \ncross-validated manner, thereby avoiding overfitting.Now ready train SL sl3_task object, washb_task.","code":"\nset.seed(4197)\nsl_fit <- sl$train(washb_task)"},{"path":"sl3.html","id":"obtain-predicted-values","chapter":"6 Super (Machine) Learning","heading":"4. Obtain predicted values","text":"Now fit SL, ready calculate predicted outcome\nsubject.can also obtain summary results.","code":"# we did it! now we have SL predictions\nsl_preds <- sl_fit$predict()\nhead(sl_preds)\n[1] -0.65442 -0.77055 -0.67359 -0.65109 -0.65577 -0.65673sl_fit$cv_risk(loss_fun = loss_squared_error)\n                          learner coefficients   risk       se  fold_sd\n 1:                           glm     0.055571 1.0202 0.023955 0.067500\n 2:                     polspline     0.055556 1.0208 0.023577 0.067921\n 3:                        enet.5     0.055564 1.0131 0.023598 0.065732\n 4:                         ridge     0.055570 1.0153 0.023739 0.065299\n 5:                         lasso     0.055564 1.0130 0.023592 0.065840\n 6:                     xgboost50     0.055591 1.1136 0.025262 0.077580\n 7:       randomforest_screen_glm     0.055546 1.0271 0.024119 0.069913\n 8: randomforest_screen_polspline     0.055561 1.0236 0.024174 0.068710\n 9:    randomforest_screen_enet.5     0.055546 1.0266 0.024101 0.070117\n10:     randomforest_screen_ridge     0.055546 1.0268 0.024120 0.069784\n11:     randomforest_screen_lasso     0.055546 1.0266 0.024101 0.070135\n12: randomforest_screen_xgboost50     0.055523 1.1399 0.026341 0.100112\n13:              lasso_screen_glm     0.055559 1.0164 0.023542 0.065018\n14:        lasso_screen_polspline     0.055559 1.0177 0.023520 0.065566\n15:           lasso_screen_enet.5     0.055559 1.0163 0.023544 0.065017\n16:            lasso_screen_ridge     0.055559 1.0166 0.023553 0.064869\n17:            lasso_screen_lasso     0.055559 1.0163 0.023544 0.065020\n18:        lasso_screen_xgboost50     0.055521 1.1256 0.025939 0.084270\n19:                  SuperLearner           NA 1.0135 0.023615 0.067434\n    fold_min_risk fold_max_risk\n 1:       0.89442        1.1200\n 2:       0.89892        1.1255\n 3:       0.88839        1.1058\n 4:       0.88559        1.1063\n 5:       0.88842        1.1060\n 6:       0.96019        1.2337\n 7:       0.90251        1.1326\n 8:       0.90167        1.1412\n 9:       0.90030        1.1319\n10:       0.90068        1.1311\n11:       0.90043        1.1321\n12:       0.92377        1.2549\n13:       0.90204        1.1156\n14:       0.89742        1.1162\n15:       0.90184        1.1154\n16:       0.90120        1.1146\n17:       0.90183        1.1154\n18:       0.96251        1.2327\n19:       0.88685        1.1102"},{"path":"sl3.html","id":"cross-validated-super-learner","chapter":"6 Super (Machine) Learning","heading":"Cross-validated Super Learner","text":"can cross-validate SL see well SL performs unseen data, \nobtain estimate cross-validated risk SL.estimation procedure requires outer/external layer \ncross-validation, also called nested cross-validation, involves setting\naside separate holdout sample don’t use fit SL. external\ncross-validation procedure may also incorporate 10 folds, default\nsl3. However, incorporate 2 outer/external folds \ncross-validation computational efficiency.also need specify loss function evaluate SL. Documentation \navailable loss functions can found sl3 Loss Function\nReference.","code":"\nwashb_task_new <- make_sl3_Task(\n  data = washb_data,\n  covariates = covars,\n  outcome = outcome,\n  folds = origami::make_folds(washb_data, fold_fun = folds_vfold, V = 2)\n)\nCVsl <- CV_lrnr_sl(\n  lrnr_sl = sl_fit, task = washb_task_new, loss_fun = loss_squared_error\n)\nCVsl %>%\n  kable(digits = 4) %>%\n  kableExtra:::kable_styling(fixed_thead = T) %>%\n  scroll_box(width = \"100%\", height = \"300px\")"},{"path":"sl3.html","id":"variable-importance-measures-with-sl3","chapter":"6 Super (Machine) Learning","heading":"Variable Importance Measures with sl3","text":"Variable importance can interesting informative. can also \ncontradictory confusing. Nevertheless, like , \ncollaborators, created variable importance function sl3! sl3\nimportance function returns table variables listed decreasing order\nimportance (.e., important first row).measure importance sl3 based risk ratio, risk difference,\nlearner fit removed, permuted, covariate learner\nfit true covariate, across covariates. manner, larger\nrisk difference, important variable prediction.intuition measure calculates risk (terms \naverage loss predictive accuracy) losing one covariate, keeping\neverything else fixed, compares risk covariate \nlost. risk ratio one, risk difference zero, losing \ncovariate impact, thus important measure. \nacross covariates. stated , can remove covariate \nrefit SL without , just permute covariate (faster)\nhope shuffling distort meaningful information \npresent covariate. idea permuting instead removing saves lot\ntime, also incorporated randomForest variable importance\nmeasures. However, permutation approach risky, importance function\ndefault remove refit.Let’s explore sl3 variable importance measurements washb data.","code":"\nwashb_varimp <- importance(sl_fit, loss = loss_squared_error, type = \"permute\")\nwashb_varimp %>%\n  kable(digits = 4) %>%\n  kableExtra:::kable_styling(fixed_thead = TRUE) %>%\n  scroll_box(width = \"100%\", height = \"300px\")\n# plot variable importance\nimportance_plot(\n  washb_varimp,\n  main = \"sl3 Variable Importance for WASH Benefits Example Data\"\n)"},{"path":"sl3.html","id":"sl3-exercises","chapter":"6 Super (Machine) Learning","heading":"6.2 Exercises","text":"","code":""},{"path":"sl3.html","id":"sl3ex1","chapter":"6 Super (Machine) Learning","heading":"6.2.1 Predicting Myocardial Infarction with sl3","text":"Follow steps predict myocardial infarction (mi) using \navailable covariate data. thank Prof. David Benkeser Emory University \nmaking Cardiovascular Health Study (CHS) data accessible.Let’s take quick peek data:Create sl3 task, setting myocardial infarction mi outcome \nusing available covariate data.Make library seven relatively fast base learning algorithms (.e., \nconsider BART HAL). Customize hyperparameters one \nlearners. Feel free use learners sl3 SuperLearner. may\nuse base learning library presented .Incorporate least one pipeline feature selection. screener \nlearner(s) can used.Fit metalearning step default metalearner.metalearner base learners, make Super Learner (SL) \ntrain task.Print SL fit results adding $cv_risk(loss_squared_error) \nfit object. squared error loss specified , since ’s \nused default metalearner.Cross-validate SL fit see well performs unseen data.\nSpecify loss_squared_error loss function evaluate SL ’s\nloss used default metalearner. Print result.Use importance() function identify “important” predictor \nmyocardial infarction, according sl3 importance metrics. Print \nresult.","code":"\n# load the data set\ndb_data <- url(\n  paste0(\n    \"https://raw.githubusercontent.com/benkeser/sllecture/master/\",\n    \"chspred.csv\"\n  )\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)"},{"path":"sl3.html","id":"concluding-remarks","chapter":"6 Super (Machine) Learning","heading":"6.3 Concluding Remarks","text":"Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.straightforward plug estimator returned SL \ntarget parameter mapping.\nexample, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).\nuse SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.\nConsidering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).\nobtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.\nsl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.\nstraightforward plug estimator returned SL \ntarget parameter mapping.example, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).use SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.Considering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).obtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.sl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!end “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?\nasymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].\nSubstitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\n\nend “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].efficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].canonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.Practitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.","code":""},{"path":"sl3.html","id":"appendix","chapter":"6 Super (Machine) Learning","heading":"6.4 Appendix","text":"","code":""},{"path":"sl3.html","id":"sl3ex1-sol","chapter":"6 Super (Machine) Learning","heading":"6.4.1 Exercise 1 Solution","text":"potential solution sl3 Exercise 1 – Predicting Myocardial\nInfarction sl3.","code":"\ndb_data <- url(\n  \"https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv\"\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\n\n# make task\nchspred_task <- make_sl3_Task(\n  data = chspred,\n  covariates = head(colnames(chspred), -1),\n  outcome = \"mi\"\n)\n\n# make learners\nglm_learner <- Lrnr_glm$new()\nlasso_learner <- Lrnr_glmnet$new(alpha = 1)\nridge_learner <- Lrnr_glmnet$new(alpha = 0)\nenet_learner <- Lrnr_glmnet$new(alpha = 0.5)\n# curated_glm_learner uses formula = \"mi ~ smoke + beta + waist\"\ncurated_glm_learner <- Lrnr_glm_fast$new(covariates = c(\"smoke, beta, waist\"))\nmean_learner <- Lrnr_mean$new() # That is one mean learner!\nglm_fast_learner <- Lrnr_glm_fast$new()\nranger_learner <- Lrnr_ranger$new()\nsvm_learner <- Lrnr_svm$new()\nxgb_learner <- Lrnr_xgboost$new()\n\n# screening\nscreen_cor <- make_learner(Lrnr_screener_correlation)\nglm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)\n\n# stack learners together\nstack <- make_learner(\n  Stack,\n  glm_pipeline, glm_learner,\n  lasso_learner, ridge_learner, enet_learner,\n  curated_glm_learner, mean_learner, glm_fast_learner,\n  ranger_learner, svm_learner, xgb_learner\n)\n\n# make and train SL\nsl <- Lrnr_sl$new(\n  learners = stack\n)\nsl_fit <- sl$train(chspred_task)\nsl_fit$cv_risk(loss_squared_error)\n\nCVsl <- CV_lrnr_sl(sl_fit, chspred_task, loss_squared_error)\nCVsl\n\nvarimp <- importance(sl_fit)\nimportance_plot(varimp) "},{"path":"sl3.html","id":"sl3ex2-sol","chapter":"6 Super (Machine) Learning","heading":"6.4.2 Exercise 2 Solution","text":"potential solution sl3 Exercise 2 – Predicting Recurrent\nIschemic Stroke RCT sl3.","code":"\nlibrary(ROCR) # for AUC calculation\n\nist_data <- paste0(\n  \"https://raw.githubusercontent.com/tlverse/\",\n  \"tlverse-handbook/master/data/ist_sample.csv\"\n) %>% fread()\n\n# stack\nist_task <- make_sl3_Task(\n  data = ist_data,\n  outcome = \"DRSISC\",\n  covariates = colnames(ist_data)[-which(names(ist_data) == \"DRSISC\")],\n  drop_missing_outcome = TRUE\n)\n\n# learner library\nlrn_glm <- Lrnr_glm$new()\nlrn_lasso <- Lrnr_glmnet$new(alpha = 1)\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_enet <- Lrnr_glmnet$new(alpha = 0.5)\nlrn_mean <- Lrnr_mean$new()\nlrn_ranger <- Lrnr_ranger$new()\nlrn_svm <- Lrnr_svm$new()\n# xgboost grid\ngrid_params <- list(\n  max_depth = c(2, 5, 8),\n  eta = c(0.01, 0.15, 0.3)\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nparams_default <- list(nthread = getOption(\"sl.cores.learners\", 1))\nxgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {\n  do.call(Lrnr_xgboost$new, c(params_default, as.list(params_tune)))\n})\nlearners <- unlist(list(\n  xgb_learners, lrn_ridge, lrn_mean, lrn_lasso,\n  lrn_glm, lrn_enet, lrn_ranger, lrn_svm\n),\nrecursive = TRUE\n)\n\n# SL\nsl <- Lrnr_sl$new(learners)\nsl_fit <- sl$train(ist_task)\n\n# AUC\npreds <- sl_fit$predict()\nobs <- c(na.omit(ist_data$DRSISC))\nAUC <- performance(prediction(sl_preds, obs), measure = \"auc\")@y.values[[1]]\nplot(performance(prediction(sl_preds, obs), \"tpr\", \"fpr\"))\n\n# CVsl\nist_task_CVsl <- make_sl3_Task(\n  data = ist_data,\n  outcome = \"DRSISC\",\n  covariates = colnames(ist_data)[-which(names(ist_data) == \"DRSISC\")],\n  drop_missing_outcome = TRUE,\n  folds = origami::make_folds(\n    n = sum(!is.na(ist_data$DRSISC)),\n    fold_fun = folds_vfold,\n    V = 5\n  )\n)\nCVsl <- CV_lrnr_sl(sl_fit, ist_task_CVsl, loss_loglik_binomial)\nCVsl\n\n# sl3 variable importance plot\nist_varimp <- importance(sl_fit, type = \"permute\")\nist_varimp %>%\n  importance_plot(\n    main = \"Variable Importance for Predicting Recurrent Ischemic Stroke\"\n  )"},{"path":"tmle3.html","id":"tmle3","chapter":"7 The TMLE Framework","heading":"7 The TMLE Framework","text":"Jeremy CoyleBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learn-tmle","chapter":"7 The TMLE Framework","heading":"7.1 Learning Objectives","text":"end chapter, able toUnderstand use TMLE effect estimation.Use tmle3 estimate Average Treatment Effect (ATE).Understand use tmle3 “Specs” objects.Fit tmle3 custom set target parameters.Use delta method estimate transformations target parameters.","code":""},{"path":"tmle3.html","id":"tmle-intro","chapter":"7 The TMLE Framework","heading":"7.2 Introduction","text":"previous chapter sl3 learned estimate regression\nfunction like \\(\\mathbb{E}[Y \\mid X]\\) data. ’s important first step\nlearning data, can use predictive model estimate\nstatistical causal effects?Going back roadmap targeted learning, suppose ’d like \nestimate effect treatment variable \\(\\) outcome \\(Y\\). discussed,\none potential parameter characterizes effect Average Treatment\nEffect (ATE), defined \\(\\psi_0 = \\mathbb{E}_W[\\mathbb{E}[Y \\mid =1,W] - \\mathbb{E}[Y \\mid =0,W]]\\) interpreted difference mean outcome\ntreatment \\(=1\\) \\(=0\\), averaging distribution \ncovariates \\(W\\). ’ll illustrate several potential estimators \nparameter, motivate use TMLE (targeted maximum likelihood\nestimation; targeted minimum loss-based estimation) framework, using \nfollowing example data:small ticks right indicate mean outcomes (averaging \\(W\\))\n\\(=1\\) \\(=0\\) respectively, difference quantity ’d\nlike estimate.hope motivate application TMLE chapter, refer \ninterested reader two Targeted Learning books associated works \nfull technical details.","code":""},{"path":"tmle3.html","id":"substitution-est","chapter":"7 The TMLE Framework","heading":"7.3 Substitution Estimators","text":"can use sl3 fit Super Learner regression model estimate\noutcome regression function \\(\\mathbb{E}_0[Y \\mid ,W]\\), often refer\n\\(\\overline{Q}_0(,W)\\) whose estimate denote \\(\\overline{Q}_n(,W)\\).\nconstruct estimate ATE \\(\\psi_n\\), need “plug-” \nestimates \\(\\overline{Q}_n(,W)\\), evaluated two intervention contrasts,\ncorresponding ATE “plug-” formula:\n\\(\\psi_n = \\frac{1}{n}\\sum(\\overline{Q}_n(1,W)-\\overline{Q}_n(0,W))\\). kind\nestimator called plug-substitution estimator, since accurate\nestimates \\(\\psi_n\\) parameter \\(\\psi_0\\) may obtained substituting\nestimates \\(\\overline{Q}_n(,W)\\) relevant regression functions\n\\(\\overline{Q}_0(,W)\\) .Applying sl3 estimate outcome regression example, can see\nensemble machine learning predictions fit data quite well:solid lines indicate sl3 estimate regression function, \ndotted lines indicating tmle3 updates (described ).substitution estimators intuitive, naively using approach \nSuper Learner estimate \\(\\overline{Q}_0(,W)\\) several limitations. First,\nSuper Learner selecting learner weights minimize risk across entire\nregression function, instead “targeting” ATE parameter hope \nestimate, leading biased estimation. , sl3 trying well \nfull regression curve left, instead focusing small ticks \nright. ’s , sampling distribution approach \nasymptotically linear, therefore inference possible.can see limitations illustrated estimates generated \nexample data:see Super Learner, estimates true parameter value (indicated \ndashed vertical line) accurately GLM. However, still less\naccurate TMLE, valid inference possible. contrast, TMLE\nachieves less biased estimator valid inference.","code":""},{"path":"tmle3.html","id":"tmle","chapter":"7 The TMLE Framework","heading":"7.4 Targeted Maximum Likelihood Estimation","text":"TMLE takes initial estimate \\(\\overline{Q}_n(,W)\\) well estimate \npropensity score \\(g_n(\\mid W) = \\mathbb{P}(= 1 \\mid W)\\) produces \nupdated estimate \\(\\overline{Q}^{\\star}_n(,W)\\) “targeted” \nparameter interest. TMLE keeps benefits substitution estimators (\none), augments original, potentially erratic estimates correct \nbias also resulting asymptotically linear (thus normally\ndistributed) estimator accommodates inference via asymptotically consistent\nWald-style confidence intervals.","code":""},{"path":"tmle3.html","id":"tmle-updates","chapter":"7 The TMLE Framework","heading":"7.4.1 TMLE Updates","text":"different types TMLEs (, sometimes, multiple set \ntarget parameters) – , give example algorithm TML\nestimation ATE. \\(\\overline{Q}^{\\star}_n(,W)\\) TMLE-augmented\nestimate \\(f(\\overline{Q}^{\\star}_n(,W)) = f(\\overline{Q}_n(,W)) + \\epsilon \\cdot H_n(,W)\\), \\(f(\\cdot)\\) appropriate link function (e.g.,\n\\(\\text{logit}(x) = \\log(x / (1 - x))\\)), estimate \\(\\epsilon_n\\) \ncoefficient \\(\\epsilon\\) “clever covariate” \\(H_n(,W)\\) computed. \nform covariate \\(H_n(,W)\\) differs across target parameters; case\nATE, \\(H_n(,W) = \\frac{}{g_n(\\mid W)} - \\frac{1-}{1-g_n(, W)}\\), \\(g_n(,W) = \\mathbb{P}(=1 \\mid W)\\) estimated propensity\nscore, estimator depends initial fit (sl3) \noutcome regression (\\(\\overline{Q}_n\\)) propensity score (\\(g_n\\)).several robust augmentations used across tlverse,\nincluding use additional layer cross-validation avoid\n-fitting bias (.e., CV-TMLE) well approaches consistently\nestimating several parameters simultaneously (e.g., points survival\ncurve).","code":""},{"path":"tmle3.html","id":"tmle-infer","chapter":"7 The TMLE Framework","heading":"7.4.2 Statistical Inference","text":"Since TMLE yields asymptotically linear estimator, obtaining statistical\ninference convenient. TML estimator corresponding\n(efficient) influence function (often, “EIF”, short) describes \nasymptotic distribution estimator. using estimated EIF, Wald-style\ninference (asymptotically correct confidence intervals) can constructed\nsimply plugging form EIF initial estimates\n\\(\\overline{Q}_n\\) \\(g_n\\), computing sample standard error.following sections describe simple detailed way \nspecifying estimating TMLE tlverse. designing tmle3, \nsought replicate closely possible general estimation framework\nTMLE, theoretical object relevant TMLE encoded \ncorresponding software object/method. First, present simple\napplication tmle3 WASH Benefits example, go describe\nunderlying objects greater detail.","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"7 The TMLE Framework","heading":"7.5 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating average treatment effect.","code":""},{"path":"tmle3.html","id":"load-the-data","chapter":"7 The TMLE Framework","heading":"7.5.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tmle3)\nlibrary(sl3)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"7 The TMLE Framework","heading":"7.5.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\)\n(outcome) data structure. tmle3 needs know variables dataset\ncorrespond roles. use list character vectors tell\n. call “Node List” corresponds nodes Directed\nAcyclic Graph (DAG), way displaying causal relationships variables.","code":"\nnode_list <- list(\n  W = c(\n    \"month\", \"aged\", \"sex\", \"momage\", \"momedu\",\n    \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\", \"asset_wardrobe\",\n    \"asset_table\", \"asset_chair\", \"asset_khat\",\n    \"asset_chouki\", \"asset_tv\", \"asset_refrig\",\n    \"asset_bike\", \"asset_moto\", \"asset_sewmach\",\n    \"asset_mobile\"\n  ),\n  A = \"tr\",\n  Y = \"whz\"\n)"},{"path":"tmle3.html","id":"handle-missingness","chapter":"7 The TMLE Framework","heading":"7.5.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median- (continuous) mode- (discrete)\nimputed, additional covariates indicating imputation generated, just\ndescribed sl3 chapter.Missing treatment variables excluded – observations dropped.Missing outcomes efficiently handled automatic calculation (\nincorporation estimators) inverse probability censoring weights\n(IPCW); also known IPCW-TMLE may thought joint\nintervention remove missingness analogous procedure used \nclassical inverse probability weighted estimators.steps implemented process_missing function tmle3:","code":"\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list"},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"7 The TMLE Framework","heading":"7.5.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set components specification\n(“Spec”) , minimal additional detail, can run fit TMLE.’ll start using one specs, work way \ninternals tmle3.","code":"\nate_spec <- tmle_ATE(\n  treatment_level = \"Nutrition + WSH\",\n  control_level = \"Control\"\n)"},{"path":"tmle3.html","id":"define-the-learners","chapter":"7 The TMLE Framework","heading":"7.5.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":"\n# choose base learners\nlrnr_mean <- make_learner(Lrnr_mean)\nlrnr_rf <- make_learner(Lrnr_ranger)\n\n# define metalearners appropriate to data types\nls_metalearner <- make_learner(Lrnr_nnls)\nmn_metalearner <- make_learner(\n  Lrnr_solnp, metalearner_linear_multinomial,\n  loss_loglik_multinomial\n)\nsl_Y <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = ls_metalearner\n)\nsl_A <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = mn_metalearner\n)\nlearner_list <- list(A = sl_A, Y = sl_Y)"},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"7 The TMLE Framework","heading":"7.5.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)\nprint(tmle_fit)\nA tmle3_Fit that took 1 step(s)\n   type                                    param  init_est tmle_est       se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.005231  0.00812 0.050679\n       lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.091208 0.10745         0.00812         -0.091208           0.10745"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"7 The TMLE Framework","heading":"7.5.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"estimates <- tmle_fit$summary$psi_transformed\nprint(estimates)\n[1] 0.00812"},{"path":"tmle3.html","id":"tmle3-components","chapter":"7 The TMLE Framework","heading":"7.6 tmle3 Components","text":"Now ’ve successfully used spec obtain TML estimate, let’s look\nhood components. spec number functions \ngenerate objects necessary define fit TMLE.","code":""},{"path":"tmle3.html","id":"tmle3_task","chapter":"7 The TMLE Framework","heading":"7.6.1 tmle3_task","text":"First , tmle3_Task, analogous sl3_Task, containing data ’re\nfitting TMLE , well NPSEM generated node_list\ndefined , describing variables relationships.","code":"\ntmle_task <- ate_spec$make_tmle_task(washb_data, node_list)tmle_task$npsem\n$W\ntmle3_Node: W\n    Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight\n    Parents: \n\n$A\ntmle3_Node: A\n    Variables: tr\n    Parents: W\n\n$Y\ntmle3_Node: Y\n    Variables: whz\n    Parents: A, W"},{"path":"tmle3.html","id":"initial-likelihood","chapter":"7 The TMLE Framework","heading":"7.6.2 Initial Likelihood","text":"Next, object representing likelihood, factorized according \nNPSEM described :components likelihood indicate factors estimated: \nmarginal distribution \\(W\\) estimated using NP-MLE, conditional\ndistributions \\(\\) \\(Y\\) estimated using sl3 fits (defined \nlearner_list) .can use tandem tmle_task object obtain likelihood\nestimates observation:","code":"\ninitial_likelihood <- ate_spec$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\nprint(initial_likelihood)\nW: Lf_emp\nA: LF_fit\nY: LF_fitinitial_likelihood$get_likelihoods(tmle_task)\n               W       A        Y\n   1: 0.00021299 0.34925 -0.35834\n   2: 0.00021299 0.36117 -0.93261\n   3: 0.00021299 0.34740 -0.80873\n   4: 0.00021299 0.34248 -0.94020\n   5: 0.00021299 0.34134 -0.57866\n  ---                            \n4691: 0.00021299 0.23375 -0.58997\n4692: 0.00021299 0.23366 -0.22769\n4693: 0.00021299 0.22660 -0.74235\n4694: 0.00021299 0.28944 -0.91796\n4695: 0.00021299 0.19533 -1.03878"},{"path":"tmle3.html","id":"targeted-likelihood-updater","chapter":"7 The TMLE Framework","heading":"7.6.3 Targeted Likelihood (updater)","text":"also need define “Targeted Likelihood” object. special type\nlikelihood able updated using tmle3_Update object. \nobject defines update strategy (e.g., submodel, loss function, CV-TMLE \n).constructing targeted likelihood, can specify different update\noptions. See documentation tmle3_Update details different\noptions. example, can disable CV-TMLE (default tmle3) \nfollows:","code":"\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\ntargeted_likelihood_no_cv <-\n  Targeted_Likelihood$new(initial_likelihood,\n    updater = list(cvtmle = FALSE)\n  )"},{"path":"tmle3.html","id":"parameter-mapping","chapter":"7 The TMLE Framework","heading":"7.6.4 Parameter Mapping","text":"Finally, need define parameters interest. , spec defines \nsingle parameter, ATE. next section, ’ll see add additional\nparameters.","code":"tmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)\nprint(tmle_params)\n[[1]]\nParam_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]"},{"path":"tmle3.html","id":"putting-it-all-together","chapter":"7 The TMLE Framework","heading":"7.6.5 Putting it all together","text":"used spec manually generate components, can now\nmanually fit tmle3:result equivalent fitting using tmle3 function .","code":"tmle_fit_manual <- fit_tmle3(\n  tmle_task, targeted_likelihood, tmle_params,\n  targeted_likelihood$updater\n)\nprint(tmle_fit_manual)\nA tmle3_Fit that took 1 step(s)\n   type                                    param   init_est tmle_est       se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0045451  0.01174 0.050807\n      lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.08784 0.11132         0.01174          -0.08784           0.11132"},{"path":"tmle3.html","id":"fitting-tmle3-with-multiple-parameters","chapter":"7 The TMLE Framework","heading":"7.7 Fitting tmle3 with multiple parameters","text":", fit tmle3 just one parameter. tmle3 also supports fitting\nmultiple parameters simultaneously. illustrate , ’ll use \ntmle_TSM_all spec:spec generates Treatment Specific Mean (TSM) level \nexposure variable. Note must first generate new targeted likelihood,\nold one targeted ATE. However, can recycle initial\nlikelihood fit , saving us super learner step.","code":"tsm_spec <- tmle_TSM_all()\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\nall_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)\nprint(all_tsm_params)\n[[1]]\nParam_TSM: E[Y_{A=Control}]\n\n[[2]]\nParam_TSM: E[Y_{A=Handwashing}]\n\n[[3]]\nParam_TSM: E[Y_{A=Nutrition}]\n\n[[4]]\nParam_TSM: E[Y_{A=Nutrition + WSH}]\n\n[[5]]\nParam_TSM: E[Y_{A=Sanitation}]\n\n[[6]]\nParam_TSM: E[Y_{A=WSH}]\n\n[[7]]\nParam_TSM: E[Y_{A=Water}]"},{"path":"tmle3.html","id":"delta-method","chapter":"7 The TMLE Framework","heading":"7.7.1 Delta Method","text":"can also define parameters based Delta Method Transformations \nparameters. instance, can estimate ATE using delta method two\nTSM parameters:can similarly used estimate derived parameters like Relative\nRisks, Population Attributable Risks","code":"ate_param <- define_param(\n  Param_delta, targeted_likelihood,\n  delta_param_ATE,\n  list(all_tsm_params[[1]], all_tsm_params[[4]])\n)\nprint(ate_param)\nParam_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]"},{"path":"tmle3.html","id":"fit","chapter":"7 The TMLE Framework","heading":"7.7.2 Fit","text":"can now fit TMLE simultaneously TSM parameters, well \ndefined ATE parameter","code":"all_params <- c(all_tsm_params, ate_param)\n\ntmle_fit_multiparam <- fit_tmle3(\n  tmle_task, targeted_likelihood, all_params,\n  targeted_likelihood$updater\n)\n\nprint(tmle_fit_multiparam)\nA tmle3_Fit that took 1 step(s)\n   type                                       param   init_est  tmle_est\n1:  TSM                            E[Y_{A=Control}] -0.5959678 -0.620830\n2:  TSM                        E[Y_{A=Handwashing}] -0.6188184 -0.660230\n3:  TSM                          E[Y_{A=Nutrition}] -0.6111402 -0.606586\n4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.6005128 -0.608949\n5:  TSM                         E[Y_{A=Sanitation}] -0.5857464 -0.578472\n6:  TSM                                E[Y_{A=WSH}] -0.5205610 -0.448252\n7:  TSM                              E[Y_{A=Water}] -0.5657364 -0.537709\n8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.0045451  0.011881\n         se     lower    upper psi_transformed lower_transformed\n1: 0.029901 -0.679435 -0.56223       -0.620830         -0.679435\n2: 0.041719 -0.741998 -0.57846       -0.660230         -0.741998\n3: 0.042047 -0.688996 -0.52418       -0.606586         -0.688996\n4: 0.041285 -0.689867 -0.52803       -0.608949         -0.689867\n5: 0.042396 -0.661566 -0.49538       -0.578472         -0.661566\n6: 0.045506 -0.537442 -0.35906       -0.448252         -0.537442\n7: 0.039253 -0.614644 -0.46077       -0.537709         -0.614644\n8: 0.050801 -0.087688  0.11145        0.011881         -0.087688\n   upper_transformed\n1:          -0.56223\n2:          -0.57846\n3:          -0.52418\n4:          -0.52803\n5:          -0.49538\n6:          -0.35906\n7:          -0.46077\n8:           0.11145"},{"path":"tmle3.html","id":"exercises-1","chapter":"7 The TMLE Framework","heading":"7.8 Exercises","text":"","code":""},{"path":"tmle3.html","id":"tmle3-ex1","chapter":"7 The TMLE Framework","heading":"7.8.1 Estimation of the ATE with tmle3","text":"Follow steps estimate average treatment effect using data \nCollaborative Perinatal Project (CPP), available sl3 package. \nsimplify example, define binary intervention variable, parity01 –\nindicator one children current child \nbinary outcome, haz01 – indicator average height \nage.Define variable roles \\((W,,Y)\\) creating list nodes.\nInclude following baseline covariates \\(W\\): apgar1, apgar5,\ngagebrth, mage, meducyrs, sexn. \\(\\) \\(Y\\) specified\n. missingness data (specifically, missingness \ncolumns specified node list) need taking care .\nprocess_missing function can used accomplish , like \nwashb_data example .Define tmle3_Spec object ATE, tmle_ATE().Using base learning libraries defined , specify sl3 base\nlearners estimation \\(\\overline{Q}_0 = \\mathbb{E}_0(Y \\mid , W)\\) \n\\(g_0 = \\mathbb{P}(= 1 \\mid W)\\).Define metalearner like .Define one super learner estimating \\(\\overline{Q}_0\\) another \nestimating \\(g_0\\). Use metalearner super learners.Create list two super learners defined step call\nobject learner_list. list names (defining super\nlearner estimation \\(g_0\\)) Y (defining super learner \nestimation \\(\\overline{Q}_0\\)).Fit TMLE tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimation \n\\(g_0\\) \\(\\overline{Q}_0\\), defined Step 6. Note: Like ,\nneed explicitly make copy data (work around\ndata.table optimizations), e.g., (cpp2 <- data.table::copy(cpp)), \nuse cpp2 data going forward.","code":"\n# load the data set\ndata(cpp)\ncpp <- cpp %>%\n  as_tibble() %>%\n  dplyr::filter(!is.na(haz)) %>%\n  mutate(\n    parity01 = as.numeric(parity > 0),\n    haz01 = as.numeric(haz > 0)\n  )\nmetalearner <- make_learner(\n  Lrnr_solnp,\n  loss_function = loss_loglik_binomial,\n  learner_function = metalearner_logistic_binomial\n)"},{"path":"tmle3.html","id":"tmle3-ex2","chapter":"7 The TMLE Framework","heading":"7.8.2 Estimation of Strata-Specific ATEs with tmle3","text":"exercise, work random sample 5,000 patients \nparticipated International Stroke Trial (IST). data described \nChapter 3.2 tlverse handbook. included data \nsummarized description relevant exercise.outcome, \\(Y\\), indicates recurrent ischemic stroke within 14 days \nrandomization (DRSISC); treatment interest, \\(\\), randomized\naspirin vs. aspirin treatment allocation (RXASP ist); \nadjustment set, \\(W\\), consists simply variables measured baseline. \ndata, outcome occasionally missing, need create \nvariable indicating missingness (\\(\\Delta\\)) analyses \ntlverse, since missingness automatically detected NA present\noutcome. Covariates missing values (RATRIAL, RASP3 RHEP24)\nalready imputed. Additional covariates created\n(MISSING_RATRIAL_RASP3 MISSING_RHEP24), indicate whether \ncovariate imputed. missingness identical RATRIAL \nRASP3, one covariate indicating imputation two\ncovariates created.Estimate average effect randomized aspirin treatment (RXASP = 1) \nrecurrent ischemic stroke. Even though missingness mechanism \\(Y\\),\n\\(\\Delta\\), need specified node list, still need\naccounted TMLE. words, estimation problem,\n\\(\\Delta\\) relevant factor likelihood. Thus, defining \nlist sl3 learners likelihood factor, sure include list\nlearners estimation \\(\\Delta\\), say sl_Delta, specify \nlearner list, like \nlearner_list <- list(= sl_A, delta_Y = sl_Delta, Y = sl_Y).Recall RCT conducted internationally. Suppose concern\ndose aspirin may varied across geographical regions, \naverage across geographical regions may warranted. Calculate \nstrata specific ATEs according geographical region (REGION).","code":"\nist_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/deming2019-workshop/\",\n    \"master/data/ist_sample.csv\"\n  )\n)"},{"path":"tmle3.html","id":"summary","chapter":"7 The TMLE Framework","heading":"7.9 Summary","text":"tmle3 general purpose framework generating TML estimates. easiest\nway use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. However, digging \nhood allows users specify wide range TMLEs. next sections,\n’ll see framework can used estimate advanced parameters \noptimal treatments stochastic shift interventions.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"optimal-individualized-treatment-regimes","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8 Optimal Individualized Treatment Regimes","text":"Ivana MalenicaBased tmle3mopttx R package\nIvana Malenica, Jeremy Coyle, Mark van der Laan.Updated: 2021-12-22","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"learning-objectives-4","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.1 Learning Objectives","text":"Differentiate dynamic optimal dynamic treatment interventions static\ninterventions.Explain benefits, challenges, associated using optimal\nindividualized treatment regimes practice.Contrast impact implementing optimal individualized treatment\nregime population impact implementing static dynamic\ntreatment regimes population.Estimate causal effects optimal individualized treatment regimes \ntmle3mopttx R package.Assess mean optimal individualized treatment resource\nconstraints.Implement optimal individualized treatment rules based sub-optimal\nrules, “simple” rules, recognize practical benefit rules.Construct “realistic” optimal individualized treatment regimes respect\nreal data subject-matter knowledge limitations interventions \nconsidering interventions supported data.Interpret estimated optimal individualized treatment rule.Measure variable importance defined terms optimal individualized\ntreatment interventions.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"introduction-to-optimal-individualized-interventions","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.2 Introduction to Optimal Individualized Interventions","text":"Identifying intervention effective patient based \nlifestyle, genetic environmental factors common goal precision\nmedicine. put context, Abacavir Tenofovir commonly prescribed\npart antiretroviral therapy Human Immunodeficiency Virus (HIV)\npatients. However, individuals benefit two medications equally.\nparticular, patients renal dysfunction might deteriorate \nprescribed Tenofovir, due high nephrotoxicity caused medication.\nTenofovir still highly effective treatment option HIV patients, \norder maximize patient’s well-, beneficial prescribe\nTenofovir individuals healthy kidney function. another example,\nconsider HIV trial goal improve retention HIV care.\nrandomized clinical trial, several interventions show efficacy- including\nappointment reminders text messages, small cash incentives time\nclinic visits, peer health workers. Ideally, want improve effectiveness\nassigning patient intervention likely benefit ,\nwell improve efficiency allocating resources individuals need\n, benefit .\nFIGURE 5.1: Dynamic Treatment Regime Clinical Setting\nOne opts administer intervention individuals profit ,\ninstead assigning treatment population level. know \nintervention works patient? aim motivates different type \nintervention, opposed static exposures described previous chapters.\nparticular, chapter learn dynamic “individualized”\ninterventions tailor treatment decision based collected\ncovariates. Formally, dynamic treatments represent interventions \ntreatment-decision stage allowed respond currently available\ntreatment covariate history. dynamic treatment rule can thought \nrule input available set collected covariates, \noutput individualized treatment patient\n(Bembom Laan 2007, @robins1986, @moodie2013).statistics community treatment strategy termed \nindividualized treatment regime (ITR), also known optimal\ndynamic treatment rule, optimal treatment regime, optimal strategy,\noptimal policy (Murphy 2003, @robins2004). (counterfactual)\npopulation mean outcome ITR value ITR (Murphy 2003, @robins2004).\nEven , suppose one wishes maximize population mean \noutcome, individual access set measured\ncovariates. means, example, can learn individual\ncharacteristics assigning treatment increases probability beneficial\noutcome. ITR maximal value referred \noptimal ITR optimal individualized treatment. Consequently, value\noptimal ITR termed optimal value, \nmean optimal individualized treatment.problem estimating optimal individualized treatment received much\nattention statistics literature years, especially \nadvancement precision medicine; see Murphy (2003), Robins (2004), Zhang et al. (2016),\nZhao et al. (2012), Chakraborty Moodie (2013) Robins Rotnitzky (2014) name . However, much \nearly work depends parametric assumptions. , even randomized\ntrial, statistical inference optimal individualized treatment relies\nassumptions generally believed false, can lead biased\nresults.chapter, consider estimation mean outcome optimal\nindividualized treatment candidate rules restricted depend \nuser-supplied subset baseline covariates. estimation problem \naddressed statistical model data distribution \nnonparametric, places restrictions probability patient\nreceiving treatment given covariates (randomized trial). , \ndon’t need make assumptions relationship outcome \ntreatment covariates, relationship treatment \ncovariates. , provide Targeted Maximum Likelihood Estimator \nmean optimal individualized treatment allows us generate valid\ninference parameter, without parametric assumptions.following, provide brief overview methodology focus \nbuilding intuition target parameter importance — aided simulations,\ndata examples software demonstrations. information technical aspects\nalgorithm, practical advice overview, interested reader invited \nadditionally consult van der Laan Luedtke (2015), Luedtke van der Laan (2016), Montoya, Laan, et al. (2021) \nMontoya, Skeem, et al. (2021).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"data-structure-and-notation","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.3 Data Structure and Notation","text":"Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\). denote \\(\\) categorical treatment, \\(Y\\)\nfinal outcome. particular, define \\(\\\\mathcal{}\\) \n\\(\\mathcal{} \\equiv \\{a_1, \\cdots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories (possibly two, binary setup).\nNote treat \\(W\\) vector-valued, representing collected\nbaseline covariates. Therefore, single random individual \\(\\), \nobserved data \\(O_i\\): corresponding baseline covariates \\(W_i\\),\ntreatment \\(A_i\\), final outcome \\(Y_i\\). Let \\(O^n = \\{O_i\\}_{=1}^n\\) denote\n\\(n\\) observed samples. , say \\(O^n \\sim P_0\\), \ndata drawn true probability distribution \\(P_0\\). Let \\(\\mathcal{M}\\)\ndenote statistical model probability distribution data \nnonparametric, beyond possible knowledge treatment mechanism. words, \nmeans make assumptions relationship variables, might\nable say something relationship \\(\\) \\(W\\), case \nrandomized trial. general, know, willing assume \nexperiment produces data, smaller model. true data generating\ndistribution \\(P_0\\) part statistical model \\(\\mathcal{M}\\), write\n\\(P_0 \\\\mathcal{M}\\). previous chapters, denote \\(P_n\\) empirical distribution\ngives observation weight \\(1/n\\).use structural equation model (SEM) order define\nprocess gives rise observed (endogenous) observed\n(exogenous) variables, described Pearl (2009). particular, \ndenote \\(U=(U_W,U_A,U_Y)\\) exogenous random variables, drawn \\(U \\sim P_U\\).\nendogenous variables, written \\(O=(W,,Y)\\), correspond observed data.\ncan define relationships variables following structural equations:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y),\n  \\tag{8.1}\n\\end{align}\\]\ncollection \\(f=(f_W,f_A,f_Y)\\) denotes unspecified functions, beyond possible\nknowledge treatment mechanism function, \\(f_A\\). Note \ncase randomized trial, can write NPSEM \n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= U_A \\\\ Y &= f_Y(, W, U_Y),\n  \\tag{8.2}\n\\end{align}\\]\n\\(U_A\\) known distribution \\(U_A\\) independent \\(U_W\\). discuss\nlater sections identifiability.likelihood data admits factorization, implied time ordering\n\\(O\\). denote true density \\(O\\) \\(p_0\\), corresponding \ndistribution \\(P_0\\) dominating measure \\(\\mu\\).\n\\[\\begin{equation}\n  p_0(O) = p_{Y,0}(Y \\mid ,W) p_{,0}(\\mid W) p_{W,0}(W) =\n    q_{Y,0}(Y \\mid ,W) g_{,0}(\\mid W) q_{W,0}(W),\n  \\tag{8.3}\n\\end{equation}\\]\n\\(p_{Y,0}(Y|,W)\\) conditional density \\(Y\\) given \\((, W)\\) \nrespect dominating measure \\(\\mu_Y\\), \\(p_{,0}\\) conditional density\n\\(\\) given \\(W\\) respect counting measure \\(\\mu_A\\), \\(p_{W,0}\\) \ndensity \\(W\\) respect dominating measure \\(\\mu_W\\). order \nmatch relevant Targeted Learning literature, also\nwrite \\(P_{Y,0}(Y \\mid , W) = Q_{Y,0}(Y \\mid ,W)\\), \\(P_{,0}(\\mid W) = g_0(\\mid W)\\)\n\\(P_{W,0}(W)=Q_{W,0}(W)\\) corresponding conditional\ndistribution \\(Y\\) given \\((,W)\\), treatment mechanism \\(\\) given \\(W\\), \ndistribution baseline covariates. notational simplicity, additionally define\n\\(\\bar{Q}_{Y,0}(,W) \\equiv \\E_0[Y \\mid ,W]\\) conditional expectation \n\\(Y\\) given \\((,W)\\).Lastly, define \\(V\\) subset baseline covariates optimal\nindividualized rule depends , \\(V \\W\\). Note \\(V\\) \n\\(W\\), empty set, depending subject matter knowledge. particular,\nresearcher might want consider known effect modifiers available time\ntreatment decision possible \\(V\\) covariates, consider dynamic treatment\nrules based measurments can easily obtained clinical setting.\nDefining \\(V\\) restrictive set baseline covariates allows us consider\npossibly sub-optimal rules easier estimate, thereby allows \nstatistical inference counterfactual mean outcome sub-optimal rule;\nelaborate later sections.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"defining-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4 Defining the Causal Effect of an Optimal Individualized Intervention","text":"Consider dynamic treatment rules, denoted \\(d\\), set possible rules\n\\(\\mathcal{D}\\). , point treatment setting, \\(d\\) deterministic function\ntakes input \\(V\\) outputs treatment decision \n\\(V \\rightarrow d(V) \\\\{a_1, \\cdots, a_{n_A} \\}\\). use dynamic treatment rules,\ncorresponding treatment decision, describe intervention \ntreatment mechanism corresponding outcome dynamic treatment rule.mentioned previous section, causal effects defined terms \nhypothetical interventions SEM (8.1). given\nrule \\(d\\), modified system takes following form:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= d(V) \\\\ Y_{d(V)} &= f_Y(d(V), W, U_Y),\n  \\tag{8.4}\n\\end{align}\\]\ndynamic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(V)\\). couterfactual outcome\n\\(Y_{d(V)}\\) denotes outcome patient treatment assigned using \ndynamic rule \\(d(V)\\), possibly contrary fact. Similarly, counterfactual\noutcomes patients assigned treatment (\\(=1\\)), given control (\\(=0\\)), \nwritten \\(Y_1\\) \\(Y_0\\). Finally, denote distribution counterfactual outcomes\n\\(P_{U,X}\\), implied distribution exogenous variables \\(U\\) structural\nequations \\(f\\). set possible counterfactual distributions encompased\ncausal model \\(\\mathcal{M}^F\\), \\(P_{U,X} \\\\mathcal{M}^F\\).goal causal analysis motivated dynamic interventions \nestimate parameter defined counterfactual mean outcome \nrespect modified intervention distribution. , subject’s outcome ,\npossibly contrary fact, subject received treatment \nassigned rule \\(d(V)\\). Equivalently, ask following causal question:\n“expected outcome every subject received treatment according \n(optimal) individualized treatment?” order estimate optimal individualized\ntreatment, set following optimization problem:\\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\\mathcal{D}}\n\\E_{P_{U,X}}[Y_{d(V)}], \\]\noptimal individualized rule rule maximal value. note , case\nproblem hand requires minimizing mean outcome, optimal individualized\nrule rule minimal value instead.mind, can consider different\ntreatment rules, set \\(\\mathcal{D}\\):true rule, \\(d_{0,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{0,\\text{opt}}(V)}]\\) denoting expected outcome \ntrue optimal treatment rule \\(d_{0,\\text{opt}}(V)\\).true rule, \\(d_{0,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{0,\\text{opt}}(V)}]\\) denoting expected outcome \ntrue optimal treatment rule \\(d_{0,\\text{opt}}(V)\\).estimated rule, \\(d_{n,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{n,\\text{opt}}(V)}]\\) denoting expected outcome \nestimated optimal treatment rule \\(d_{n,\\text{opt}}(V)\\).estimated rule, \\(d_{n,\\text{opt}}\\), corresponding causal parameter\n\\(\\E_{U,X}[Y_{d_{n,\\text{opt}}(V)}]\\) denoting expected outcome \nestimated optimal treatment rule \\(d_{n,\\text{opt}}(V)\\).chapter, focus value estimated optimal rule \\(d_{n,\\text{opt}}\\),\ndata-adaptive parameter. Note true value depends sample! Finally,\ncausal target parameter interest expected outcome \nestimated optimal individualized rule:\\[\\Psi_{d_{n, \\text{opt}}(V)}(P_{U,X}) \\coloneqq \\E_{P_{U,X}}[Y_{d_{n,\n\\text{opt}}(V)}].\\]","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"identification-and-statistical-estimand","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4.1 Identification and Statistical Estimand","text":"optimal individualized rule, well value optimal\nindividualized rule, causal parameters based unobserved\ncounterfactuals. order causal quantities estimated \nobserved data, need identified statistical parameters. step\nroadmap requires make assumptions:Strong ignorability: \\(\\indep Y^{d_{n, \\text{opt}}(v)} \\mid W\\), \\(\\\\mathcal{}\\).Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(\\mid W) > 0) = 1\\)assumptions, can identify causal target parameter\nobserved data using G-computation formula. value individualized\nrule can now expressed \\[\\E_0[Y_{d_{n, \\text{opt}}(V)}] = \\E_{0,W}[\\bar{Q}_{Y,0}(=d_{n, \\text{opt}}(V),W)],\\], assumptions, interpreted mean outcome \n(possibly contrary fact), treatment assigned according optimal rule.\nFinally, statistical counterpart causal parameter interest \ndefined \\[\\psi_0 = \\E_{0,W}[\\bar{Q}_{Y,0}(=d_{n,\\text{opt}}(V),W)].\\]Inference optimal value shown difficult exceptional\nlaws, defined probability distributions positive\nprobability set \\(W\\) values conditional expectation \\(Y\\)\ngiven \\(\\) \\(W\\) constant \\(\\) - treatments equally\nbenefitial. Inference similarly difficult finite samples \ntreatment effect small strata, even though valid asymptotic\nestimators exist setting. mind, address estimation\nproblem assumption non-exceptional laws effect.Many methods learning optimal rule data developed\n(Murphy 2003; Robins 2004; Zhang et al. 2016; Zhao et al. 2012; Chakraborty Moodie 2013). \nchapter, focus methods discussed Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015). Note however, tmle3mopttx also supports widely\nused Q-learning approach, optimal individualized rule based \ninitial estimate \\(\\bar{Q}_{Y,0}(,W)\\) (Sutton, Barto, others 1998).follow methodology outlined Luedtke van der Laan (2016) \nvan der Laan Luedtke (2015), learn optimal ITR using Super Learner\n(van der Laan, Polley, Hubbard 2007), estimate value cross-validated Targeted Minimum\nLoss-based Estimation (CV-TMLE) (Zheng van der Laan 2010). great generality, first\nneed estimate true individual treatment regime, \\(d_0(V)\\), \ncorresponds dynamic treatment rule takes subset covariates\n\\(V\\) assigns treatment individual based observed\ncovariates \\(v\\). estimate true optimal ITR hand, can\nestimate corresponding value.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"binary-treatment","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4.2 Binary treatment","text":"estimate optimal individualized treatment regime? case \nbinary treatment, key quantity optimal ITR blip function. One can\nshow optimal ITR assigns treatment individuals falling strata \nstratum specific average treatment effect, blip, \npositive assign treatment individuals quantity \nnegative. Therefore binary treatment, causal assumptions, define\nblip function :\n\\[\\bar{Q}_0(V) \\equiv \\E_0[Y_1-Y_0 \\mid V] \\equiv \\E_0[\\bar{Q}_{Y,0}(1,W) -\n\\bar{Q}_{Y,0}(0,W) \\mid V],\\]\naverage treatment effect within stratum \\(V\\). note \noptimal individualized rule can now derived \\(d_{n,\\text{opt}}(V) = \\mathbb{}(\\bar{Q}_{n}(V) > 0)\\).package tmle3mopttx relies using Super Learner estimate blip\nfunction. mind, loss function utilized learning optimal\nindividualized rule corresponds conditional mean type losses. however worth\nmentioning Luedtke van der Laan (2016) present three different approaches learning optimal\nrule. Namely, focus :Super Learner blip function using squared error loss,Super Learner blip function using squared error loss,Super Learner \\(d_0\\) using weighted classification loss function,Super Learner \\(d_0\\) using weighted classification loss function,Super Learner \\(d_0\\) uses library candidate estimators \nimplied estimators blip well estimators directly go \n\\(d_0\\) weighted classification.Super Learner \\(d_0\\) uses library candidate estimators \nimplied estimators blip well estimators directly go \n\\(d_0\\) weighted classification.benefit relying blip function, implemented tmle3mopttx, \none can look distribution predicted outcomes blip given\nsample. estimate blip allows one identify patients sample\nbenefit (least) treatment. Additionally, blip-based approach\nallows straight-forward extension categorical treatment, interpretable rules,\nOIT resource constrains, percent population can receive\ntreatment (. R. Luedtke van der Laan 2016).Relying Targeted Maximum Likelihood (TML) estimator Super Learner\nestimate blip function, follow steps order obtain\nvalue ITR:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(\\mid W)\\) using sl3. denote \nestimates \\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(\\mid W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW)\ntransform outcome (double-robust pseudo-outcome), define:\n\\[D_{\\bar{Q}_Y,g,}(O) \\equiv \\frac{\\mathbb{}(=)}{g(\\mid W)} (Y -\n\\bar{Q}_Y(,W)) + \\bar{Q}_Y(=,W).\\]Note randomization positivity assumptions \n\\(\\E[D_{\\bar{Q}_Y,g,}(O) \\mid V] = \\E[Y_a \\mid V]\\). emphasize double\nrobust nature -IPW transform — consistency \\(\\E[Y_a \\mid V]\\) depend\ncorrect estimation either \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(\\mid W)\\). \n, randomized trial, guaranteed consistent estimate \\(\\E[Y_a \\mid V]\\)\neven get \\(\\bar{Q}_{Y,0}(,W)\\) wrong! alternative double-robust pseudo-outcome\njust presented single stage Q-learning, estimate \\(\\bar{Q}_{Y,0}(,W)\\)\nused predict \\(\\bar{Q}_{Y,n}(=1,W)\\) \\(\\bar{Q}_{Y,n}(=0,W)\\). provides\nestimate blip function, \\(\\bar{Q}_{Y,n}(=1,W) - \\bar{Q}_{Y,n}(=0,W)\\), \nrelies good job estimating \\(\\bar{Q}_{Y,0}(,W)\\).Using double-robust pseudo-outcome, can define following contrast:\n\\[D_{\\bar{Q}_Y,g}(O) = D_{\\bar{Q}_Y, g, =1}(O) - D_{\\bar{Q}_Y, g, =0}(O).\\]estimate blip function, \\(\\bar{Q}_{0,}(V)\\), regressing\n\\(D_{\\bar{Q}_Y,g}(O)\\) \\(V\\) using specified sl3 library learners \nappropriate loss function. Finally, ready final steps.estimated rule corresponds \\(\\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).estimated rule corresponds \\(\\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).obtain inference mean outcome estimated optimal rule\nusing CV-TMLE.obtain inference mean outcome estimated optimal rule\nusing CV-TMLE.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"categorical-treatment","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4.3 Categorical treatment","text":"line approach considered binary treatment, extend blip\nfunction allow categorical treatment. denote blip function\nextensions pseudo-blips, new estimation targets \ncategorical setting. define pseudo-blips vector-valued entities \noutput given \\(V\\) vector length equal number treatment\ncategories, \\(n_A\\). , define :\n\\[\\bar{Q}_0^{pblip}(V) = \\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}\\]implement three different pseudo-blips tmle3mopttx.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv \\E_0[Y_a-Y_0 \\mid V]\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case \nbinary treatment, strategy reduces approach described \nbinary setup.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference. Hence :\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv \\E_0[Y_a-Y_0 \\mid V]\\] \\(Y_0\\) \nspecified reference category \\(=0\\). Note , case \nbinary treatment, strategy reduces approach described \nbinary setup.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv \\E_0 [Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a \\mid V].\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip2 approach corresponds defining blip relative average \ncategories. , can define \\(\\bar{Q}_{0,}^{pblip-avg}(V)\\) :\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv \\E_0 [Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_a \\mid V].\\]\ncase subject-matter knowledge regarding reference category\nuse available, blip2 might viable option.Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv \\E_0 [ Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=\\mid V) \\mid V ].\\]Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv \\E_0 [ Y_a - \\frac{1}{n_A} \\sum_{\\\n  \\mathcal{}} Y_{} P(=\\mid V) \\mid V ].\\]Just like binary case, pseudo-blips estimated regressing contrasts\ncomposed using -IPW transform \\(V\\).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"technical-note-inference-and-data-adaptive-parameter","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4.4 Technical Note: Inference and data-adaptive parameter","text":"randomized trial, statistical inference relies second-order\ndifference estimate optimal individualized treatment \noptimal individualized treatment asymptotically negligible. \nreasonable condition consider rules depend small number \ncovariates, willing make smoothness assumptions. Alternatively,\ncan consider TMLEs statistical inference data-adaptive target\nparameters defined terms estimate optimal individualized\ntreatment. particular, instead trying estimate mean true\noptimal individualized treatment, aim estimate mean \nestimated optimal individualized treatment. , develop cross-validated\nTMLE approach provides asymptotic inference minimal conditions \nmean estimate optimal individualized treatment. \nparticular, considering data adaptive parameter allows us avoid\nconsistency rate condition fitted optimal rule, required \nasymptotic linearity TMLE mean actual, true optimal\nrule. Practically, estimated (data-adaptive) rule preferred, \npossibly sub-optimal rule one implemented population.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"technical-note-why-cv-tmle","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.4.5 Technical Note: Why CV-TMLE?","text":"discussed van der Laan Luedtke (2015), CV-TMLE necessary \nnon-cross-validated TMLE biased upward mean outcome rule,\ntherefore overly optimistic. generally however, using CV-TMLE allows us\nfreedom estimation therefore greater data adaptivity, without\nsacrificing inference.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"interpreting-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.5 Interpreting the Causal Effect of an Optimal Individualized Intervention","text":"summary, mean outcome optimal individualized treatment \ncounterfactual quantity interest representing mean outcome \neverybody, contrary fact, received treatment optimized\noutcome. optimal individualized treatment regime rule \noptimizes mean outcome dynamic treatment, candidate\nrules restricted respond user-supplied subset baseline\ncovariates. essence, target parameter answers key\naim precision medicine: allocating available treatment tailoring \nindividual characteristics patient, goal optimizing \nfinal outcome.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"oit-eval-bin","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6 Evaluating the Causal Effect of an OIT with Binary Treatment","text":"Finally, demonstrate evaluate mean outcome optimal\nindividualized treatment using tmle3mopptx. start, let’s load packages\n’ll use set seed:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mopttx)\nlibrary(devtools)\n\nset.seed(111)"},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6.1 Simulated Data","text":"First, load simulated data. start general setup\ntreatment binary variable; later chapter consider\nanother data-generating distribution \\(\\) categorical. example,\ndata generating distribution following form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{3 \\times 3})\\\\\n  \\P(=1 \\mid W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  \\P(Y=1 \\mid ,W) &= 0.5\\text{logit}^{-1}[-5I(=1)(W_1-0.5)+5I(=0)(W_1-0.5)] +\n     0.5\\text{logit}^{-1}(W_2W_3)\n\\end{align*}\\]composes observed data structure \\(O = (W, , Y)\\). Note \ntruth \\(\\psi=0.578\\) data generating distribution.formally express fact using tlverse grammar introduced \ntmle3 package, create single data object specify functional\nrelationships nodes directed acyclic graph (DAG) via\nstructural equation models (SEMs), reflected node list\nset :now observed data structure (data) specification role\nvariable dataset plays nodes DAG.","code":"\ndata(\"data_bin\")\n# organize data and nodes for tmle3\ndata <- data_bin\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\"),\n  A = \"A\",\n  Y = \"Y\"\n)"},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6.2 Constructing Optimal Stacked Regressions with sl3","text":"easily incorporate ensemble machine learning estimation procedure,\nrely facilities provided sl3 R\npackage. Using framework provided sl3\npackage, nuisance parameters TML estimator\nmay fit ensemble learning, using cross-validation framework \nSuper Learner algorithm van der Laan, Polley, Hubbard (2007).seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression (Q), propensity score\n(g), blip function (B). make explicit respect \nstandard notation bundling ensemble learners list object :learner_list object specifies role ensemble\nlearners ’ve generated play computing initial estimators. Recall \nneed initial estimators relevant parts likelihood order \nbuild TMLE parameter interest. particular, learner_list\nmakes explicit fact Y used fitting outcome regression,\nused fitting treatment mechanism regression, finally B\nused fitting blip function.","code":"\n# Define sl3 library and metalearners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\n\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\nlrn_lasso <- Lrnr_glmnet$new()\n\n## Define the Q learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner:\ng_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the B learner:\nb_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso,lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere. specify argument\nV = c(\"W1\", \"W2\", \"W3\") initializing tmle3_Spec object order \ncommunicate ’re interested learning rule dependent V\ncovariates. Note don’t specify V — result rule\nbased collected covariates; see example like \nshortly. also need specify type\n(pseudo) blip use estimation problem, list learners used\nestimate blip function, whether want maximize minimize final\noutcome, advanced features including searching less\ncomplex rule, realistic interventions possible resource constraints.seen , tmle3_mopttx_blip_revere specification object\n(like tmle3_Spec objects) store data \nspecific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally.elaborate initialization specifications. initializing \nspecification TMLE parameter interest, specified \nset covariates rule depends (V), type (pseudo) blip use\n(type), learners used estimating relevant parts \nlikelihood blip function. addition, need specify whether \nwant maximize mean outcome rule (maximize), whether \nwant estimate rule covariates \\(V\\) provided user\n(complex). FALSE, tmle3mopttx instead consider possible\nrules smaller set covariates including static rules, optimize\nmean outcome subsets \\(V\\). , user might \nprovided full set collected covariates input \\(V\\), possible\ntrue rule depends subset set provided user. \ncase, returned mean optimal individualized rule based\nsmaller subset. addition, provide option search realistic\noptimal individualized interventions via realistic specification. \nTRUE, treatments supported data considered, therefore\nalleviating concerns regarding practical positivity issues. Finally, can incorporate\nsource constrains setting resource argument less 1. explore \nimportant extensions tmle3mopttx later sections.studying output generated, can see confidence interval covers \ntrue parameter, expected.","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 1\n)# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se   lower   upper psi_transformed\n1:  TSM E[Y_{A=NULL}]  0.35553  0.55371 0.02598 0.50279 0.60463         0.55371\n   lower_transformed upper_transformed\n1:           0.50279           0.60463"},{"path":"optimal-individualized-treatment-regimes.html","id":"resource-constraint","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6.3.1 Resource constraint","text":"can restrict number individuals get treatment \ntreating \\(k\\) percent samples. , patients biggest benefit (according\nestimated blip) receive treatment. order impose \nresource constraint, specify percent individuals can\nget treatment. example, resource=1, \nindividuals blip higher zero get treatment; resource=0,\nnoone treated.can compare number individuals got treatment without \nresource constraint:","code":"\n# initialize a tmle specification\ntmle_spec_resource <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 0.90\n)# fit the TML estimator\nfit_resource <- tmle3(tmle_spec_resource, data, node_list, learner_list)\nfit_resource\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se   lower  upper psi_transformed\n1:  TSM E[Y_{A=NULL}]  0.34304  0.55841 0.02612 0.50721 0.6096         0.55841\n   lower_transformed upper_transformed\n1:           0.50721            0.6096# Number of individuals getting treatment (no resource constraint):\ntable(tmle_spec$return_rule)\n\n  0   1 \n275 725 \n\n# Number of individuals getting treatment (resource constraint):\ntable(tmle_spec_resource$return_rule)\n\n  0   1 \n351 649 "},{"path":"optimal-individualized-treatment-regimes.html","id":"empty-v","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.6.3.2 Empty V","text":"show example \\(V\\) specified, \nresource constraint.","code":"\n# initialize a tmle specification\ntmle_spec_V_empty <- tmle3_mopttx_blip_revere(\n  type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 0.90\n)# fit the TML estimator\nfit_V_empty <- tmle3(tmle_spec_V_empty, data, node_list, learner_list)\nfit_V_empty\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.31575  0.51694 0.013528 0.49043 0.54346\n   psi_transformed lower_transformed upper_transformed\n1:         0.51694           0.49043           0.54346"},{"path":"optimal-individualized-treatment-regimes.html","id":"oit-eval-cat","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.7 Evaluating the Causal Effect of an optimal ITR with Categorical Treatment","text":"section, consider evaluate mean outcome optimal\nindividualized treatment \\(\\) two categories. \nprocedure analogous previously described binary treatment, now need\npay attention type blip define estimation stage, well\nconstruct learners.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data-1","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.7.1 Simulated Data","text":"First, load simulated data. data generating distribution \nfollowing form:\n\\[\\begin{align*}\n  W &\\sim \\mathcal{N}(\\bf{0},I_{4 \\times 4})\\\\\n  \\P(=\\mid W) &= \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\\\\n  \\P(Y=1 \\mid ,W) = 0.5\\text{logit}^{-1}[15I(=1)(W_1-0.5) - \\\\\n    3I(=2)(2W_1+0.5) + \\\\\n    3I(=3)(3W_1-0.5)] +\\text{logit}^{-1}(W_2W_1) \\\\\n\\end{align*}\\]can just load data available part package follows:composes observed data structure \\(O = (W, , Y)\\). Note \ntruth now \\(\\psi_0=0.658\\), quantity aim estimate.can see number observed categories treatment :","code":"\ndata(\"data_cat_realistic\")\n# organize data and nodes for tmle3\ndata <- data_cat_realistic\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\", \"W4\"),\n  A = \"A\",\n  Y = \"Y\"\n)# organize data and nodes for tmle3\ntable(data$A)\n\n  1   2   3 \n 24 528 448 "},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-optimal-stacked-regressions-with-sl3-1","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.7.2 Constructing Optimal Stacked Regressions with sl3","text":"QUESTION: categorical treatment, dimension blip now?\ndimension current example? go estimating ?now create new ensemble learners using \nsl3 learners initialized previously:seen , generate three different ensemble learners must fit,\ncorresponding learners outcome regression, propensity score, \nblip function. Note need estimate \\(g_0(\\mid W)\\) \ncategorical \\(\\) — therefore, use multinomial Super Learner option\navailable within sl3 package learners can address multi-class\nclassification problems. order see learners can used estimate\n\\(g_0(\\mid W)\\) sl3, run following:Since corresponding blip vector valued, \ncolumn additional level treatment. , need create\nmultivariate learners helper function create_mv_learners takes \nlist initialized learners input.make explicit respect standard notation bundling \nensemble learners list object :","code":"\n# Initialize few of the learners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_xgboost_100, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# Define the g learner, which is a multinomial learner:\n# specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp,\n  loss_function = loss_loglik_multinomial,\n  learner_function = metalearner_linear_multinomial\n)\ng_learner <- make_learner(Lrnr_sl, list(lrn_xgboost_100, lrn_xgboost_500, lrn_mean), mn_metalearner)\n\n# Define the Blip learner, which is a multivariate learner:\nlearners <- list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm)\nb_learner <- create_mv_learners(learners = learners)# See which learners support multi-class classification:\nsl3_list_learners(c(\"categorical\"))\n [1] \"Lrnr_bound\"                \"Lrnr_caret\"               \n [3] \"Lrnr_cv_selector\"          \"Lrnr_glmnet\"              \n [5] \"Lrnr_grf\"                  \"Lrnr_gru_keras\"           \n [7] \"Lrnr_h2o_glm\"              \"Lrnr_h2o_grid\"            \n [9] \"Lrnr_independent_binomial\" \"Lrnr_lightgbm\"            \n[11] \"Lrnr_lstm_keras\"           \"Lrnr_mean\"                \n[13] \"Lrnr_multivariate\"         \"Lrnr_nnet\"                \n[15] \"Lrnr_optim\"                \"Lrnr_polspline\"           \n[17] \"Lrnr_pooled_hazards\"       \"Lrnr_randomForest\"        \n[19] \"Lrnr_ranger\"               \"Lrnr_rpart\"               \n[21] \"Lrnr_screener_correlation\" \"Lrnr_solnp\"               \n[23] \"Lrnr_svm\"                  \"Lrnr_xgboost\"             \n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v1","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.7.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"can see confidence interval covers truth.NOTICE distribution assigned treatment! need shortly.","code":"\n# initialize a tmle specification\ntmle_spec_cat <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)# fit the TML estimator\nfit_cat <- tmle3(tmle_spec_cat, data, node_list, learner_list)\nfit_cat\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.53783  0.62117 0.065863 0.49208 0.75025\n   psi_transformed lower_transformed upper_transformed\n1:         0.62117           0.49208           0.75025\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat$return_rule)\n\n  1   2   3 \n250 432 318 "},{"path":"optimal-individualized-treatment-regimes.html","id":"extensions-to-causal-effect-of-an-oit","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.8 Extensions to Causal Effect of an OIT","text":"section, consider two extensions procedure described \nestimating value OIT. First one considers setting user\nmight interested grid possible sub-optimal rules, corresponding \npotentially limited knowledge potential effect modifiers. second\nextension concerns implementation realistic optimal individual\ninterventions certain regimes might preferred, due practical \nglobal positivity restraints, realistic implement.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v2","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.8.1 Simpler Rules","text":"order consider ambitious fully \\(V\\)-optimal rule, \ndefine \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\). \nparticular, allows us define Super Learner \\(d_0\\) includes\nrange estimators simple (e.g., statis rules) complex\n(e.g. full \\(V\\)), let discrete Super Learner select simple rule \nappropriate. allows us consider sub-optimal rules easier estimate \npotentially provide realistic rules. Within tmle3mopttx paradigm, just need\nchange complex parameter FALSE:Even though specified baseline covariates basis\nrule estimation, simpler rule sufficient maximize mean outcome.QUESTION: set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?","code":"\n# initialize a tmle specification\ntmle_spec_cat_simple <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = FALSE, realistic = FALSE\n)# fit the TML estimator\nfit_cat_simple <- tmle3(tmle_spec_cat_simple, data, node_list, learner_list)\nfit_cat_simple\nA tmle3_Fit that took 1 step(s)\n   type          param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{d(V=W1)}]  0.54336  0.61838 0.060848 0.49912 0.73764\n   psi_transformed lower_transformed upper_transformed\n1:         0.61838           0.49912           0.73764"},{"path":"optimal-individualized-treatment-regimes.html","id":"oit-eval-cat-v3","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.8.2 Realistic Optimal Individual Regimes","text":"addition considering less complex rules, tmle3mopttx also provides \noption estimate mean realistic, implementable, optimal\nindividualized treatment. often case assigning particular regime\nmight ability fully maximize (minimize) desired outcome, \ndue global practical positivity constrains, treatment can never \nimplemented real life (highly unlikely). , specifying\nrealistic TRUE, consider possibly suboptimal treatments optimize\noutcome question supported data.QUESTION: Referring back data-generating distribution, \nthink distribution allocated treatment changed distribution\n“non-realistic”\" rule?","code":"\n# initialize a tmle specification\ntmle_spec_cat_realistic <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE, realistic = TRUE\n)# fit the TML estimator\nfit_cat_realistic <- tmle3(tmle_spec_cat_realistic, data, node_list, learner_list)\nfit_cat_realistic\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est      se   lower   upper psi_transformed\n1:  TSM E[Y_{A=NULL}]  0.54035  0.65821 0.02135 0.61636 0.70005         0.65821\n   lower_transformed upper_transformed\n1:           0.61636           0.70005\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat_realistic$return_rule)\n\n  2   3 \n506 494 "},{"path":"optimal-individualized-treatment-regimes.html","id":"missingness-and-tmle3mopttx","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.8.3 Missingness and tmle3mopttx","text":"section, present use tmle3mopttx package data subject\nmissingness \\(Y\\). Let’s start add missingness outcome, first.start, must first add library — now also need estimate \nmissigness process well.learner_list object specifies role ensemble\nlearners ’ve generated play computing initial estimators needed\nbuilding TMLE parameter interest. particular, makes\nexplicit fact Y used fitting outcome regression\nused fitting treatment mechanism regression,\nB fitting blip function, delta_Y fits missing outcome process.Now, additional estimation step associated missingness added, can\nproceed usual.","code":"data_missing <- data_cat_realistic\n\n#Add some random missingless:\nrr <- sample(nrow(data_missing), 100, replace = FALSE)\ndata_missing[rr,\"Y\"]<-NA\n\nsummary(data_missing$Y)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.000   0.000   0.464   1.000   1.000     100 \ndelta_learner <- Lrnr_sl$new(\n  learners = list(lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner, delta_Y=delta_learner)\n# initialize a tmle specification\ntmle_spec_cat_miss <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)# fit the TML estimator\nfit_cat_miss <- tmle3(tmle_spec_cat_miss, data_missing, node_list, learner_list)\nfit_cat_miss\nA tmle3_Fit that took 1 step(s)\n   type                    param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL, delta_Y=1}]  0.53537    0.727 0.061309 0.60683 0.84716\n   psi_transformed lower_transformed upper_transformed\n1:           0.727           0.60683           0.84716"},{"path":"optimal-individualized-treatment-regimes.html","id":"q-learning","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.8.4 Q-learning","text":"Alternatively, estimate mean optimal individualized\ntreatment using Q-learning. optimal rule can learned fitting \nlikelihood, consequently estimating optimal rule fit \nlikelihood (Sutton, Barto, others 1998; Murphy 2003).outline use tmle3mopttx package order estimate mean\nITR using Q-learning. demonstrated previous sections, \nfirst need initialize specification TMLE parameter \ninterest. opposed previous section however, now use\ntmle3_mopttx_Q instead tmle3_mopttx_blip_revere order indicate \nwant use Q-learning instead TMLE.","code":"\n# initialize a tmle specification\ntmle_spec_Q <- tmle3_mopttx_Q(maximize = TRUE)\n\n# Define data:\ntmle_task <- tmle_spec_Q$make_tmle_task(data, node_list)\n\n# Define likelihood:\ninitial_likelihood <- tmle_spec_Q$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\n\n# Estimate the parameter:\nQ_learning(tmle_spec_Q, initial_likelihood, tmle_task)[1]"},{"path":"optimal-individualized-treatment-regimes.html","id":"variable-importance-analysis-with-oit","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.9 Variable Importance Analysis with OIT","text":"Suppose one wishes assess importance observed covariate, \nterms maximizing (minimizing) population mean outcome \noptimal individualized treatment regime. particular, covariate \nmaximizes (minimizes) population mean outcome optimal\nindividualized treatment considered covariates optimal\nassignment might considered important outcome. put \ncontext, perhaps optimal allocation treatment 1, denoted \\(A_1\\), results \nlarger mean outcome optimal allocation another treatment 2, denoted \\(A_2\\).\nTherefore, label \\(A_1\\) higher variable importance \nregard maximizing (minimizing) mean outcome optimal\nindividualized treatment.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data-2","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.9.1 Simulated Data","text":"illustration purpose, bin baseline covariates corresponding \ndata-generating distribution described previously:node list now includes \\(W_1\\) treatments well! Don’t worry,\nstill properly adjust baseline covariates.","code":"\n# bin baseline covariates to 3 categories:\ndata$W1<-ifelse(data$W1<quantile(data$W1)[2],1,ifelse(data$W1<quantile(data$W1)[3],2,3))\n\nnode_list <- list(\n  W = c(\"W3\", \"W4\", \"W2\"),\n  A = c(\"W1\", \"A\"),\n  Y = \"Y\"\n)"},{"path":"optimal-individualized-treatment-regimes.html","id":"variable-importance-using-targeted-estimation-of-the-value-of-the-itr","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.9.2 Variable Importance using Targeted Estimation of the value of the ITR","text":"previous sections seen obtain contrast mean\noptimal individualized rule mean observed outcome\nsingle covariate — now ready run variable importance analysis\nspecified covariates. order run variable importance\nanalysis, first need initialize specification TMLE \nparameter interest done . addition, need specify\ndata corresponding list nodes, well appropriate\nlearners outcome regression, propensity score, blip function.\nFinally, need specify whether adjust \ncovariates assessing variable importance . adjust \\(W\\)s\nanalysis, adjust_for_other_A=TRUE, also \\(\\) covariates\ntreated exposure variable importance loop.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle3_mopttx_vim. First, indicate method used learning optimal\nindividualized treatment specifying method argument \ntmle3_mopttx_vim. method=\"Q\", using Q-learning rule\nestimation, need specify V, type learners arguments\nspec, since important Q-learning. However, \nmethod=\"SL\", corresponds learning optimal individualized\ntreatment using outlined methodology, need specify type\n(pseudo) blip use estimation problem, whether want \nmaximize minimize outcome, complex realistic rules, resource constraint.\nFinally, method=\"SL\" also need communicate ’re interested learning \nrule dependent V covariates specifying V argument. \nmethod=\"Q\" method=\"SL\", need indicate whether want maximize\nminimize mean optimal individualized rule. Finally, also\nneed specify whether final comparison mean optimal\nindividualized rule mean observed outcome \nmultiplicative scale (risk ratio) linear (similar average treatment\neffect).final result tmle3_vim tmle3mopttx spec ordered list\nmean outcomes optimal individualized treatment categorical\ncovariates dataset.","code":"\n# initialize a tmle specification\ntmle_spec_vim <- tmle3_mopttx_vim(\n  V=c(\"W2\"),\n  type = \"blip2\",\n  learners = learner_list,\n  maximize = FALSE,\n  method = \"SL\",\n  complex = TRUE,\n  realistic = FALSE\n)# fit the TML estimator\nvim_results <- tmle3_vim(tmle_spec_vim, data, node_list, learner_list,\n  adjust_for_other_A = TRUE\n)\n\nprint(vim_results)\n   type                param    init_est  tmle_est       se      lower\n1:  ATE E[Y_{A=NULL}] - E[Y]  0.00066272  0.040712 0.016866  0.0076547\n2:  ATE E[Y_{A=NULL}] - E[Y] -0.01042913 -0.039754 0.022095 -0.0830588\n       upper psi_transformed lower_transformed upper_transformed  A           W\n1: 0.0737684        0.040712         0.0076547         0.0737684  A W3,W4,W2,W1\n2: 0.0035505       -0.039754        -0.0830588         0.0035505 W1  W3,W4,W2,A\n    Z_stat      p_nz p_nz_corrected\n1:  2.4138 0.0078932       0.015786\n2: -1.7993 0.0359882       0.035988"},{"path":"optimal-individualized-treatment-regimes.html","id":"exercises-2","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.10 Exercises","text":"","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"real-world-data-and-tmle3mopttx","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.10.1 Real World Data and tmle3mopttx","text":"Finally, cement everything learned far real data application.previous sections, using WASH Benefits data,\ncorresponding effect water quality, sanitation, hand washing, \nnutritional interventions child development rural Bangladesh.main aim cluster-randomized controlled trial assess \nimpact six intervention groups, including:control;control;hand-washing soap;hand-washing soap;improved nutrition counseling provision lipid-based nutrient\nsupplements;improved nutrition counseling provision lipid-based nutrient\nsupplements;combined water, sanitation, hand-washing, nutrition;combined water, sanitation, hand-washing, nutrition;improved sanitation;improved sanitation;combined water, sanitation, hand-washing;combined water, sanitation, hand-washing;chlorinated drinking water.chlorinated drinking water.aim estimate optimal ITR corresponding value optimal\nITR main intervention WASH Benefits data.outcome interest weight--height Z-score, whereas primary\ntreatment six intervention groups aimed improving living conditions.Questions:Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)? want minimize \nmaximize outcome? (pseudo) blip type use?Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)? want minimize \nmaximize outcome? (pseudo) blip type use?Load WASH Benefits data, define appropriate nodes treatment\noutcome. Use rest covariates \\(W\\) except \nmomheight now. Construct appropriate sl3 library \\(\\), \\(Y\\) \n\\(B\\).Load WASH Benefits data, define appropriate nodes treatment\noutcome. Use rest covariates \\(W\\) except \nmomheight now. Construct appropriate sl3 library \\(\\), \\(Y\\) \n\\(B\\).Based \\(V\\) defined previous question, estimate mean \nITR main randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value \noptimal ITR? change initial estimate? \nintervention prominent? think ?Based \\(V\\) defined previous question, estimate mean \nITR main randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value \noptimal ITR? change initial estimate? \nintervention prominent? think ?Using formulation questions 1 2, estimate realistic\noptimal ITR corresponding value realistic ITR. results\nchange? intervention prominent realistic rules? \nthink ?Using formulation questions 1 2, estimate realistic\noptimal ITR corresponding value realistic ITR. results\nchange? intervention prominent realistic rules? \nthink ?Consider simpler rules WASH benefits data example. covariates \nfinal rule depend ?Consider simpler rules WASH benefits data example. covariates \nfinal rule depend ?Change treatment binary variable (asset_sewmach), estimate \nvalue ITR setting \\(60\\%\\) resource constraint. \nresults indicate?Change treatment binary variable (asset_sewmach), estimate \nvalue ITR setting \\(60\\%\\) resource constraint. \nresults indicate?Change treatment , now mother’s education (momedu), \nestimate value ITR setting. results\nindicate? Can intervene variable?Change treatment , now mother’s education (momedu), \nestimate value ITR setting. results\nindicate? Can intervene variable?","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"review-of-key-concepts-1","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.10.2 Review of Key Concepts","text":"difference dynamic optimal individualized regimes?difference dynamic optimal individualized regimes?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?’s intuition behind using different blip types? switch\nblip1 blip2 considering categorical treatment? \nadvantages ?Look back results generated section categorical\ntreatments, compare mean optimal\nindividualized treatment section complex categorical\ntreatments. set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?Look back results generated section categorical\ntreatments, compare mean optimal\nindividualized treatment section complex categorical\ntreatments. set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?Compare distribution treatments assigned true optimal\nindividualized treatment realistic optimal individualized treatment.\nReferring back data-generating distribution, think \ndistribution allocated treatment changed?Compare distribution treatments assigned true optimal\nindividualized treatment realistic optimal individualized treatment.\nReferring back data-generating distribution, think \ndistribution allocated treatment changed?Using simulation, perform variable importance analysis using\nQ-learning. results change ?Using simulation, perform variable importance analysis using\nQ-learning. results change ?","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"advanced-topics-1","chapter":"8 Optimal Individualized Treatment Regimes","heading":"8.10.3 Advanced Topics","text":"can extend current approach include exceptional laws?can extend current approach include exceptional laws?can extend current approach continuous interventions?can extend current approach continuous interventions?","code":""},{"path":"stochastic-treatment-regimes.html","id":"stochastic-treatment-regimes","chapter":"9 Stochastic Treatment Regimes","heading":"9 Stochastic Treatment Regimes","text":"Nima HejaziBased tmle3shift R package\nNima Hejazi, Jeremy Coyle, Mark van der Laan.Updated: 2021-12-22","code":""},{"path":"stochastic-treatment-regimes.html","id":"learning-objectives-5","chapter":"9 Stochastic Treatment Regimes","heading":"Learning Objectives","text":"Differentiate stochastic treatment regimes static, dynamic, optimal\ndynamic treatment regimes.Describe real-world data analysis may incorporate assessing causal\neffects stochastic treatment regimes.Contrast population-level (general) stochastic treatment regime \n(individualized) modified treatment policy.Estimate population-level causal effects modified treatment policies\ntmle3shift R package.Specify interpret set causal effects based upon differing modified\ntreatment policies arising grid counterfactual shifts.Construct marginal structural models measure variable importance terms\nstochastic interventions, using grid counterfactual shifts.Implement, tmle3shift R package, modified treatment policies\nshift individual units extent supported observed\ndata.","code":""},{"path":"stochastic-treatment-regimes.html","id":"why-stochastic-interventions","chapter":"9 Stochastic Treatment Regimes","heading":"9.1 Why Stochastic Interventions?","text":"Stochastic treatment regimes, stochastic interventions, constitute \nrelatively simple yet extremely flexible expressive framework defining\nrealistic causal effects. contrast intervention regimens discussed\npreviously, stochastic interventions may applied nearly manner \ntreatment variable – binary, ordinal, continuous – allowing rich set \ncausal effects defined formalism. chapter focuses \nexamining types stochastic interventions may applied \ncontinuous treatment variables, static dynamic treatment regimes\neasily applied. Notably, resultant causal effects conveniently \nendowed interpretation echoing ordinary regression adjustment.next chapter, introduce two alternative uses stochastic\ninterventions – recently formulated intervention applicable binary\ntreatment variables (Kennedy 2019) definition causal\neffects presence post-treatment, mediating, variables. , \nfocus tools provided tmle3shift R\npackage, exposes targeted minimum\nloss-based estimators causal effects stochastic interventions \nadditively shift observed value treatment variable. \ncomprehensive, technical presentations aspects material \nchapter appear Dı́az van der Laan (2012), Dı́az van der Laan (2018),\nHejazi, van der Laan, et al. (2020), Hejazi (2021).","code":""},{"path":"stochastic-treatment-regimes.html","id":"data-structure-and-notation-1","chapter":"9 Stochastic Treatment Regimes","heading":"9.2 Data Structure and Notation","text":"Let us return familiar data unit \\(O = (W, , Y)\\), \\(W\\) denote\nbaseline covariates (e.g., age, biological sex, education level), \\(\\) \ntreatment variable (e.g., dose nutritional supplements), \\(Y\\) outcome\ninterest (e.g., disease status). , consider \\(\\) \ncontinuous-valued (.e., \\(\\\\R\\)) ordinal many levels. given\nstudy, consider observing \\(n\\) independent identically distributed units\n\\(O_1, \\ldots, O_n\\).Following roadmap, let \\(O \\sim P \\\\M\\), \\(\\M\\) \nnonparametric statistical model, minimizing restrictions form \ndata-generating distribution \\(P\\). formalize definition stochastic\ninterventions corresponding causal effects, introduce structural\ncausal model (SCM), based Pearl (2009), define system\nchanges posited interventions:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y).\n  \\tag{9.1}\n\\end{align}\\]\nset structural equations provide mechanistic model describing \nrelationships variables composing observed data unit \\(O\\). SCM\ndescribes temporal ordering variables (.e., \\(Y\\) occurs \n\\(\\), occurs \\(W\\)); specifies deterministic functions \\(\\{f_W, f_A, f_Y\\}\\) generating variable \\(\\{W, , Y\\}\\) based preceding \nexogenous (unobserved) variable \\(\\{U_W, U_A, U_Y\\}\\); requires \nexogenous variable assumed contain unobserved causes \ncorresponding observed variable.can factorize likelihood data unit \\(O\\) follows, revealing\northogonal components density, \\(p_0^O\\), evaluated typical\nobservation \\(o\\):\n\\[\\begin{equation}\n  p_0(x) = q_{0,Y}(y \\mid = , W = w) g_{0,}(\\mid W = w) q_{0,W}(w),\n  \\tag{9.2}\n\\end{equation}\\]\n\\(q_{0, Y}\\) conditional density \\(Y\\) given \\(\\{, W\\}\\) respect\ndominating measure, \\(g_{0, }\\) conditional density \\(\\) given\n\\(W\\) respect dominating measure \\(\\mu\\), \\(q_{0, W}\\) density \n\\(W\\) respect dominating measure \\(\\nu\\). interest continuing \nuse familiar notation, let \\(Q(, W) = \\E[Y \\mid , W]\\), \\(g(\\mid W) = g_{}(\\mid W)\\), \\(q_W\\) marginal distribution \\(W\\). Importantly, \nSCM parameterizes \\(p_0\\) terms distribution random variables \\((O, U)\\) modeled system equations. turn, implies model \ndistribution counterfactual random variables generated interventions \ndata-generating process.","code":""},{"path":"stochastic-treatment-regimes.html","id":"defining-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.3 Defining the Causal Effect of a Stochastic Intervention","text":"Causal effects defined terms contrasts hypothetical interventions\nSCM (9.1). Stochastic interventions modifying\ncomponents SCM may thought two equivalent ways. general\nstochastic intervention replaces equation \\(f_A\\), gives rise \\(\\),\n\\(g(\\mid W)\\), natural conditional density , candidate\ndensity \\(g_{A_{\\delta}}(\\mid W)\\). absence intervention, \nconsider given value \\(\\\\mathcal{}\\), support \\(\\) – , \nresult evaluating function \\(f_A\\) given value \\(W = w\\) – \nresult random draw distribution defined conditional density\n\\(g(\\mid W)\\), , \\(A_{\\delta} \\sim g_{A_{\\delta}}(\\cdot \\mid W)\\). \napplying intervention, simply remove structural equation \\(f_A\\),\ninstead drawing post-intervention value \\(A_{\\delta}\\) distribution\ndefined candidate density \\(g_{A_{\\delta}}(\\mid W)\\). \npost-intervention value \\(A_{\\delta}\\) stochastically modified sense\ndrawn arbitrary (practice, user-defined)\ndistribution. Note familiar case static interventions can \nrecovered choosing degenerate candidate distributions, place mass\njust single value. Stock (1989) first considered estimating \ntotal effects stochastic interventions.restrictions choice candidate post-treatment\ndensity \\(g_{A_{\\delta}}(\\mid W)\\), practice, often chosen based \nknowledge natural (pre-intervention) density \\(g(\\mid W)\\). \n\\(g_{A_{\\delta}}(\\mid W)\\) piecewise smooth invertible ()\n(Haneuse Rotnitzky 2013), direct correspondence \npost-intervention density \\(g_{A_{\\delta}}(\\mid W)\\) function \\(d(, W; \\delta)\\) maps observed pair \\(\\{, W\\}\\) post-intervention quantity\n\\(A_{\\delta}\\). cases, stochastic intervention, defined \\(d(, W; \\delta)\\), said depend natural value treatment \ntermed modified treatment policy (MTP) (Haneuse Rotnitzky 2013; Dı́az van der Laan 2018; Hejazi 2021). Haneuse Rotnitzky (2013) \nYoung, Hernán, Robins (2014) provide detailed discussions contrasting \ninterpretations causal effects modified treatment policies \ngeneral stochastic interventions.Definition 9.1  (Piecewise Smooth Invertibility) \\(w \\\\mathcal{W}\\), assume interval \\(\\mathcal{}(w) = (l(w,), u(w))\\) may partitioned subintervals \\(\\mathcal{}_{\\delta,j}(w): j = 1, \\ldots, J(w)\\) \\(d(, w; \\delta)\\) equal \\(d_j(, w; \\delta)\\) \\(\\mathcal{}_{\\delta,j}(w)\\) \\(d_j(\\cdot,w; \\delta)\\) inverse\nfunction \\(h_j(\\cdot, w; \\delta)\\) derivative \\(h_j'(\\cdot, w; \\delta)\\).stochastic intervention generates counterfactual random variable\n\\(Y_{A_{\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), counterfactual outcome\n\\(Y_{A_{\\delta}} \\sim \\mathcal{P}_0^{A_{\\delta}}\\) arises replacing \nnatural value \\(\\) \\(A_{\\delta}\\) (whether draw \n\\(g_{A_{\\delta}}(\\mid W)\\) evaluating \\(d(, W; \\delta)\\)). \nremainder chapter, focus additive MTPs form\n\\[\\begin{equation}\n  d(, w) =\n  \\begin{cases}\n    + \\delta & \\text{} + \\delta \\leq u(w) \\\\\n    & \\text{} + \\delta > u(w),\n  \\end{cases}\n  \\tag{9.3}\n\\end{equation}\\]\n\\(\\delta \\\\mathbb{R}^{+}\\) defines degree observed \\(= \\)\nshifted, context stratum \\(W = w\\), \\(l(w)\\) \n\\(u(w)\\) minimum maximum values treatment \\(\\) stratum\n\\(W = w\\). Consider, example, case \\(\\) denotes (continuous-valued)\ndosage nutritional supplements (e.g., number vitamin pills) assume\ndistribution \\(\\) conditional \\(W = w\\) support interval\n\\((l(w), u(w))\\). , minimum number pills taken individual\ncovariate stratum defined \\(W = w\\) \\(l(w)\\); similarly, \nmaximum \\(u(w)\\). stochastic intervention may interpreted \nresult clinic policy encouraging individuals consume \\(\\delta\\) \nvitamin pills (\\(\\delta\\)) normally recommended (\\(\\)) based\nbaseline characteristics \\(W\\). class stochastic interventions\nintroduced Dı́az van der Laan (2012) discussed \nHaneuse Rotnitzky (2013), Dı́az van der Laan (2018), Hejazi, van der Laan, et al. (2020), \nHejazi (2021). class interventions may expressed \ngeneral stochastic intervention, per Dı́az van der Laan (2012), considering \nrandom draw \\(\\P_{A_{\\delta}}(g_{0, })(= \\mid W) = g_{0,}(- \\delta(W) \\mid W)\\).order evaluate causal effect intervention, consider \nparameter interest counterfactual mean outcome \nstochastically modified intervention distribution. target causal estimand\n\\(\\psi_{0, \\delta} \\coloneqq \\E_{P_0^{A_{\\delta}}}\\{Y_{A_{\\delta}}\\}\\), \nmean counterfactual outcome variable \\(Y_{A_{\\delta}}\\).\nDı́az van der Laan (2018) showed \\(\\psi_{0, \\delta}\\) may identified \nfunctional distribution \\(O\\):\n\\[\\begin{align}\n  \\psi_{0,\\delta} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\E_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\nonumber \\\\ &q_{0, }(\\mid W = w)\n   q_{0, W}(w) d\\mu()d\\nu(w).\n  \\tag{9.4}\n\\end{align}\\]\ncertain identification conditions, enumerate shortly, \nstatistical parameter Equation (9.4) matches exactly\ncounterfactual mean \\(\\psi_{0, \\delta}\\). book concerned\nidentification causal parameters – , establishing\nstatistical functionals observed data causal interpretations\ncertain assumptions – review key assumptions identifying \ncounterfactual mean \\(\\psi_{0, \\delta}\\) . SCM introduced prior\ngenerates independent identically distributed units \\(O\\), common\nidentification assumptions consistency (\\(Y^{A_{\\delta,}}_i = Y_i\\) \nevent \\(A_i = d(a_i, w_i)\\), \\(= 1, \\ldots, n\\)) lack interference\n(\\(Y^{A_{\\delta,}}_i\\) depend \\(d(a_j, w_j)\\) \\(= 1, \\ldots, n\\)\n\\(j \\neq \\)) hold. Beyond , require unmeasured confounding (\nanalog randomization assumption observational studies) positivity.unmeasured confounders: \\(A_i \\indep Y^{A_{\\delta,}}_i \\mid W_i\\), \n\\(= 1, \\ldots, n\\). observational study analog well-known\nrandomization assumption.Positivity (overlap): \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes \nsupport \\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).","code":""},{"path":"stochastic-treatment-regimes.html","id":"estimating-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.4 Estimating the Causal Effect of a Stochastic Intervention","text":"Dı́az van der Laan (2012) provided derivation efficient influence function\n(EIF), key quantity constructing efficient estimators, \nnonparametric model \\(\\M\\) developed classical efficient estimators\nquantity, including substitution, inverse probability weighted, one-step\ntargeted maximum likelihood (TML) estimators. one-step TML\nestimators allow semiparametric-efficient estimation inference \ntarget quantity interest \\(\\psi_{0, \\delta}\\). described \nDı́az van der Laan (2018), EIF \\(\\psi_{0, \\delta}\\), respect \nnonparametric model \\(\\M\\), \n\\[\\begin{equation}\n  D(P_0)(x) = H(, w)({y - \\overline{Q}(, w)}) +\n  \\overline{Q}(d(, w), w) - \\Psi(P_0),\n  \\tag{9.5}\n\\end{equation}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_0(- \\delta \\mid w)}\n  {g_0(\\mid w)} + \\mathbb{}(+ \\delta \\geq u(w)),\n  \\tag{9.6}\n\\end{equation}\\]\nmay reduced \n\\[\\begin{equation}\n  H(,w) = \\frac{g_0(- \\delta \\mid w)}{g_0(\\mid w)} + 1\n  \\tag{9.7}\n\\end{equation}\\]\ntreatment \\(\\) lies within limits defined covariate strata\n\\(W\\), , \\(A_i \\(u(w) - \\delta, u(w))\\). efficient influence\nfunction key ingredient construction semiparametric-efficient\nestimators. Next, focus targeted maximum likelihood (TML) estimator, \nDı́az van der Laan (2018) give following recipe:Construct initial estimators \\(g_n\\) \\(g_0(, W)\\) \\(Q_n\\) \n\\(\\overline{Q}_0(, W)\\), ideally using data-adaptive regression techniques.observation \\(\\), compute estimate \\(H_n(a_i, w_i)\\) \nauxiliary covariate \\(H(a_i,w_i)\\).Construct one-dimensional logistic regression model,\n\\[ \\text{logit}\\overline{Q}_{\\epsilon, n}(, w) =\n\\text{logit}\\overline{Q}_n(, w) + \\epsilon H_n(, w),\\]\nanalogous regression model incorporating \\(H_n\\) weights. Estimate \nregression model’s parameter \\(\\epsilon\\), obtaining \\(\\epsilon_n\\). outcome\nregression model yields \\(\\overline{Q}_n^{\\star}\\).Compute TML estimator \\(\\Psi_n\\) target parameter, defining update\n\\(\\overline{Q}_n^{\\star}\\) initial estimate\n\\(\\overline{Q}_{n, \\epsilon_n}\\):\n\\[\\begin{equation}\n  \\psi_n = \\Psi(P_n^{\\star}) = \\frac{1}{n} \\sum_{= 1}^n\n  \\overline{Q}_n^{\\star}(d(A_i, W_i), W_i).\n  \\tag{9.8}\n\\end{equation}\\]discussed previously, TML estimators constructed \nasymptotically linear usually doubly robust. Asymptotic linearity\nmeans asymptotic difference estimator \\(\\psi_n\\) \ntarget parameter \\(\\psi_0\\) can expressed terms EIF, ,\n\\[\\begin{equation}\n  \\sqrt{n}(\\psi_n - \\psi_0) = \\frac{1}{\\sqrt{n}}\\sum_{=1}^n D(P_0)(O_i) +\n    o_p(1).\n  \\tag{9.9}\n\\end{equation}\\]\nTogether regularity, asymptotic linearity establishes class estimators\nwhose asymptotic variance bounded asymptotic variance \nEIF. means estimators solutions EIF estimating\nequation (.e., plugging TML estimator \\(\\psi_n\\) EIF equation\nresults solution close zero) sampling variance may \napproximated variance EIF closed form. latter fact \ncomputationally convenient, resampling methods (e.g., bootstrap) \nstrictly necessary variance estimation. central limit theorem establishes\nasymptotic distribution estimator \\(\\psi_n\\) centered \n\\(\\psi_0\\) Gaussian:\n\\[\\begin{equation}\n  \\sqrt{n}(\\psi_n - \\psi_0) \\\\text{Normal}(0, \\sigma^2(D(P_0))).\n  \\tag{9.10}\n\\end{equation}\\]\nThus, estimate \\(\\sigma_n^2\\) variance \\(\\sigma^2(D(P_0))\\) may \ncomputed\n\\[\\begin{equation}\n  \\sigma_n^2 = \\frac{1}{n} \\sum_{= 1}^{n} D^2(\\overline{Q}_n^{\\star},\n  g_n)(O_i),\n  \\tag{9.11}\n\\end{equation}\\]\nallowing Wald-style confidence intervals coverage level \\((1 - \\alpha)\\) \ncomputed \\(\\psi_n \\pm z_{(1 - \\alpha/2)} \\cdot \\sigma_n / \\sqrt{n}\\). \ncertain conditions, resampling based bootstrap may also used \ncompute \\(\\sigma_n^2\\) (van der Laan Rose 2011).","code":""},{"path":"stochastic-treatment-regimes.html","id":"interpreting-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.5 Interpreting the Causal Effect of a Stochastic Intervention","text":"\nFIGURE 5.1: counterfactual outcome changes natural treatment distribution shifted simple stochastic intervention\n","code":""},{"path":"stochastic-treatment-regimes.html","id":"evaluating-the-causal-effect-of-a-stochastic-intervention","chapter":"9 Stochastic Treatment Regimes","heading":"9.6 Evaluating the Causal Effect of a Stochastic Intervention","text":"start, let’s load packages ’ll using throughout simple data exampleWe need estimate two components likelihood order construct TML\nestimator. first components outcome regression,\n\\(\\overline{Q}_n\\), simple regression form \\(\\E[Y \\mid ,W]\\). \nestimate quantity may constructed using Super Learner\nalgorithm. construct components sl3-style Super Learner \nregression , using small variety parametric nonparametric\nregression techniques:second estimate treatment mechanism, \\(g_n\\), .e., \ngeneralized propensity score. case continuous intervention node\n\\(\\), quantity takes form \\(p(\\mid W)\\), conditional\ndensity. Generally speaking, conditional density estimation challenging\nproblem received much attention literature. estimate \ntreatment mechanism, must make use learning algorithms specifically suited\nconditional density estimation; list learners may extracted \nsl3 using sl3_list_learners():proceed, ’ll select two learners, Lrnr_haldensify using\nhighly adaptive lasso conditional density estimation, based \nalgorithm given Dı́az van der Laan (2011) implemented Hejazi, Benkeser, van der Laan (2020), \nsemiparametric location-scale conditional density estimators implemented \nsl3 package. Super Learner may \nconstructed pooling estimates modified conditional density\nregression techniques (note exclude approach based \nhaldensify learner Super Learner account computationally\nintensive nature approach).Finally, construct learner_list object use constructing TML\nestimator target parameter interest:learner_list object specifies role ensemble\nlearners generated play computing initial estimators \nused building TMLE parameter interest . particular, \nmakes explicit fact Q_learner used fitting outcome\nregression g_learner used estimating treatment mechanism.","code":"\nlibrary(data.table)\nlibrary(haldensify)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3shift)\n# learners used for conditional mean of the outcome\nmean_lrnr <- Lrnr_mean$new()\nfglm_lrnr <- Lrnr_glm_fast$new()\nrf_lrnr <- Lrnr_ranger$new()\nhal_lrnr <- Lrnr_hal9001$new(max_degree = 3, n_folds = 3)\n\n# SL for the outcome regression\nsl_reg_lrnr <- Lrnr_sl$new(\n  learners = list(mean_lrnr, fglm_lrnr, rf_lrnr, hal_lrnr),\n  metalearner = Lrnr_nnls$new()\n)sl3_list_learners(\"density\")\n[1] \"Lrnr_density_discretize\"     \"Lrnr_density_hse\"           \n[3] \"Lrnr_density_semiparametric\" \"Lrnr_haldensify\"            \n[5] \"Lrnr_solnp_density\"         \n# learners used for conditional densities for (g_n)\nhaldensify_lrnr <- Lrnr_haldensify$new(\n  n_bins = c(5, 10, 20),\n  lambda_seq = exp(seq(-1, -10, length = 200))\n)\n# semiparametric density estimator with homoscedastic errors (HOSE)\nhose_hal_lrnr <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = hal_lrnr\n)\n# semiparametric density estimator with heteroscedastic errors (HESE)\nhese_rf_glm_lrnr <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = rf_lrnr,\n  var_learner = fglm_lrnr\n)\n\n# SL for the conditional treatment density\nsl_dens_lrnr <- Lrnr_sl$new(\n  learners = list(hose_hal_lrnr, hese_rf_glm_lrnr),\n  metalearner = Lrnr_solnp_density$new()\n)\nlearner_list <- list(Y = sl_reg_lrnr, A = sl_dens_lrnr)"},{"path":"stochastic-treatment-regimes.html","id":"example-with-simulated-data","chapter":"9 Stochastic Treatment Regimes","heading":"9.6.1 Example with Simulated Data","text":"composes observed data structure \\(O = (W, , Y)\\). formally\nexpress fact using tlverse grammar introduced tmle3 package,\ncreate single data object specify functional relationships \nnodes directed acyclic graph (DAG) via SCM, reflected \nnode list set .now observed data structure (data) specification role\nvariable data set plays nodes DAG.start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_val = 0.5 initializing \ntmle3_Spec object communicate ’re interested shift \\(0.5\\) \nscale treatment \\(\\) – , specify \\(\\delta = 0.5\\) (\narbitrarily chosen value example).seen , tmle_shift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"# simulate simple data for tmle-shift sketch\nn_obs <- 400 # number of observations\ntx_mult <- 2 # multiplier for the effect of W = 1 on the treatment\n\n## baseline covariates -- simple, binary\nW <- replicate(2, rbinom(n_obs, 1, 0.5))\n\n## create treatment based on baseline W\nA <- rnorm(n_obs, mean = tx_mult * W, sd = 1)\n\n## create outcome as a linear function of A, W + white noise\nY <- rbinom(n_obs, 1, prob = plogis(A + W))\n\n# organize data and nodes for tmle3\ndata <- data.table(W, A, Y)\nsetnames(data, c(\"W1\", \"W2\", \"A\", \"Y\"))\nnode_list <- list(\n  W = c(\"W1\", \"W2\"),\n  A = \"A\",\n  Y = \"Y\"\n)\nhead(data)\n   W1 W2         A Y\n1:  1  1  0.271651 1\n2:  0  0 -0.663368 1\n3:  0  0  0.113366 0\n4:  0  1 -0.732558 0\n5:  1  1  0.388835 1\n6:  0  0  0.043986 0\n# initialize a tmle specification\ntmle_spec <- tmle_shift(\n  shift_val = 0.5,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)"},{"path":"stochastic-treatment-regimes.html","id":"targeted-estimation-of-stochastic-interventions-effects","chapter":"9 Stochastic Treatment Regimes","heading":"9.6.2 Targeted Estimation of Stochastic Interventions Effects","text":"print method resultant tmle_fit object conveniently displays \nresults computing TML estimator \\(\\psi_n\\). standard error estimate\ncomputed based estimated EIF.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 548.8338     Pars:  0.94736 0.05264\nIter: 2 fn: 548.8338     Pars:  0.94736 0.05264\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.76372  0.76011 0.022838 0.71535 0.80488\n   psi_transformed lower_transformed upper_transformed\n1:         0.76011           0.71535           0.80488"},{"path":"stochastic-treatment-regimes.html","id":"selecting-stable-stochastic-interventions","chapter":"9 Stochastic Treatment Regimes","heading":"9.7 Selecting Stable Stochastic Interventions","text":"times, particular choice shift parameter \\(\\delta\\) may lead \npositivity violations downstream instability estimation process. \norder curb issues, can make choices \\(\\delta\\) based impact\ncandidate values estimator. Recall simplified expression \nauxiliary covariate TMLE \\(\\psi\\) \\(H = \\frac{g(- \\delta \\mid w)}{g(\\mid w)}\\), \\(g(- \\delta \\mid w)\\) defined stochastic\nintervention interest. can design stochastic intervention avoid\nviolations positivity assumption considering bound \\(C(\\delta) = \\frac{g(- \\delta \\mid w)}{g(\\mid w)} < M\\), \\(M\\) potentially\nuser-specified upper bound \\(C(\\delta)\\). Note \\(C(\\delta)\\) corresponds \ninverse weight assigned unit counterfactual treatment value \\(= + \\delta\\), natural treatment value \\(= \\), covariates \\(W = w\\). ,\n\\(C(\\delta)\\) can viewed measure influence given observation\nestimator \\(\\psi_n\\). limiting \\(C(\\delta)\\), whether choice\n\\(M\\) \\(\\delta\\), can limit potential instability estimator. \ncan formalize procedure defining new shift function \\(\\delta(, W)\\):\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & \\delta_{\\text{min}}(,w) \\leq \\delta \\leq\n        \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{max}}(,w), & \\delta \\geq \\delta_{\\text{max}}(,w) \\\\\n      \\delta_{\\text{min}}(,w), & \\delta \\leq \\delta_{\\text{min}}(,w) \\\\\n    \\end{cases},\n    \\tag{9.12}\n\\end{equation}\\]\n\\[\\delta_{\\text{max}}(, w) = \\text{argmax}_{\\left\\{\\delta \\geq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}\\] \n\\[\\delta_{\\text{min}}(, w) = \\text{argmin}_{\\left\\{\\delta \\leq 0,\n\\frac{g(- \\delta \\mid w)}{g(\\mid w)} \\leq M \\right\\}} \\frac{g(- \\delta\n\\mid w)}{g(\\mid w)}.\\]provides strategy implementing shift level given\nobservation \\((a_i, w_i)\\), thereby allowing observations shifted \nappropriate value, whether \\(\\delta_{\\text{min}}\\), \\(\\delta\\), \n\\(\\delta_{\\text{max}}\\). tmle3shift\npackage implements functions shift_additive_bounded \nshift_additive_bounded_inv, define variation strategy:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n  \\tag{9.13}\n\\end{equation}\\]\ncorresponding intervention natural value treatment \\(= \\)\nshifted value \\(\\delta\\) ratio \\(C(\\delta)\\) \npost-intervention density \\(g(- \\delta \\mid w)\\) natural treatment\ndensity \\(g(\\mid w)\\) exceed bound \\(M\\). \\(C(\\delta)\\) exceeds \nbound \\(M\\), stochastic intervention exempts given unit treatment\nmodification, leaving natural value treatment \\(= \\).","code":""},{"path":"stochastic-treatment-regimes.html","id":"initializing-vimshift-through-its-tmle3_spec","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.1 Initializing vimshift through its tmle3_Spec","text":"start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_grid = seq(-1, 1, = 1)\ninitializing tmle3_Spec object communicate ’re interested\nassessing mean counterfactual outcome grid shifts \\(\\delta \\\\{-1, 0, 1\\}\\) scale treatment \\(\\) (n.b., make arbitrary\nchoice shift values example).seen , tmle_vimshift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"\n# what's the grid of shifts we wish to consider?\ndelta_grid <- seq(-1, 1, 1)\n\n# initialize a tmle specification\ntmle_spec <- tmle_vimshift_delta(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)"},{"path":"stochastic-treatment-regimes.html","id":"targeted-estimation-of-stochastic-interventions-effects-1","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.2 Targeted Estimation of Stochastic Interventions Effects","text":"One may walk step--step procedure fitting TML estimator\nmean counterfactual outcome shift grid, using \nmachinery exposed tmle3 R package.One may invoke tmle3 wrapper function (user-facing convenience utility)\nfit series TML estimators (one parameter defined grid\ndelta) single function call:Remark: print method resultant tmle_fit object conveniently\ndisplays results computing TML estimator.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 547.4323     Pars:  0.9998973 0.0001027\nIter: 2 fn: 547.4323     Pars:  0.99997059 0.00002941\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n         type          param init_est tmle_est        se   lower   upper\n1:        TSM  E[Y_{A=NULL}]  0.56957  0.57130 0.0213472 0.52946 0.61314\n2:        TSM  E[Y_{A=NULL}]  0.69881  0.69750 0.0229958 0.65243 0.74257\n3:        TSM  E[Y_{A=NULL}]  0.81733  0.81516 0.0171125 0.78162 0.84870\n4: MSM_linear MSM(intercept)  0.69524  0.69465 0.0190943 0.65723 0.73208\n5: MSM_linear     MSM(slope)  0.12388  0.12193 0.0085622 0.10515 0.13871\n   psi_transformed lower_transformed upper_transformed\n1:         0.57130           0.52946           0.61314\n2:         0.69750           0.65243           0.74257\n3:         0.81516           0.78162           0.84870\n4:         0.69465           0.65723           0.73208\n5:         0.12193           0.10515           0.13871"},{"path":"stochastic-treatment-regimes.html","id":"estimation-and-inference-with-marginal-structural-models","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.3 Estimation and Inference with Marginal Structural Models","text":"can challenging select value shift parameter \\(\\delta\\) \nadvance. One solution specify grid shifts \\(\\delta\\) used\ndefining set related stochastic interventions (Hejazi, van der Laan, et al. 2020).\nconsider estimating counterfactual mean \\(\\psi_n\\) several\nchoices \\(\\delta\\), single summary measure estimated quantities can\nestablished working marginal structural models (MSMs). Summarizing\nestimates \\(\\psi_n\\) working MSM allows inference trend\nappearing grid \\(\\delta\\), may evaluating simple\nhypothesis test slope parameter \\(\\beta_0\\) working MSM. Consider \ngrid \\(\\delta\\), \\(\\{\\delta_1, \\ldots, \\delta_k\\}\\), corresponding \ncounterfactual means \\(\\{\\psi_{\\delta_1}, \\ldots, \\psi_{\\delta_k}\\}\\). Next, let\n\\(\\psi(\\delta) = (\\psi_{\\delta}: \\delta)\\) denote grid \ncounterfactual means grid defined \\(\\delta\\) let \\(\\psi_n(\\delta)\\)\ndenote TML estimators \\(\\psi(\\delta)\\). MSM summarizing change \n\\(\\psi_n\\) function \\(\\delta\\) may expressed \\(m_{\\beta}(\\psi_{\\delta}) = \\beta_0 + \\beta_1 \\delta\\). simple working model summarizes changes \n\\(\\psi_{\\delta}\\) function parameters \\((\\beta_0, \\beta_1)\\), \nlatter slope line resulting projecting counterfactual\nmeans onto simple two-parameter working model.general expression MSM \\(m_{\\beta}(\\delta)\\) \\(\\beta_0 = \\text{argmin}_{\\beta} \\sum_{\\delta}(\\psi_{\\delta}(P_0) - m_{\\beta}(\\delta))^2 h(\\delta)\\), solution estimating equation\n\\[u(\\beta, (\\psi_{\\delta}: \\delta)) = \\sum_{\\delta}h(\\delta)\n\\left(\\psi_{\\delta}(P_0) - m_{\\beta}(\\delta) \\right) \\frac{d}{d\\beta}\nm_{\\beta}(\\delta) = 0.\\]\n\nNow, say, \\(\\psi = (\\psi(\\delta): \\delta)\\) d-dimensional. may express \nEIF MSM parameter \\(\\beta_0\\) terms EIFs individual\ncounterfactual means:\n\\[\\begin{equation}\n   D_{\\beta}(O) = \\left(\\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta}\n   m_{\\beta}(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)^t \\right)^{-1}\n   \\sum_{\\delta} h(\\delta) \\frac{d}{d\\beta} m_{\\beta}(\\delta)\n   D_{\\psi_{\\delta}}(O).\n   \\tag{9.14}\n\\end{equation}\\]\n, Equation (9.14), first component dimension\n\\(d \\times d\\) second dimension \\(d \\times 1\\). , \nassume linear working MSM; however, analogous procedure may applied \nworking MSMs based GLMs., utilized straightforward application delta method obtain\nEIF \\(\\beta\\). Inference parameter working MSM follows \nevaluation EIF \\(D_{\\beta}\\), expressed terms EIFs \ncorresponding estimates \\(\\psi_n(\\delta)\\). limit distribution \n\\(\\beta_n\\) may expressed \\[\\sqrt{n}(\\beta_n - \\beta_0) \\N(0, \\Sigma),\\]\n\\(\\Sigma\\) empirical covariance matrix \\(D_{\\beta}(O)\\). ,\ncan estimate trend counterfactual means across \ngrid \\(\\delta\\), can also evaluate whether slope estimate \nstatistically significant, terms hypothesis tests form \\((H_0: \\beta_0 = 0; H_1: \\beta_0 \\neq 0)\\) equivalent Wald-style confidence\nintervals. Note estimator \\(\\beta_n\\) parameter \\(\\beta_0\\) \nMSM asymptotically linear (, fact, TML estimator) consequence \nconstruction individual TML estimators.strategy just discussed constructs estimate \\(\\beta_n\\) working MSM\nslope \\(\\beta_0\\) first evaluating TML estimates counterfactual\nmeans \\(\\psi_{n,\\delta}\\) grid \\(\\{\\delta_1, \\ldots, \\delta_k\\}\\); however,\nnecessarily best strategy, especially giving consideration\nestimation stability small samples. smaller samples, may prudent\nperform TML estimation targeting directly parameter \\(\\beta_0\\), opposed\nconstructing applying delta method several independently\ntargeted TML estimates., consider TML estimator targeting \\(\\beta_0\\) (parameter \nworking MSM \\(m_{\\beta}\\)), uses targeting update step form\n\\(\\overline{Q}_{n, \\epsilon}(,W) = \\overline{Q}_n(,W) + \\epsilon (H_{\\beta_0}(g), H_{\\beta_1}(g))\\), \\(\\delta\\), \\(H_{\\beta_0}(g)\\) \nauxiliary covariate \\(\\beta_0\\) (intercept) \\(H_{\\beta}(g)\\) \nauxiliary covariate \\(\\beta_1\\) (slope). Note forms \nauxiliary covariates depend EIF \\(D_{\\beta}\\). TML estimator avoids\nestimating \\(\\psi_{\\delta}\\) grid directly, instead cleverly\nconcatenating auxiliary covariates appropriate \\(\\beta_0\\)\n\\(\\beta_1\\). construct targeted maximum likelihood estimator \ndirectly targets parameters working MSM, may use \ntmle_vimshift_msm Spec (instead tmle_vimshift_delta Spec).","code":"\n# initialize a tmle specification\ntmle_msm_spec <- tmle_vimshift_msm(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)\n\n# fit the TML estimator and examine the results\ntmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)\ntmle_msm_fit"},{"path":"stochastic-treatment-regimes.html","id":"example-with-the-wash-benefits-data","chapter":"9 Stochastic Treatment Regimes","heading":"9.7.4 Example with the WASH Benefits Data","text":"complete walk , let’s turn using stochastic interventions \ninvestigate data WASH Benefits trial. start, let’s load \ndata, convert columns class numeric, take quick look itNext, specify NPSEM via node_list object. example analysis,\n’ll consider outcome weight--height Z-score (previous\nchapters), intervention interest mother’s age time \nchild’s birth, take covariates potential confounders.consider counterfactual weight--height Z-score shifts \nage mother child’s birth, interpret estimates \nparameter? simplify interpretation, consider shift just year \nmother’s age (.e., \\(\\delta = 1\\)); setting, stochastic\nintervention correspond policy advocating potential mothers\ndefer child single calendar year, possibly implemented \nencouragement design deployed clinical setting.example, ’ll use variable importance strategy considering \ngrid stochastic interventions evaluate weight--height Z-score \nshift mother’s age two years (\\(\\delta = -2\\)) two years\n(\\(\\delta = 2\\)). , simply initialize Spec tmle_vimshift_delta\njust previous example:Prior running analysis, ’ll modify learner_list object \ncreated density estimation procedure rely \nlocation-scale conditional density estimation procedure, nonparametric\nconditional density approach based highly adaptive lasso (Dı́az van der Laan 2011; Benkeser van der Laan 2016; Coyle, Hejazi, van der Laan 2020; Hejazi, Coyle, van der Laan 2020; Hejazi, Benkeser, van der Laan 2020)\ncurrently unable accommodate larger datasets.made preparations, ’re now ready estimate \ncounterfactual mean weight--height Z-score small grid \nshifts mother’s age child’s birth. Just , \nsimple call tmle3 wrapper function:","code":"washb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nwashb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]\nhead(washb_data, 3)\n     whz tr fracode month aged sex momage momedu momheight hfiacat Nlt18 Ncomp\n1:  0.00  1       4     9  268   2     30      2    146.40       1     3    11\n2: -1.16  1       4     9  286   2     25      2    148.75       3     2     4\n3: -1.05  1      20     9  264   2     25      2    152.15       1     1    10\n   watmin elec floor walls roof asset_wardrobe asset_table asset_chair\n1:      0    1     0     1    1              0           1           1\n2:      0    1     0     1    1              0           1           0\n3:      0    0     0     1    1              0           0           1\n   asset_khat asset_chouki asset_tv asset_refrig asset_bike asset_moto\n1:          1            0        1            0          0          0\n2:          1            1        0            0          0          0\n3:          0            1        0            0          0          0\n   asset_sewmach asset_mobile\n1:             0            1\n2:             0            1\n3:             0            1\nnode_list <- list(\n  W = names(washb_data)[!(names(washb_data) %in%\n    c(\"whz\", \"momage\"))],\n  A = \"momage\",\n  Y = \"whz\"\n)\n# initialize a tmle specification for the variable importance parameter\nwashb_vim_spec <- tmle_vimshift_delta(\n  shift_grid = c(-2, 2),\n  max_shifted_ratio = 2\n)\n# we need to turn on cross-validation for the HOSE learner\ncv_hose_hal_lrnr <- Lrnr_cv$new(\n  learner = hose_hal_lrnr,\n  full_fit = TRUE\n)\n\n# modify learner list, using existing SL for Q fit\nlearner_list <- list(Y = sl_reg_lrnr, A = cv_hose_hal_lrnr)\nwashb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)\nwashb_tmle_fit"},{"path":"stochastic-treatment-regimes.html","id":"exercises-3","chapter":"9 Stochastic Treatment Regimes","heading":"9.8 Exercises","text":"","code":""},{"path":"stochastic-treatment-regimes.html","id":"the-ideas-in-action-1","chapter":"9 Stochastic Treatment Regimes","heading":"9.8.1 The Ideas in Action","text":"Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate \ncounterfactual mean mother’s age child’s birth (momage) \nshift \\(\\delta = 0\\). counterfactual mean equate terms\nobserved data?Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate \ncounterfactual mean mother’s age child’s birth (momage) \nshift \\(\\delta = 0\\). counterfactual mean equate terms\nobserved data?Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable chosen preceding\nquestion, summarizing trend sequence shifts using marginal\nstructural model.Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable chosen preceding\nquestion, summarizing trend sequence shifts using marginal\nstructural model.Repeat preceding analysis, using grid shifts, instead\ndirectly targeting parameters marginal structural model.\nInterpret results – , slope marginal\nstructural model tell us trend across chosen sequence \nshifts?Repeat preceding analysis, using grid shifts, instead\ndirectly targeting parameters marginal structural model.\nInterpret results – , slope marginal\nstructural model tell us trend across chosen sequence \nshifts?","code":""},{"path":"stochastic-treatment-regimes.html","id":"review-of-key-concepts-2","chapter":"9 Stochastic Treatment Regimes","heading":"9.8.2 Review of Key Concepts","text":"Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.can information provided estimates across several shifts\n\\(\\{ \\delta_1, \\ldots, \\delta_k \\}\\) marginal structural model\nparameter summarizing trend \\(\\delta\\) used enrich \ninterpretation findings?can information provided estimates across several shifts\n\\(\\{ \\delta_1, \\ldots, \\delta_k \\}\\) marginal structural model\nparameter summarizing trend \\(\\delta\\) used enrich \ninterpretation findings?advantages, , targeting directly parameters \nmarginal structural model?advantages, , targeting directly parameters \nmarginal structural model?","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis","chapter":"10 Causal Mediation Analysis","heading":"10 Causal Mediation Analysis","text":"Nima HejaziBased tmle3mediate R\npackage Nima Hejazi, James\nDuncan, David McCoy.Updated: 2021-12-22","code":""},{"path":"causal-mediation-analysis.html","id":"learning-objectives-6","chapter":"10 Causal Mediation Analysis","heading":"Learning Objectives","text":"Examine presence post-treatment mediating variables can complicate\ncausal analysis, direct indirect effects can defined \nresolve complications.Describe essential similarities differences direct \nindirect causal effects, including definition terms stochastic\ninterventions.Differentiate joint interventions required define direct indirect\neffects static, dynamic, stochastic interventions yield\ntotal causal effects.Describe assumptions needed identification natural direct \nindirect effects, well limitations effect definitions.Estimate natural direct indirect effects binary treatment using\ntmle3mediate R package.Differentiate population intervention direct indirect effects \nstochastic interventions natural direct indirect effects,\nincluding differences assumptions required identification.Estimate population intervention direct effect binary treatment\nusing tmle3mediate R package.","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis-1","chapter":"10 Causal Mediation Analysis","heading":"10.1 Causal Mediation Analysis","text":"applications ranging biology epidemiology economics \npsychology, scientific inquires often concerned ascertaining effect\ntreatment outcome variable particular pathways \ntwo. presence post-treatment intermediate variables affected \nexposure (, mediators), path-specific effects allow complex,\nmechanistic relationships teased apart. causal effects \nwide interest definition identification object \nstudy statistics nearly century – indeed, earliest examples \nmodern causal mediation analysis can traced back work path analysis\n(Wright 1934). recent decades, renewed interest resulted \nformulation novel direct indirect effects within potential\noutcomes nonparametric structural equation modeling frameworks\n(J. M. Robins 1986; Pearl 1995, 2009; Spirtes et al. 2000; Dawid 2000). Generally, indirect effect (IE) portion \ntotal effect found work mediating variables, direct\neffect (DE) encompasses components total effect, including\neffect treatment directly outcome effect\npaths explicitly involving mediators. mechanistic\nknowledge conveyed direct indirect effects can used improve\nunderstanding treatments may efficacious.Modern approaches causal inference allowed significant advances \nmethodology traditional path analysis, overcoming significant\nrestrictions imposed use parametric modeling approaches\n(VanderWeele 2015). Using distinct frameworks,\nRobins Greenland (1992) Pearl (2001) provided equivalent\nnonparametric decompositions average treatment effect natural\ndirect indirect effects. VanderWeele (2015) provides \ncomprehensive overview classical causal mediation analysis. provide \nalternative perspective, focusing instead construction efficient\nestimators quantities, appeared recently\n(Tchetgen Tchetgen Shpitser 2012; Zheng van der Laan 2012), well flexible\ndirect indirect definitions based upon stochastic interventions\n(Dı́az Hejazi 2020).","code":""},{"path":"causal-mediation-analysis.html","id":"data-structure-and-notation-2","chapter":"10 Causal Mediation Analysis","heading":"10.2 Data Structure and Notation","text":"Let us return familiar sample \\(n\\) units \\(O_1, \\ldots, O_n\\), \nnow consider slightly complex data structure \\(O = (W, , Z, Y)\\) \ngiven observational unit. , \\(W\\) represents vector observed\ncovariates, \\(\\) binary continuous treatment, \\(Y\\) binary continuous\noutcome; new post-treatment variable \\(Z\\) represents (possibly\nmultivariate) set mediators. Avoiding assumptions unsupported background\nscientific knowledge, assume \\(O \\sim P_0 \\\\M\\), \\(\\M\\) \nnonparametric statistical model places assumptions form \ndata-generating distribution \\(P_0\\).preceding chapters, structural causal model (SCM) (Pearl 2009)\nhelps formalize definition counterfactual variables:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Z &= f_Z(W, , U_Z) \\\\\n  Y &= f_Y(W, , Z, U_Y).\n  \\tag{10.1}\n\\end{align}\\]\nset equations\nconstitutes mechanistic model generating observed data \\(O\\); furthermore,\nSCM encodes several fundamental assumptions. Firstly, implicit\ntemporal ordering: \\(W\\) occurs first, depending exogenous factors \\(U_W\\);\n\\(\\) happens next, based \\(W\\) exogenous factors \\(U_A\\); come \nmediators \\(Z\\), depend \\(\\), \\(W\\), another set exogenous factors\n\\(U_Z\\); finally appears outcome \\(Y\\). assume neither access set\nexogenous factors \\(\\{U_W, U_A, U_Z, U_Y\\}\\) knowledge forms \ndeterministic generating functions \\(\\{f_W, f_A, f_Z, f_Y\\}\\). practice, \navailable knowledge data-generating experiment incorporated\n– example, data randomized controlled trial (RCT), form\n\\(f_A\\) may known. SCM corresponds following DAG:factorizing likelihood data \\(O\\), can express \\(p_0^O\\), \ndensity \\(O\\) respect product measure, evaluated \nparticular observation \\(o\\), terms several orthogonal components:\n\\[\\begin{align}\n  p_0(x) = &q_{0,Y}(y \\mid Z = z, = , W = w) \\cdot \\\\ \\nonumber\n    &q_{0,Z}(z \\mid = , W = w) \\cdot \\\\ \\nonumber\n    &g_{0,}(\\mid W = w) \\cdot \\\\ \\nonumber\n    &q_{0,W}(w).\\\\ \\nonumber\n  \\tag{10.2}\n\\end{align}\\]\nEquation (10.2), \\(q_{0, Y}\\) \nconditional density \\(Y\\) given \\(\\{Z, , W\\}\\), \\(q_{0, Z}\\) conditional\ndensity \\(Z\\) given \\(\\{, W\\}\\), \\(g_{0, }\\) conditional density \\(\\)\ngiven \\(W\\), \\(q_{0, W}\\) marginal density \\(W\\). convenience \nconsistency notation, define \\(\\overline{Q}_Y(Z, , W) := \\E[Y \\mid Z, , W]\\), \\(Q_Z(Z \\mid , W) := \\P[Z \\mid , W]\\), \n\\(g(\\mid W) := \\P(\\mid W)\\).Finally, note explicitly excluded potential confounders \nmediator-outcome relationship affected exposure (.e., variables affected \n\\(\\) affecting \\(Z\\) \\(Y\\)). Mediation analysis presence \nvariables exceptionally challenging (Avin, Shpitser, Pearl 2005); thus, \nefforts develop definitions causal direct indirect effects explicitly\ndisallowed form confounding. refrain discussing \nmatter , interested reader may consult recent advances vast\nliterature causal mediation analysis, including interventional direct \nindirect effects (Didelez, Dawid, Geneletti 2006; VanderWeele, Vansteelandt, Robins 2014; Lok 2016; Vansteelandt Daniel 2017; Rudolph et al. 2017; Nguyen, Schmid, Stuart 2019),\nwhose identification robust complex form post-treatment (\nintermediate) confounding. Within thread literature,\nDı́az et al. (2020) Benkeser Ran (2021) provide considerations \nnonparametric effect decompositions efficiency theory, \nHejazi, Rudolph, et al. (2021) formulate novel class effects utilizing stochastic\ninterventions.","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Z\n    W -> Y\n    A -> Z\n    A -> Y\n    Z -> Y\n    W -> A -> Y\n    W -> A -> Z -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"causal-mediation-analysis.html","id":"defining-the-natural-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.3 Defining the Natural Direct and Indirect Effects","text":"","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-average-treatment-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.1 Decomposing the Average Treatment Effect","text":"natural direct indirect effects arise decomposition ATE:\n\\[\\begin{equation*}\n  \\E[Y(1) - Y(0)] =\n    \\underbrace{\\E[Y(1, Z(0)) - Y(0, Z(0))]}_{\\text{NDE}} +\n    \\underbrace{\\E[Y(1, Z(1)) - Y(1, Z(0))]}_{\\text{NIE}}.\n\\end{equation*}\\]\nparticular, natural indirect effect (NIE) measures effect \ntreatment \\(\\\\{0, 1\\}\\) outcome \\(Y\\) mediators \\(Z\\), \nnatural direct effect (NDE) measures effect treatment \noutcome paths. Identification natural direct \nindirect effects requires following non-testable causal assumptions. Note\nstandard assumptions consistency interference (.e., SUTVA\n(Rubin 1978, 1980)) hold owing fact \nSCM consider restricted give rise independent \nidentically distributed units \\(O\\).Exchangeability: \\(Y(, z) \\indep (, Z) \\mid W\\), implies \n\\(\\E\\{Y(, z) \\mid =, W=w, Z=z\\} \\equiv \\E\\{Y(, z) \\mid W=w\\}\\). \nspecial, restrictive case standard assumption unmeasured\ncounfounding presence mediators. analogous randomization\nassumption requires randomization treatment mediators.Treatment positivity: \\(\\\\mathcal{}\\) \\(w \\\\mathcal{W}\\),\nconditional probability treatment \\(g(\\mid w)\\) bounded away \nlimits unit interval small factor \\(\\xi > 0\\). precisely,\n\\(\\xi < g(\\mid w) < 1 - \\xi\\). mirrors standard positivity\nassumption required static interventions, discussed previously.Mediator positivity: \\(z \\\\mathcal{Z}\\), \\(\\\\mathcal{}\\), \n\\(w \\\\mathcal{W}\\), conditional mediator density must bounded away\nzero small factor \\(\\epsilon > 0\\), specifically, \\(\\epsilon < Q_Z(z \\mid , w)\\). Essentially, requires conditional mediator density\nbounded away zero \\(\\{z, , w\\}\\) joint support\n\\(\\mathcal{Z} \\times \\mathcal{} \\times \\mathcal{W}\\), say \nmust possible observe given mediator value across strata.Cross-world counterfactual independence: \\(\\neq '\\), \\(, ' \\\\mathcal{}\\), \\(z \\\\mathcal{Z}\\), \\(Y(', z)\\) must independent \n\\(Z()\\), given \\(W\\). , counterfactual outcome treatment\ncontrast \\(' \\\\mathcal{}\\) counterfactual mediator value \\(Z() \\\\mathcal{Z}\\) (alternative contrast \\(\\\\mathcal{}\\)) must \nindependent. term “cross-world” refers two counterfactuals \\(Z()\\)\n\\(Y(', z)\\) existing incompatible treatment contrasts. Though \njoint distributions counterfactuals well-defined, \njointly realized.first three assumptions may familiar based simpler analogs\ndiscussed preceding chapters, last assumption unique NDE \nNIE. cross-world counterfactual independence (.e., independence \ncounterfactuals indexed distinct interventions) , fact, serious\nlimitation scientific relevance effect definitions, NDE\nNIE unidentifiable randomized trials (Robins Richardson 2010),\ndirectly implying corresponding scientific claims falsified\nexperimentation (Popper 1934; Dawid 2000), key pillar \nscientific method. many attempts made weaken last\nassumption (Petersen, Sinisi, van der Laan 2006; Imai, Keele, Yamamoto 2010; Vansteelandt, Bekaert, Lange 2012; Vansteelandt VanderWeele 2012), results either\nimpose stringent modeling assumptions, propose alternative interpretations \nnatural effects, provide limited degree additional flexibility \ndeveloping alternative estimation strategies. motivated reader may wish \nexamine details independently. next review estimation \nNDE NIE, remain widely used modern applications causal mediation\nanalysis.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.2 Estimating the Natural Direct Effect","text":"NDE defined \n\\[\\begin{align*}\n  \\Psi_{NDE} =& \\E[Y(1, Z(0)) - Y(0, Z(0))] \\\\\n  \\overset{\\text{rand.}}{=}& \\sum_w \\sum_z\n  [\\underbrace{\\E(Y \\mid = 1, z, w)}_{\\bar{Q}_Y(= 1, z, w)} -\n  \\underbrace{\\E(Y \\mid = 0, z, w)}_{\\bar{Q}_Y(= 0, z, w)}] \\\\&\\times\n  \\underbrace{p(z \\mid = 0, w)}_{q_Z(Z \\mid 0, w))}\n  \\underbrace{p(w)}_{q_W},\n\\end{align*}\\]\nlikelihood factors \\(p(z \\mid = 0, w)\\) \\(p(w)\\) (among \nconditional densities) arise factorization joint likelihood:\n\\[\\begin{equation*}\n  p(w, , z, y) = \\underbrace{p(y \\mid w, , z)}_{q_Y(, W, Z)}\n  \\underbrace{p(z \\mid w, )}_{q_Z(Z \\mid , W)}\n  \\underbrace{p(\\mid w)}_{g(\\mid W)}\n  \\underbrace{p(w)}_{q_W}.\n\\end{equation*}\\]process estimating NDE begins constructing \\(\\overline{Q}_{Y, n}\\),\nestimate conditional mean outcome, given \\(Z\\), \\(\\), \\(W\\).\nestimate conditional mean hand, predictions \ncounterfactual quantities \\(\\overline{Q}_Y(Z, 1, W)\\) (setting \\(= 1\\)) ,\nlikewise, \\(\\overline{Q}_Y(Z, 0, W)\\) (setting \\(= 0\\)) readily obtained. \ndenote difference quantities \\(\\overline{Q}_{\\text{diff}} = \\overline{Q}_Y(Z, 1, W) - \\overline{Q}_Y(Z, 0, W)\\). \\(\\bar{Q}_{\\text{diff}}\\)\nrepresents difference conditional mean \\(Y\\) across contrasts \n\\(\\).estimation procedure treats \\(\\overline{Q}_{\\text{diff}}\\) \nnuisance parameter, regressing estimate \\(\\overline{Q}_{\\text{diff}, n}\\) \n\\(W\\), among control observations (.e., \\(= 0\\) observed);\ngoal step remove part marginal impact \\(Z\\) \n\\(\\overline{Q}_{\\text{diff}}\\), since \\(W\\) parent \\(Z\\). Regressing \ndifference \\(W\\) among controls recovers expected\n\\(\\overline{Q}_{\\text{diff}}\\), individuals set control\ncondition \\(= 0\\). residual additive effect \\(Z\\) \n\\(\\overline{Q}_{\\text{diff}}\\) removed TML estimation step using \nauxiliary (“clever”) covariate, accounts mediators \\(Z\\). \nauxiliary covariate takes form\n\\[\\begin{equation*}\n  C_Y(Q_Z, g)(O) = \\Bigg\\{\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\n  \\frac{Q_Z(Z \\mid 0, W)}{Q_Z(Z \\mid 1, W)} -\n  \\frac{\\mathbb{}(= 0)}{g(0 \\mid W)} \\Bigg\\}.\n\\end{equation*}\\]\nBreaking , \\(\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\\) inverse\npropensity score weight \\(= 1\\) , likewise, \\(\\frac{\\mathbb{}(= 0)} {g(0 \\mid W)}\\) inverse propensity score weight \\(= 0\\). middle\nterm ratio conditional densities mediator control\n(\\(= 0\\)) treatment (\\(= 1\\)) conditions.subtle appearance ratio conditional densities concerning –\ntools estimate quantities sparse statistics literature,\nunfortunately, problem still complicated (computationally\ntaxing) \\(Z\\) high-dimensional. ratio conditional\ndensities required, convenient re-parametrization may achieved, ,\n\\[\\begin{equation*}\n  \\frac{p(= 0 \\mid Z, W) g(0 \\mid W)}{p(= 1 \\mid Z, W) g(1 \\mid W)}.\n\\end{equation*}\\]\nGoing forward, denote re-parameterized conditional probability\n\\(e(\\mid Z, W) := p(\\mid Z, W)\\). Similar re-parameterizations used\nZheng van der Laan (2012) Tchetgen Tchetgen (2013). particularly useful\nsince reformulation reduces problem one concerning \nestimation conditional means, opening door use wide range \nmachine learning algorithms.Underneath hood, counterfactual outcome difference\n\\(\\bar{Q}_{\\text{diff}}\\) \\(e(\\mid Z, W)\\), conditional probability \\(\\)\ngiven \\(Z\\) \\(W\\), used constructing auxiliary covariate TML\nestimation. nuisance parameters play important role \nbias-correcting update step TMLE procedure.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-indirect-effect","chapter":"10 Causal Mediation Analysis","heading":"10.3.3 Estimating the Natural Indirect Effect","text":"Derivation estimation NIE analogous NDE. Recall\nNIE effect \\(\\) \\(Y\\) mediator(s) \\(Z\\).\nquantity, may expressed \\(\\E(Y(Z(1), 1) - \\E(Y(Z(0), 1)\\),\ncorresponds difference conditional mean \\(Y\\) given \\(= 1\\) \n\\(Z(1)\\) (values mediator take \\(= 1\\)) conditional\nexpectation \\(Y\\) given \\(= 1\\) \\(Z(0)\\) (values mediator take\n\\(= 0\\)).NDE, re-parameterization can used estimate \\(\\E(\\mid Z, W)\\), avoiding estimation possibly multivariate conditional density.\nHowever, case, mediated mean outcome difference, denoted\n\\(\\Psi_Z(Q)\\), instead estimated follows\n\\[\\begin{equation*}\n  \\Psi_{\\text{NIE}}(Q) = \\E (\\Psi_{\\text{NIE}, Z}(Q)(1, W) -\n  \\Psi_{\\text{NIE}, Z}(Q)(0, W))\n\\end{equation*}\\], \\(\\bar{Q}_Y(Z, 1, W)\\), predicted values \\(Y\\) given \\(Z\\) \\(W\\) \n\\(= 1\\), regressed \\(W\\), among treated units (.e., \\(= 1\\) observed) obtain conditional mean \\(\\Psi_{\\text{NIE}, Z}(Q)(1, W)\\).\nPerforming procedure, instead regressing \\(\\bar{Q}_Y(Z, 1, W)\\) \\(W\\)\namong control units (.e., \\(= 0\\) observed) yields\n\\(\\Psi_{\\text{NIE},Z}(Q)(0, W)\\). difference two estimates NIE\ncan thought additive marginal effect treatment \nconditional mean \\(Y\\) given \\(\\{W, = 1, Z\\}\\) effect \\(Z\\). ,\ncase NIE, estimate \\(\\psi_n\\) slightly different, \nquantity \\(e(\\mid Z, W)\\) comes play auxiliary covariate.","code":""},{"path":"causal-mediation-analysis.html","id":"the-population-intervention-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.4 The Population Intervention Direct and Indirect Effects","text":"times, natural direct indirect effects may prove limiting, \neffect definitions based static interventions (.e., setting\n\\(= 0\\) \\(= 1\\)), may unrealistic real-world interventions. \ncases, one may turn instead population intervention direct effect\n(PIDE) population intervention indirect effect (PIIE), based\ndecomposing effect population intervention effect (PIE) \nflexible stochastic interventions (Dı́az Hejazi 2020).particular type stochastic intervention well-suited working binary\ntreatments incremental propensity score intervention (IPSI), first\nproposed Kennedy (2019). interventions \ndeterministically set treatment level observed unit fixed\nquantity (.e., setting \\(= 1\\)), instead alter odds receiving \ntreatment fixed amount (\\(0 \\leq \\delta \\leq \\infty\\)) individual.\nparticular, intervention takes form\n\\[\\begin{equation*}\n  g_{\\delta}(1 \\mid w) = \\frac{\\delta g(1 \\mid w)}{\\delta g(1 \\mid w) + 1\n  - g(1\\mid w)},\n\\end{equation*}\\]\nscalar \\(0 < \\delta < \\infty\\) specifies change odds \nreceiving treatment. described Dı́az Hejazi (2020), stochastic\nintervention special case exponential tilting, framework unifies\npost-intervention treatment values draws altered distribution.Unlike natural direct indirect effects, conditions required \nidentifiability population intervention direct indirect effects \nlax. importantly, differences involve (1) treatment positivity\nassumption requires counterfactual treatment \nobserved support treatment \\(\\mathcal{}\\), (2) requirement \nindependence cross-world counterfactuals.","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-population-intervention-effect","chapter":"10 Causal Mediation Analysis","heading":"10.4.1 Decomposing the Population Intervention Effect","text":"may decompose population intervention effect (PIE) terms \npopulation intervention direct effect (PIDE) population\nintervention indirect effect (PIIE):\n\\[\\begin{equation*}\n  \\mathbb{E}\\{Y(A_\\delta)\\} - \\mathbb{E}Y =\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z(A_\\delta))\n      - Y(A_\\delta, Z)\\}}^{\\text{PIIE}} +\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z) - Y(, Z)\\}}^{\\text{PIDE}}.\n\\end{equation*}\\]decomposition PIE sum population intervention direct\nindirect effects interpretation analogous corresponding\nstandard decomposition average treatment effect. sequel, \ncompute components direct indirect effects using\nappropriate estimators followsFor \\(\\mathbb{E}\\{Y(, Z)\\}\\), sample mean \\(\\frac{1}{n}\\sum_{=1}^n Y_i\\) \nconsistent;\\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\), TML estimator effect \njoint intervention altering treatment mechanism mediation\nmechanism, based proposal Dı́az Hejazi (2020); ,\\(\\mathbb{E}\\{Y(A_{\\delta}, Z_{A_{\\delta}})\\}\\), efficient estimator \neffect joint intervention altering treatment mediation\nmechanisms, proposed Kennedy (2019) implemented \nnpcausal R package.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-effect-decomposition-term","chapter":"10 Causal Mediation Analysis","heading":"10.4.2 Estimating the Effect Decomposition Term","text":"described Dı́az Hejazi (2020), statistical functional identifying \ndecomposition term appears PIDE PIIE\n\\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\), corresponds altering treatment\nmechanism keeping mediation mechanism fixed, \n\\[\\begin{equation*}\n  \\theta_0(\\delta) = \\int m_0(, z, w) g_{0,\\delta}(\\mid w) p_0(z, w)\n    d\\nu(, z, w),\n\\end{equation*}\\]\nTML estimator available. corresponding efficient influence\nfunction (EIF) respect nonparametric model \\(\\mathcal{M}\\) \n\\(D_{\\eta,\\delta}(o) = D^Y_{\\eta,\\delta}(o) + D^A_{\\eta,\\delta}(o) + D^{Z,W}_{\\eta,\\delta}(o) - \\theta(\\delta)\\).TML estimator may computed basd EIF estimating equation may\nincorporate cross-validation (Zheng van der Laan 2011; Chernozhukov et al. 2018) \ncircumvent possibly restrictive entropy conditions (e.g., Donsker class). \nresultant estimator \n\\[\\begin{equation*}\n  \\hat{\\theta}(\\delta) = \\frac{1}{n} \\sum_{= 1}^n D_{\\hat{\\eta}_{j()},\n  \\delta}(O_i) = \\frac{1}{n} \\sum_{= 1}^n \\left\\{ D^Y_{\\hat{\\eta}_{j()},\n  \\delta}(O_i) + D^A_{\\hat{\\eta}_{j()}, \\delta}(O_i) +\n  D^{Z,W}_{\\hat{\\eta}_{j()}, \\delta}(O_i) \\right\\},\n\\end{equation*}\\]\nimplemented tmle3mediate (one-step estimator also avaialble,\nmedshift R package). \ndemonstrate use tmle3mediate obtain \\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\)\nvia TML estimator.","code":""},{"path":"causal-mediation-analysis.html","id":"evaluating-the-direct-and-indirect-effects","chapter":"10 Causal Mediation Analysis","heading":"10.5 Evaluating the Direct and Indirect Effects","text":"now turn estimating natural direct indirect effects, well \npopulation intervention direct effect, using WASH Benefits data,\nintroduced earlier chapters. Let’s first load data:’ll next define baseline covariates \\(W\\), treatment \\(\\), mediators \\(Z\\),\noutcome \\(Y\\) nodes NPSEM via “Node List” object:node_list encodes parents node – example, \\(Z\\) (\nmediators) parents \\(\\) (treatment) \\(W\\) (baseline confounders),\n\\(Y\\) (outcome) parents \\(Z\\), \\(\\), \\(W\\). ’ll also handle \nmissingness data invoking process_missing:’ll now construct ensemble learner using handful popular machine\nlearning algorithms:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mediate)\n\n# download data\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# make intervention node binary and subsample\nwashb_data <- washb_data[sample(.N, 600), ]\nwashb_data[, tr := as.numeric(tr != \"Control\")]\nnode_list <- list(\n  W = c(\n    \"momage\", \"momedu\", \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\"\n  ),\n  A = \"tr\",\n  Z = c(\"sex\", \"month\", \"aged\"),\n  Y = \"whz\"\n)\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list\n# SL learners used for continuous data (the nuisance parameter Z)\nenet_contin_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"gaussian\", nfolds = 3\n)\nlasso_contin_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"gaussian\", nfolds = 3\n)\nfglm_contin_learner <- Lrnr_glm_fast$new(family = gaussian())\nmean_learner <- Lrnr_mean$new()\ncontin_learner_lib <- Stack$new(\n  enet_contin_learner, lasso_contin_learner, fglm_contin_learner, mean_learner\n)\nsl_contin_learner <- Lrnr_sl$new(learners = contin_learner_lib)\n\n# SL learners used for binary data (nuisance parameters G and E in this case)\nenet_binary_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"binomial\", nfolds = 3\n)\nlasso_binary_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"binomial\", nfolds = 3\n)\nfglm_binary_learner <- Lrnr_glm_fast$new(family = binomial())\nbinary_learner_lib <- Stack$new(\n  enet_binary_learner, lasso_binary_learner, fglm_binary_learner, mean_learner\n)\nsl_binary_learner <- Lrnr_sl$new(learners = binary_learner_lib)\n\n# create list for treatment and outcome mechanism regressions\nlearner_list <- list(\n  Y = sl_contin_learner,\n  A = sl_binary_learner\n)"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-indirect-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.1 Targeted Estimation of the Natural Indirect Effect","text":"demonstrate calculation NIE , starting instantiating “Spec”\nobject encodes exactly learners use nuisance parameters\n\\(e(\\mid Z, W)\\) \\(\\Psi_Z\\). pass Spec object tmle3\nfunction, alongside data, node list (created ), learner list\nindicating machine learning algorithms use estimating nuisance\nparameters based \\(\\) \\(Y\\).Based output, conclude indirect effect treatment\nmediators (sex, month, aged) \n0.00266.","code":"tmle_spec_NIE <- tmle_NIE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NIE <- tmle3(\n  tmle_spec_NIE, washb_data, node_list, learner_list\n)\nwashb_NIE\nA tmle3_Fit that took 1 step(s)\n   type                  param  init_est  tmle_est       se     lower    upper\n1:  NIE NIE[Y_{A=1} - Y_{A=0}] 0.0022912 0.0026608 0.044295 -0.084156 0.089478\n   psi_transformed lower_transformed upper_transformed\n1:       0.0026608         -0.084156          0.089478"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.2 Targeted Estimation of the Natural Direct Effect","text":"analogous procedure applies estimation NDE, replacing \nSpec object NIE tmle_spec_NDE define learners NDE\nnuisance parameters:, can draw conclusion direct effect treatment\n(paths involving mediators (sex, month, aged)) \n0.01298. Note , together, estimates \nnatural direct indirect effects approximately recover average\ntreatment effect, , based estimates NDE NIE, \nATE roughly\n0.01564.","code":"tmle_spec_NDE <- tmle_NDE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NDE <- tmle3(\n  tmle_spec_NDE, washb_data, node_list, learner_list\n)\nwashb_NDE\nA tmle3_Fit that took 1 step(s)\n   type                  param init_est tmle_est      se   lower   upper\n1:  NDE NDE[Y_{A=1} - Y_{A=0}] 0.012983 0.012983 0.10285 -0.1886 0.21457\n   psi_transformed lower_transformed upper_transformed\n1:        0.012983           -0.1886           0.21457"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-population-intervention-direct-effect","chapter":"10 Causal Mediation Analysis","heading":"10.5.3 Targeted Estimation of the Population Intervention Direct Effect","text":"previously noted, assumptions underlying natural direct indirect\neffects may challenging justify; moreover, effect definitions\ndepend application static intervention treatment,\nsharply limiting flexibility. considering binary treatments,\nincremental propensity score shifts provide alternative class flexible,\nstochastic interventions. ’ll now consider estimating PIDE IPSI\nmodulates odds receiving treatment \\(\\delta = 3\\). \nintervention may interpreted (hypothetically) effect design \nencourages study participants opt receiving treatment, thus\nincreasing relative odds receiving said treatment. exemplify \napproach, postulate motivational intervention triples odds\n(.e., \\(\\delta = 3\\)) receiving treatment individual:Recall , based decomposition outlined previously, PIDE may \ndenoted \\(\\beta_{\\text{PIDE}}(\\delta) = \\theta_0(\\delta) - \\mathbb{E}Y\\). Thus,\nestimator PIDE, \\(\\hat{\\beta}_{\\text{PIDE}}(\\delta)\\) may expressed\ncomposition estimators constituent parameters:\n\\[\\begin{equation*}\n  \\hat{\\beta}_{\\text{PIDE}}({\\delta}) = \\hat{\\theta}(\\delta) -\n  \\frac{1}{n} \\sum_{= 1}^n Y_i.\n\\end{equation*}\\]","code":"\n# set the IPSI multiplicative shift\ndelta_ipsi <- 3\n\n# instantiate tmle3 spec for stochastic mediation\ntmle_spec_pie_decomp <- tmle_medshift(\n  delta = delta_ipsi,\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  phi_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE)\n)\n\n# compute the TML estimate\nwashb_pie_decomp <- tmle3(\n  tmle_spec_pie_decomp, washb_data, node_list, learner_list\n)\nwashb_pie_decomp\n\n# get the PIDE\nwashb_pie_decomp$summary$tmle_est - mean(washb_data[, get(node_list$Y)])"},{"path":"causal-mediation-analysis.html","id":"exercises-4","chapter":"10 Causal Mediation Analysis","heading":"10.6 Exercises","text":"","code":""},{"path":"causal-mediation-analysis.html","id":"review-of-key-concepts-3","chapter":"10 Causal Mediation Analysis","heading":"10.6.1 Review of Key Concepts","text":"Examine WASH Benefits dataset choose different set potential\nmediators effect treatment weight--height Z-score. Using\nnewly chosen set mediators (single mediator), estimate \nnatural direct indirect effects. Provide interpretation \nestimates.Examine WASH Benefits dataset choose different set potential\nmediators effect treatment weight--height Z-score. Using\nnewly chosen set mediators (single mediator), estimate \nnatural direct indirect effects. Provide interpretation \nestimates.Assess whether additivity natural direct indirect effects holds.\nUsing natural direct indirect effects estimated , \nsum recover ATE?Assess whether additivity natural direct indirect effects holds.\nUsing natural direct indirect effects estimated , \nsum recover ATE?Evaluate whether assumptions required identification natural\ndirect indirect effects plausible WASH Benefits example. \nparticular, position evaluation terms empirical diagnostics \ntreatment mediator positivity.Evaluate whether assumptions required identification natural\ndirect indirect effects plausible WASH Benefits example. \nparticular, position evaluation terms empirical diagnostics \ntreatment mediator positivity.","code":""},{"path":"causal-mediation-analysis.html","id":"the-ideas-in-action-2","chapter":"10 Causal Mediation Analysis","heading":"10.6.2 The Ideas in Action","text":"Forthcoming.","code":""},{"path":"r6.html","id":"r6","chapter":"11 R6 Class System Primer","heading":"11 R6 Class System Primer","text":"central goal Targeted Learning statistical paradigm estimate\nscientifically relevant parameters realistic (usually nonparametric) models.tlverse designed using basic OOP principles R6 OOP framework.\n’ve tried make easy use tlverse packages without worrying\nmuch OOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"r6.html","id":"classes-fields-and-methods","chapter":"11 R6 Class System Primer","heading":"11.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R’s various OOP systems,\nsee http://adv-r..co.nz/OO-essentials.html, Hadley Wickham’s Advanced\nR (Wickham 2014).","code":""},{"path":"r6.html","id":"object-oriented-programming-python-and-r","chapter":"11 R6 Class System Primer","heading":"11.2 Object Oriented Programming: Python and R","text":"OO concepts (classes inherentence) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Avin, Chen, Ilya Shpitser, Judea Pearl. 2005. “Identifiability Path-Specific Effects.” IJCAI International Joint Conference Artificial Intelligence, 357–63.Baker, Monya. 2016. “Reproducibility Crisis? Nature Survey Lifts Lid Researchers View Crisis Rocking Science Think Help.” Nature 533 (7604): 452–55.Bembom, O., M. J. van der Laan. 2007. “practical illustration importance realistic individualized treatment rules causal inference.” Electron J Stat 1: 574–96.Bengtsson, Henrik. 2021. “Unifying Framework Parallel Distributed Processing R Using Futures.” R Journal. https://doi.org/10.32614/RJ-2021-048.Benkeser, David, Jialu Ran. 2021. “Nonparametric Inference Interventional Effects Multiple Mediators.” Journal Causal Inference. https://doi.org/10.1515/jci-2020-0018.Benkeser, David, Mark J van der Laan. 2016. “Highly Adaptive Lasso Estimator.” 2016 IEEE International Conference Data Science Advanced Analytics (DSAA). IEEE. https://doi.org/10.1109/dsaa.2016.93.Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.Buckheit, Jonathan B, David L Donoho. 1995. “Wavelab Reproducible Research.” Wavelets Statistics, 55–81. Springer.Chakraborty, Bibhas, Erica EM Moodie. 2013. Statistical Methods Dynamic Treatment Regimes: Reinforcement Learning, Causal Inference, Personalized Medicine (Statistics Biology Health). Springer.Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins. 2018. “Double/Debiased Machine Learning Treatment Structural Parameters.” Econometrics Journal 21 (1). https://doi.org/10.1111/ectj.12097.Coyle, Jeremy R, Nima S Hejazi. 2018. “Origami: Generalized Framework Cross-Validation R.” Journal Open Source Software 3 (21). https://doi.org/10.21105/joss.00512.Coyle, Jeremy R, Nima S Hejazi, Ivana Malenica, Rachael V Phillips, Benjamin F Arnold, Andrew Mertens, Jade Benjamin-Chung, et al. 2021. “Targeting Learning: Robust Statistics Reproducible Research.” arXiv. https://arxiv.org/abs/2006.07333.Coyle, Jeremy R, Nima S Hejazi, Mark J van der Laan. 2020. hal9001: Scalable Highly Adaptive Lasso. https://doi.org/10.5281/zenodo.3558313.Dawid, Philip. 2000. “Causal Inference Without Counterfactuals.” Journal American Statistical Association 95 (450): 407–24.Didelez, Vanessa, Philip Dawid, Sara Geneletti. 2006. “Direct Indirect Effects Sequential Treatments.” Proceedings 22nd Annual Conference Uncertainty Artificial Intelligence, 138–46.Dı́az, Iván, Nima S Hejazi. 2020. “Causal Mediation Analysis Stochastic Interventions.” Journal Royal Statistical Society: Series B (Statistical Methodology) 82 (3): 661–83. https://doi.org/10.1111/rssb.12362.Dı́az, Iván, Nima S Hejazi, Kara E Rudolph, Mark J van der Laan. 2020. “Non-Parametric Efficient Causal Mediation Intermediate Confounders.” Biometrika. https://doi.org/10.1093/biomet/asaa085.Dı́az, Iván, Mark J van der Laan. 2011. “Super Learner Based Conditional Density Estimation Application Marginal Structural Models.” International Journal Biostatistics 7 (1): 1–20.———. 2012. “Population Intervention Causal Effects Based Stochastic Interventions.” Biometrics 68 (2): 541–49.———. 2018. “Stochastic Treatment Regimes.” Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies, 167–80. Springer Science & Business Media.Donoho, David. 2017. “50 Years Data Science.” Journal Computational Graphical Statistics 26 (4): 745–66.Dudoit, Sandrine, Mark J van der Laan. 2005. “Asymptotics Cross-Validated Risk Estimation Estimator Selection Performance Assessment.” Statistical Methodology 2 (2): 131–54.Haneuse, Sebastian, Andrea Rotnitzky. 2013. “Estimation Effect Interventions Modify Received Treatment.” Statistics Medicine 32 (30): 5260–77.Hejazi, Nima S. 2021. “Semiparametric Statistical Methods Causal Inference Stochastic Treatment Regimes.” PhD thesis, University California, Berkeley. https://www.stat.berkeley.edu/~nhejazi/publications/thesis-phd-biostat.pdf.Hejazi, Nima S, David C Benkeser, Mark J van der Laan. 2020. haldensify: Highly Adaptive Lasso Conditional Density Estimation. https://github.com/nhejazi/haldensify. https://doi.org/10.5281/zenodo.3698329.Hejazi, Nima S, Jeremy R Coyle, Mark J van der Laan. 2020. “hal9001: Scalable Highly Adaptive Lasso Regression R.” Journal Open Source Software. https://doi.org/10.21105/joss.02526.Hejazi, Nima S, Kara E Rudolph, Mark J van der Laan, Iván Dı́az. 2021. “Nonparametric Causal Mediation Analysis Stochastic Interventional ()direct Effects.” Revision Invited Biostatistics. https://arxiv.org/abs/2009.06203.Hejazi, Nima S, Mark J van der Laan, Holly E Janes, Peter B Gilbert, David C Benkeser. 2020. “Efficient Nonparametric Inference Effects Stochastic Interventions Two-Phase Sampling, Applications Vaccine Efficacy Trials.” Biometrics. https://doi.org/10.1111/biom.13375.Imai, Kosuke, Luke Keele, Teppei Yamamoto. 2010. “Identification, Inference Sensitivity Analysis Causal Mediation Effects.” Statistical Science, 51–71.Imbens, Guido W, Donald B Rubin. 2015. Causal Inference Statistics, Social, Biomedical Sciences. Cambridge University Press.Kennedy, Edward H. 2019. “Nonparametric Causal Effects Based Incremental Propensity Score Interventions.” Journal American Statistical Association 114 (526): 645–56.Lok, Judith J. 2016. “Defining Estimating Causal Direct Indirect Effects Setting Mediator Specific Values Feasible.” Statistics Medicine 35 (22): 4008–20.Luedtke, Alexander R, Mark J van der Laan. 2016. “Optimal Individualized Treatments Resource-Limited Settings.” International Journal Biostatisics 12 (1): 283–303.Luedtke, ., M. J van der Laan. 2016. “Super-Learning Optimal Dynamic Treatment Rule.” International Journal Biostatistics 12 (1): 305–32.Montoya, Lina, Mark van der Laan, Alexander Luedtke, Jennifer Skeem, Jeremy Coyle, Maya Petersen. 2021. “Optimal Dynamic Treatment Rule Superlearner: Considerations, Performance, Application.” http://arxiv.org/abs/2101.12326.Montoya, Lina, Jennifer Skeem, Mark van der Laan, Maya Petersen. 2021. “Performance Application Estimators Value Optimal Dynamic Treatment Rule.” http://arxiv.org/abs/2101.12333.Munafò, Marcus R, Brian Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, John PA Ioannidis. 2017. “Manifesto Reproducible Science.” Nature Human Behaviour 1 (1): 0021.Murphy, Susan . 2003. “Optimal Dynamic Treatment Regimes.” Journal Royal Statistical Society: Series B (Statistical Methodology) 65 (2): 331–55.Nature Editorial (Anonymous). 2015a. “Scientists Fool — Can Stop.” Nature 526 (7572).———. 2015b. “Let’s Think Cognitive Bias.” Nature 526 (7572). https://doi.org/10.1038/526163a.Neyman, Jerzy. 1938. “Contribution Theory Sampling Human Populations.” Journal American Statistical Association 33 (201): 101–16.Nguyen, Trang Quynh, Ian Schmid, Elizabeth Stuart. 2019. “Clarifying Causal Mediation Analysis Applied Researcher: Defining Effects Based Want Learn.” arXiv Preprint arXiv:1904.08515.Nosek, Brian , Charles R Ebersole, Alexander C DeHaven, David T Mellor. 2018. “Preregistration Revolution.” Proceedings National Academy Sciences 115 (11): 2600–2606.Pearl, Judea. 1995. “Causal Diagrams Empirical Research.” Biometrika 82 (4): 669–88.———. 2001. “Direct Indirect Effects.” arXiv Preprint arXiv:1301.2300.———. 2009. Causality: Models, Reasoning, Inference. Cambridge University Press.Peng, Roger. 2015. “Reproducibility Crisis Science: Statistical Counterattack.” Significance 12 (3): 30–32.Petersen, Maya L, Sandra E Sinisi, Mark J van der Laan. 2006. “Estimation Direct Causal Effects.” Epidemiology, 276–84.Polley, Eric C, Mark J van der Laan. 2010. “Super Learner Prediction.” Division Biostatistics, University California, Berkeley; bepress.Popper, Karl. 1934. Logic Scientific Discovery. Routledge.Pullenayegum, Eleanor M, Robert W Platt, Melanie Barwick, Brian M Feldman, Martin Offringa, Lehana Thabane. 2016. “Knowledge Translation Biostatistics: Survey Current Practices, Preferences, Barriers Dissemination Uptake New Statistical Methods.” Statistics Medicine 35 (6): 805–18.R Core Team. 2021. “R: Language Environment Statistical Computing.” Vienna, Austria: R Foundation Statistical Computing. https://www.R-project.org/.Robins, James. 1986. “New Approach Causal Inference Mortality Studies Sustained Exposure Period—Application Control Healthy Worker Survivor Effect.” Mathematical Modelling 7 (9): 1393–1512. https://doi.org/https://doi.org/10.1016/0270-0255(86)90088-6.Robins, James M. 1986. “New Approach Causal Inference Mortality Studies Sustained Exposure Periods — Application Control Healthy Worker Survivor Effect.” Mathematical Modelling 7: 1393–1512.———. 2004. “Optimal Structural Nested Models Optimal Sequential Decisions.” Proceedings Second Seattle Symposium Biostatistics: Analysis Correlated Data, 189–326. Springer New York. https://doi.org/10.1007/978-1-4419-9076-1_11.Robins, James M, Sander Greenland. 1992. “Identifiability Exchangeability Direct Indirect Effects.” Epidemiology, 143–55.Robins, James M, Thomas S Richardson. 2010. “Alternative Graphical Causal Models Identification Direct Effects.” Causality Psychopathology: Finding Determinants Disorders Cures, 103–58.Robins, James, Andrea Rotnitzky. 2014. “Discussion ‘Dynamic Treatment Regimes: Technical Challenges Applications’.” Electron. J. Statist. 8 (1): 1273–89. https://doi.org/10.1214/14-EJS908.Rubin, Donald B. 1978. “Bayesian Inference Causal Effects: Role Randomization.” Annals Statistics, 34–58.———. 1980. “Randomization Analysis Experimental Data: Fisher Randomization Test Comment.” Journal American Statistical Association 75 (371): 591–93.———. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal American Statistical Association 100 (469): 322–31.Rudolph, Kara E, Oleg Sofrygin, Wenjing Zheng, Mark J van der Laan. 2017. “Robust Flexible Estimation Stochastic Mediation Effects: Proposed Method Example Randomized Trial Setting.” Epidemiologic Methods 7 (1).Spirtes, Peter, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory Cooper, Thomas Richardson. 2000. Causation, Prediction, Search. MIT press.Stark, Philip B, Andrea Saltelli. 2018. “Cargo-Cult Statistics Scientific Crisis.” Significance 15 (4): 40–43.Stock, James H. 1989. “Nonparametric Policy Analysis.” Journal American Statistical Association 84 (406): 567–75.Stromberg, Arnold, others. 2004. “Write Statistical Software? Case Robust Statistical Methods.” Journal Statistical Software 10 (5): 1–8.Sutton, Richard S, Andrew G Barto, others. 1998. Introduction Reinforcement Learning. Vol. 135. MIT press Cambridge.Szucs, Denes, John Ioannidis. 2017. “Null Hypothesis Significance Testing Unsuitable Research: Reassessment.” Frontiers Human Neuroscience 11: 390.Tchetgen Tchetgen, Eric J. 2013. “Inverse Odds Ratio-Weighted Estimation Causal Mediation Analysis.” Statistics Medicine 32 (26): 4567–80.Tchetgen Tchetgen, Eric J, Ilya Shpitser. 2012. “Semiparametric Theory Causal Mediation Analysis: Efficiency Bounds, Multiple Robustness, Sensitivity Analysis.” Annals Statistics 40 (3): 1816–45. https://doi.org/10.1214/12-AOS990.Textor, Johannes, Juliane Hardt, Sven Knüppel. 2011. “DAGitty: Graphical Tool Analyzing Causal Diagrams.” Epidemiology 22 (5): 745.Tofail, Fahmida, Lia CH Fernald, Kishor K Das, Mahbubur Rahman, Tahmeed Ahmed, Kaniz K Jannat, Leanne Unicomb, et al. 2018. “Effect Water Quality, Sanitation, Hand Washing, Nutritional Interventions Child Development Rural Bangladesh (Wash Benefits Bangladesh): Cluster-Randomised Controlled Trial.” Lancet Child & Adolescent Health 2 (4): 255–68.Tukey, John W. 1962. “Future Data Analysis.” Annals Mathematical Statistics 33 (1): 1–67.van der Laan, Mark J, Sandrine Dudoit. 2003. “Unified Cross-Validation Methodology Selection Among Estimators General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities Examples.” Division Biostatistics, University California, Berkeley; bepress.van der Laan, Mark J, Sandrine Dudoit, Sunduz Keles. 2004. “Asymptotic Optimality Likelihood-Based Cross-Validation.” Statistical Applications Genetics Molecular Biology 3 (1): 1–23.van der Laan, Mark J, Eric C Polley, Alan E Hubbard. 2007. “Super Learner.” Statistical Applications Genetics Molecular Biology 6 (1).van der Laan, Mark J, Sherri Rose. 2011. Targeted Learning: Causal Inference Observational Experimental Data. Springer Science & Business Media.———. 2018. Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies. Springer Science & Business Media.van der Laan, Mark J, Richard JCM Starmans. 2014. “Entering Era Data Science: Targeted Learning Integration Statistics Computational Data Analysis.” Advances Statistics 2014.van der Laan, M. J, . Luedtke. 2015. “Targeted Learning Mean Outcome Optimal Dynamic Treatment Rule.” Journal Causal Inference 3 (1): 61–95.Van der Vaart, Aad W, Sandrine Dudoit, Mark J van der Laan. 2006. “Oracle Inequalities Multi-Fold Cross Validation.” Statistics & Decisions 24 (3): 351–71.VanderWeele, Tyler. 2015. Explanation Causal Inference: Methods Mediation Interaction. Oxford University Press.VanderWeele, Tyler J, Stijn Vansteelandt, James M Robins. 2014. “Effect Decomposition Presence Exposure-Induced Mediator-Outcome Confounder.” Epidemiology 25 (2): 300.Vansteelandt, Stijn, Maarten Bekaert, Theis Lange. 2012. “Imputation Strategies Estimation Natural Direct Indirect Effects.” Epidemiologic Methods 1 (1): 131–58.Vansteelandt, Stijn, Rhian M Daniel. 2017. “Interventional Effects Mediation Analysis Multiple Mediators.” Epidemiology 28 (2): 258.Vansteelandt, Stijn, Tyler J VanderWeele. 2012. “Natural Direct Indirect Effects Exposed: Effect Decomposition Weaker Assumptions.” Biometrics 68 (4): 1019–27.Wickham, Hadley. 2014. Advanced R. Chapman; Hall/CRC.Wright, Sewall. 1934. “Method Path Coefficients.” Annals Mathematical Statistics 5 (3): 161–215.Young, Jessica G, Miguel Hernán, James M Robins. 2014. “Identification, Estimation Approximation Risk Interventions Depend Natural Value Treatment Using Observational Data.” Epidemiologic Methods 3 (1): 1–19.Zhang, Baqun, Anastasios Tsiatis, Marie Davidian, Min Zhang, Eric Laber. 2016. “Estimating Optimal Treatment Regimes Classification Perspective.” Stat 5 (1): 278–78. https://doi.org/10.1002/sta4.124.Zhao, Yingqi, Donglin Zeng, John Rush, Michael R Kosorok. 2012. “Estimating Individualized Treatment Rules Using Outcome Weighted Learning.” Journal American Statistical Association 107 (499): 1106–18. https://doi.org/10.1080/01621459.2012.695674.Zheng, Wenjing, Mark J van der Laan. 2011. “Cross-Validated Targeted Minimum-Loss-Based Estimation.” Targeted Learning, 459–74. Springer.———. 2012. “Targeted Maximum Likelihood Estimation Natural Direct Effects.” International Journal Biostatistics 8 (1). https://doi.org/10.2202/1557-4679.1361.Zheng, W., M. J van der Laan. 2010. “Asymptotic Theory Cross-validated Targeted Maximum Likelihood Estimation.” U.C. Berkeley Division Biostatistics Working Paper Series.","code":""}]
