[{"path":"index.html","id":"about-this-book","chapter":"About this book","heading":"About this book","text":"Targeted Learning R: Causal Data Science tlverse Software\nEcosystem open source, reproducible electronic handbook applying \nTargeted Learning methodology practice using tlverse software\necosystem. work currently early draft\nphase available facilitate input community. view \ncontribute available content, consider visiting GitHub\nrepository.\n","code":""},{"path":"index.html","id":"outline","chapter":"About this book","heading":"0.1 Outline","text":"contents handbook meant serve reference guide \napplied research well materials can taught series short\ncourses focused applications Targeted Learning. section\nintroduces set distinct causal questions, motivated case study,\nalongside statistical methodology software assessing causal claim \ninterest. (evolving) set materials includesMotivation: need statistical\nrevolutionThe Roadmap introductory case study: WASH Beneifits dataIntroduction tlverse software\necosystemCross-validation origami\npackageEnsemble machine learning \nsl3 packageTargeted learning causal inference \ntmle3 packageOptimal treatments regimes \ntmle3mopttx packageStochastic treatment regimes \ntmle3shift packageCausal mediation analysis \ntmle3mediate package\n(work progress)Coda: need statistical\nrevolution","code":""},{"path":"index.html","id":"what-this-book-is-not","chapter":"About this book","heading":"What this book is not","text":"focus work providing -depth technical descriptions\ncurrent statistical methodology recent advancements. Instead, goal \nconvey key details state---art techniques manner \nclear complete, without burdening reader extraneous information.\nhope presentations herein serve references researchers\n– methodologists domain specialists alike – empower deploy\ncentral tools Targeted Learning efficient manner. technical\ndetails -depth descriptions classical theory recent advances\nfield Targeted Learning, interested reader invited consult\nvan der Laan Rose (2011) /van der Laan Rose (2018) appropriate. primary literature\nstatistical causal inference, machine learning, non/semiparametric theory\ninclude many recent advances Targeted Learning related areas.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"About this book","heading":"About the authors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"About this book","heading":"Mark van der Laan","text":"Mark van der Laan, PhD, Professor Biostatistics Statistics UC\nBerkeley. research interests include statistical methods computational\nbiology, survival analysis, censored data, adaptive designs, targeted maximum\nlikelihood estimation, causal inference, data-adaptive loss-based learning, \nmultiple testing. research group developed loss-based super learning \nsemiparametric models, based cross-validation, generic optimal tool \nestimation infinite-dimensional parameters, nonparametric density\nestimation prediction censored uncensored data. Building \nwork, research group developed targeted maximum likelihood estimation\ntarget parameter data-generating distribution arbitrary\nsemiparametric nonparametric models, generic optimal methodology \nstatistical causal inference. recently, Mark’s group focused \npart development centralized, principled set software tools \ntargeted learning, tlverse.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"About this book","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"About this book","heading":"Nima Hejazi","text":"Nima Hejazi PhD candidate biostatistics, working collaborative\ndirection Mark van der Laan Alan Hubbard. Nima affiliated UC\nBerkeley’s Center Computational Biology NIH Biomedical Big Data training\nprogram, well Fred Hutchinson Cancer Research Center. Previously,\nearned MA Biostatistics BA (majors Molecular Cell\nBiology, Psychology, Public Health), UC Berkeley. research\ninterests fall intersection causal inference machine learning,\ndrawing ideas non/semi-parametric estimation large, flexible\nstatistical models develop efficient robust statistical procedures \nevaluating complex target estimands observational randomized studies.\nParticular areas current emphasis include mediation/path analysis,\noutcome-dependent sampling designs, targeted loss-based estimation, vaccine\nefficacy trials. Nima also passionate statistical computing open\nsource software development applied statistics.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"About this book","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"About this book","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning causal inference; research\nintegrates personalized medicine, human-computer interaction, experimental\ndesign, regulatory policy.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"About this book","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics, former head Division \nBiostatistics UC Berkeley, head data analytics core UC Berkeley’s\nSuperFund research program. current research interests include causal\ninference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"repro","chapter":"About this book","heading":"0.2 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.0.2 (2020-06-22),\npandoc version 2.2, \nfollowing packages:","code":""},{"path":"index.html","id":"learn","chapter":"About this book","heading":"0.3 Learning resources","text":"effectively utilize handbook, reader need fully trained\nstatistician begin understanding applying methods. However, \nhighly recommended reader understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.learning R programming language recommend following (free)\nintroductory resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGarret Grolemund Hadley Wickham’s R Data\nScienceFor general introduction causal inference, recommendMiguel . Hernán James M. Robins’ Causal Inference: ,\n2021Jason . Roy’s Crash Course Causality: Inferring Causal Effects \nObservational Data \nCoursera","code":""},{"path":"index.html","id":"setup","chapter":"About this book","heading":"0.4 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"About this book","heading":"0.4.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"About this book","heading":"0.4.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"About this book","heading":"0.4.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"About this book","heading":"0.4.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"robust.html","id":"robust","chapter":"1 Robust Statistics and Reproducible Science","heading":"1 Robust Statistics and Reproducible Science","text":"“One enemy robust science humanity — appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Anonymous (2015)Scientific research unique point history. need improve rigor\nreproducibility field greater ever; corroboration moves\nscience forward, yet growing alarm results \nreproduced report false discoveries (Baker 2016). Consequences \nmeeting need result decline rate scientific\nprogression, reputation sciences, public’s trust \nfindings (Munafò et al. 2017; Editorial 2015).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state culture data analysis statistics\nactually enables human bias improper model selection. hypothesis\ntests estimators derived statistical models, obtain valid\nestimates inference critical statistical model contains \nprocess generated data. Perhaps treatment randomized \ndepended small number baseline covariates; knowledge \ncan incorporated model. Alternatively, maybe data \nobservational, knowledge data-generating process (DGP).\ncase, statistical model contain data\ndistributions. practice; however, models selected based knowledge\nDGP, instead models often selected based (1) p-values \nyield, (2) convenience implementation, /(3) analysts loyalty\nparticular model. practice “cargo-cult statistics — \nritualistic miming statistics rather conscientious practice,”\n(Stark Saltelli 2018) characterized arbitrary modeling choices, even though\nchoices often result different answers research question.\n, “increasingly often, [statistics] used instead aid \nabet weak science, role can perform well used mechanically \nritually,” opposed original purpose safeguarding weak\nscience (Stark Saltelli 2018). presents fundamental drive behind epidemic\nfalse findings scientific research suffering (van der Laan Starmans 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite\" education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.\"— Szucs Ioannidis (2017)team University California, Berkeley, uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nspreading rapidly many students colleagues greatly\nenriched field, aptly named “Targeted Learning” methodology targets \nscientific question hand counter current culture \n“convenience statistics” opens door biased estimation, misleading\nresults, false discoveries. Targeted Learning restores fundamentals \nformalized field statistics, facts statistical\nmodel represents real knowledge experiment generated data,\ntarget parameter represents seeking learn data \nfeature distribution generated (van der Laan Starmans 2014). way,\nTargeted Learning defines truth establishes principled standard \nestimation, thereby inhibiting --human biases (e.g., hindsight bias,\nconfirmation bias, outcome bias) infiltrating analysis.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)objective handbook provide training students, researchers,\nindustry professionals, faculty science, public health, statistics, \nfields empower necessary knowledge skills utilize \nsound methodology Targeted Learning — technique provides tailored\npre-specified machines answering queries, data analysis \ncompletely reproducible, estimators efficient, minimally biased, \nprovide formal statistical inference.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives, remains critical acknowledge \nrole robust software plays allowing practitioners direct access \npublished results. recall “article…scientific publication \nscholarship , merely advertising scholarship. \nactual scholarship complete software development environment \ncomplete set instructions generated figures,” thus making \navailability adoption robust statistical software key enhancing \ntransparency inherent aspect science (Buckheit Donoho 1995).statistical methodology readily accessible practice, \ncrucial accompanied robust user-friendly software\n(Pullenayegum et al. 2016; Stromberg others 2004). tlverse software\necosystem developed fulfill need Targeted Learning\nmethodology. software facilitate computationally reproducible\nefficient analyses, also tool Targeted Learning education since\nworkflow mirrors methodology. particular, tlverse\nparadigm focus implementing specific estimator small set \nrelated estimators. Instead, focus exposing statistical framework\nTargeted Learning — R packages tlverse ecosystem\ndirectly model key objects defined mathematical theoretical\nframework Targeted Learning. ’s , tlverse R packages share \ncore set design principles centered extensibility, allowing \nused conjunction built upon one cohesive\nfashion. introduction Targeted Learning, recommend recent\nreview paper Coyle et al. (2021).handbook, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuitive explanation readers build toolbox applying Targeted\nLearning statistical methodology, translate real-world causal\ninference analyses. preliminaries required prior learning\nendeavor – made available list recommended learning\nresources.","code":""},{"path":"intro.html","id":"intro","chapter":"2 The Roadmap for Targeted Learning","heading":"2 The Roadmap for Targeted Learning","text":"","code":""},{"path":"intro.html","id":"learning-objectives","chapter":"2 The Roadmap for Targeted Learning","heading":"Learning Objectives","text":"end chapter able :Translate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following causal statistical assumptions \nimplications: ..d., consistency, interference, positivity, SUTVA.","code":""},{"path":"intro.html","id":"introduction","chapter":"2 The Roadmap for Targeted Learning","heading":"Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.1 The Roadmap","text":"Following roadmap process five stages.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\M\\) \\(P_0 \\\\M\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(1) Data: A random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set ’re confronted result experiment can\nview data random variable, \\(O\\), repeat experiment\ndifferent realization experiment. particular, \nrepeat experiment many times learn probability distribution,\n\\(P_0\\), data. , observed data \\(O\\) probability distribution\n\\(P_0\\) \\(n\\) independent identically distributed (..d.) observations \nrandom variable \\(O; O_1, \\ldots, O_n\\). Note data ..d.,\nways handle non-..d. data, establishing conditional\nindependence, stratifying data create sets identically distributed data,\netc. crucial researchers absolutely clear actually\nknow data-generating distribution given problem interest.\nUnfortunately, communication statisticians researchers often\nfraught misinterpretation. roadmap provides mechanism \nensure clear communication research statistician – truly helps\ncommunication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"2 The Roadmap for Targeted Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure \\(P_0\\), allowing us learn data. \nexample, can define empirical probability measure set, \\(\\), \nproportion observations end \\(\\). ,\n\\[\\begin{equation*}\n  P_n() = \\frac{1}{n}\\sum_{=1}^{n} \\(O_i \\)\n\\end{equation*}\\]order start learning something, need ask “know \nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"the-statistical-model-m-such-that-p_0-in-m","chapter":"2 The Roadmap for Targeted Learning","heading":"(2) The statistical model \\(\\M\\) such that \\(P_0 \\in \\M\\)","text":"statistical model \\(\\M\\) defined question asked end \nStep 1. defined set possible probability distributions \nobserved data. Often \\(\\M\\) large (possibly infinite-dimensional), \nreflect fact statistical knowledge limited. case \\(\\M\\) \ninfinite-dimensional, deem nonparametric statistical model.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model parametric. \ncase, subscribe belief random variable \\(O\\) \nobserved , example, normal distribution mean \\(\\mu\\) variance\n\\(\\sigma^2\\). Formally, parametric model may defined\n\\[\\begin{equation*}\n  \\M = \\{P_{\\theta} : \\theta \\\\R^d \\}\n\\end{equation*}\\]Sadly, assumption data-generating distribution specific,\nparametric form common, especially since leap faith \nassumption made convenience. practice oversimplification \ncurrent culture data analysis typically derails attempt trying \nanswer scientific question hand; alas, statements \never-popular quip Box “models wrong useful”\nencourage data analyst make arbitrary choices even practice\noften forces starkly different answers estimation problem. \nTargeted Learning paradigm suffer bias since defines \nstatistical model representation true data-generating\ndistribution corresponding observed data.Now, Step 3: “trying learn data?”","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\M\\), parameter space (.e., real number) \\(\\R\\). \n, \\(\\Psi: \\M \\rightarrow \\R\\). estimand may seen representation \nquantity wish learn data, answer well-specified\n(often causal) question interest. contrast purely statistical\nestimands, causal estimands require identification observed data,\nbased causal models include several untestable assumptions, described \ndetail section causal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\n\\[\\begin{equation*}\n  \\Psi(P_0) = \\P(O > 5)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity ’re trying learn data. defined \\(O\\), \\(\\M\\) \n\\(\\Psi(P_0)\\) formally defined statistical estimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"obtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\M_{NP}\\) (\\(P_n \\\\M_{NP}\\)), parameter space parameter \ninterest. , \\(\\hat{\\Psi} : \\M_{NP} \\rightarrow \\R^d\\). estimator \nfunction takes input observed data, realization \\(P_n\\), \ngives output value parameter space, estimate,\n\\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical distribution\n\\(P_n\\), estimator random variable sampling distribution.\n, repeat experiment drawing \\(n\\) observations every time\nend different realization estimate estimator \nsampling distribution. sampling distribution estimators can \ntheoretically validated approximately normally distributed Central\nLimit Theorem (CLT).Central Limit Theorem (CLTs) statement regarding convergence \nsampling distribution estimator normal distribution. \ngeneral, construct estimators whose limit sampling distributions may \nshown approximately normal distributed sample size increases. large\nenough \\(n\\) ,\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\sim N \\left(\\Psi(P_0), \\frac{\\sigma^2}{n}\\right),\n\\end{equation*}\\]\npermitting statistical inference. Now, can proceed quantify \nuncertainty chosen estimator construction hypothesis tests \nconfidence intervals. example, may construct confidence interval \nlevel \\((1 - \\alpha)\\) estimand, \\(\\Psi(P_0)\\):\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\left(\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\end{equation*}\\]\n\\(z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\)\nquantile standard normal distribution. Often, interested \nconstructing 95% confidence intervals, corresponding mass \\(\\alpha = 0.05\\) \neither tail limit distribution; thus, typically take\n\\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).Note: typically estimate standard error,\n\\(\\frac{\\sigma}{\\sqrt{n}}\\).95% confidence interval means take 100 different samples\nsize \\(n\\) compute 95% confidence interval sample, \napproximately 95 100 confidence intervals contain estimand,\n\\(\\Psi(P_0)\\). practically, means 95% probability\nconfidence interval procedure generates intervals containing \ntrue estimand value (95% confidence “covering” true value). ,\nsingle estimated confidence interval either contain true estimand\n(also called “coverage”).","code":""},{"path":"intro.html","id":"roadmap-summary","chapter":"2 The Roadmap for Targeted Learning","heading":"2.2 Summary of the Roadmap","text":"Data, \\(O\\), viewed random variable probability distribution.\noften \\(n\\) units independent identically distributed units \nprobability distribution \\(P_0\\), \\(O_1, \\ldots, O_n \\sim P_0\\). \nstatistical knowledge experiment generated data. \nwords, make statement true data distribution \\(P_0\\) falls \ncertain set called statistical model, \\(\\M\\). Often sets large\nstatistical knowledge limited - hence, statistical models\noften infinite dimensional models. statistical query , “\ntrying learn data?” denoted statistical target parameter,\n\\(\\Psi\\), maps \\(P_0\\) estimand, \\(\\Psi(P_0)\\). point \nstatistical estimation problem formally defined now need\nstatistical theory guide us construction estimators. ’s lot\nstatistical theory review course , particular, relies\nCentral Limit Theorem, allowing us come estimators \napproximately normally distributed also allowing us come statistical\ninference (.e., confidence intervals hypothesis tests).","code":""},{"path":"intro.html","id":"causal","chapter":"2 The Roadmap for Targeted Learning","heading":"2.3 Causal Target Parameters","text":"many cases, interested problems ask questions regarding \neffect intervention future outcome interest. questions can\nrepresented causal estimands.","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"2 The Roadmap for Targeted Learning","heading":"The Causal Model","text":"formalizing data statistical model, can define causal\nmodel express causal parameters interest. Directed acyclic graphs (DAGs)\none useful tool express know causal relations among\nvariables. Ignoring exogenous \\(U\\) terms (explained ), assume \nfollowing ordering variables observed data \\(O\\). \nusing DAGitty (Textor, Hardt, Knüppel 2011):directed acyclic graphs (DAGs) like provide convenient means \nvisualize causal relations variables, causal relations\namong variables can represented via set structural equations, \ndefine non-parametric structural equation model (NPSEM):\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(U_W\\), \\(U_A\\), \\(U_Y\\) represent unmeasured exogenous background\ncharacteristics influence value variable. NPSEM, \\(f_W\\),\n\\(f_A\\) \\(f_Y\\) denote variable (\\(W\\), \\(\\) \\(Y\\), respectively)\nfunction parents unmeasured background characteristics, note\nimposition particular functional constraints(e.g.,\nlinear, logit-linear, one interaction, etc.). reason, \ncalled non-parametric structural equation models (NPSEMs). DAG set \nnonparametric structural equations represent exactly information \nmay used interchangeably.first hypothetical experiment consider assigning exposure \nwhole population observing outcome, assigning exposure \nwhole population observing outcome. nonparametric structural\nequations, corresponds comparison outcome distribution \npopulation two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new nonparametric structural equation models. \ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\ncase \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervened system, setting \\(\\) deterministically either values\n\\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome variable \npopulation generated respective NPSEMs ; \noften called counterfactuals (since run contrary--fact). difference\nmeans outcome two interventions defines \nparameter often called “average treatment effect” (ATE), denoted\n\\[\\begin{equation}\n  ATE = \\E_X(Y(1) - Y(0)),\n  \\tag{2.1}\n\\end{equation}\\]\n\\(\\E_X\\) mean theoretical (unobserved) full data \\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Y\n    A -> Y\n    W -> A -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"intro.html","id":"identifiability","chapter":"2 The Roadmap for Targeted Learning","heading":"Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate quantity Equation (2.1) directly. Instead, \nmake assumptions quantity may estimated observed\ndata \\(O \\sim P_0\\) data-generating distribution \\(P_0\\). Fortunately,\ngiven causal model specified NPSEM , can, handful \nuntestable assumptions, estimate ATE, even observational data. \nassumptions may summarized follows.causal graph implies \\(Y() \\perp \\) \\(\\\\mathcal{}\\), \nrandomization assumption. case observational data, \nanalogous assumption strong ignorability unmeasured confounding\n\\(Y() \\perp \\mid W\\) \\(\\\\mathcal{}\\);Although represented causal graph, also required assumption\ninterference units, , outcome unit \\(\\) \\(Y_i\\) \naffected exposure unit \\(j\\) \\(A_j\\) unless \\(=j\\);Consistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”;also necessary observed units, across strata defined \\(W\\),\nbounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\P(= \\mid W) < 1\\) \\(\\) \\(W\\)). assumption\nreferred positivity overlap.Remark: Together, (2) (3), assumptions interference \nconsistency, respectively, jointly referred stable unit\ntreatment value assumption (SUTVA).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\n\\[\\begin{equation}\n  ATE = \\E_0(Y(1) - Y(0)) = \\E_0\n    \\left(\\E_0[Y \\mid = 1, W] - \\E_0[Y \\mid = 0, W]\\right).\n  \\tag{2.2}\n\\end{equation}\\]\nwords, ATE difference predicted outcome values \nsubject, contrast treatment conditions (\\(= 0\\) versus \\(= 1\\)),\npopulation, averaged observations. Thus, parameter \ntheoretical “full” data distribution can represented estimand \nobserved data distribution. Significantly, nothing \nrepresentation Equation (2.2) requires parameteric\nassumptions; thus, regressions right hand side may estimated\nfreely machine learning. different parameters, \npotentially different identifiability assumptions resulting estimands\ncan functions different components \\(P_0\\). discuss several \ncomplex estimands later sections handbook.","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"3 Welcome to the tlverse","heading":"3 Welcome to the tlverse","text":"","code":""},{"path":"tlverse.html","id":"learning-objectives-1","chapter":"3 Welcome to the tlverse","heading":"Learning Objectives","text":"Understand tlverse ecosystem conceptuallyIdentify core components tlverseInstall tlverse R packagesUnderstand Targeted Learning roadmapLearn WASH Benefits example data","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"3 Welcome to the tlverse","heading":"What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"anatomy-of-the-tlverse","chapter":"3 Welcome to the tlverse","heading":"Anatomy of the tlverse","text":"main packages represent core tlverse:sl3: Modern Super Learning Pipelines\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.addition engines drive development tlverse, \nsupporting packages – particular, two…origami: Generalized Framework \nCross-Validation\n? generalized framework flexible cross-validation\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.\n? generalized framework flexible cross-validationWhy? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.\n? framework delayed computations (futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.key principle tlverse extensibility. , want support\nnew Targeted Learning estimators developed. model \nnew estimators implemented additional packages using core packages\n. currently two featured examples :tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule\n? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome ruleWhy? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Shift Interventions \ntlverse\n? Shift interventions continuous treatments\n? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.\n? Shift interventions continuous treatmentsWhy? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"3 Welcome to the tlverse","heading":"3.1 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use usethis package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Create PAT simply clicking “Generate token” bottom page.Copy PAT, long string lowercase letters numbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio.\n.Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8.\nType usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio..Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, press enter key ensure .Renviron\nends new line.Save .Renviron file. example shows syntax \nlook.Save .Renviron file. example shows syntax \nlook.Restart R. can restart R via drop-menu RStudio’s “Session”\ntab, located top RStudio interface. \nrestart R changes take effect!following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.\nGITHUB_PAT=yourPAT"},{"path":"data.html","id":"data","chapter":"4 Meet the Data","heading":"4 Meet the Data","text":"","code":""},{"path":"data.html","id":"wash","chapter":"4 Meet the Data","heading":"4.1 WASH Benefits Example Dataset","text":"data come study effect water quality, sanitation, hand\nwashing, nutritional interventions child development rural Bangladesh\n(WASH Benefits Bangladesh): cluster randomized controlled trial\n(Tofail et al. 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block randomized,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;hand-washing soap;combined water, sanitation, hand washing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.handbook, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nNCT01590095.purposes handbook, start treating data independent\nidentically distributed (..d.) random draws large target\npopulation. , available options, account clustering \ndata (within sampled geographic units), , simplification, avoid \ndetails handbook, although modifications methodology biased\nsamples, repeated measures, related complications, available.28 variables measured, single variable set \noutcome interest. outcome, \\(Y\\), weight--height Z-score\n(whz dat); treatment interest, \\(\\), randomized treatment\ngroup (tr dat); adjustment set, \\(W\\), consists simply \neverything else. results observed data structure \\(n\\) ..d.\ncopies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables given just , complete\nsmall visualization describing marginal characteristics \ncovariate. Note asset variables reflect socio-economic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls); , course, design.","code":"\nlibrary(readr)\n# read in data via readr::read_csv\ndat <- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  )\n)"},{"path":"data.html","id":"ist","chapter":"4 Meet the Data","heading":"4.2 International Stroke Trial Example Dataset","text":"International Stroke Trial database contains individual patient data \nInternational Stroke Trial (IST), multi-national randomized trial\nconducted 1991 1996 (pilot phase 1991 1993) aimed\nassess whether early administration aspirin, heparin, aspirin \nheparin, neither influenced clinical course acute ischaemic stroke\n(Sandercock et al. 1997). IST dataset includes data 19,435 patients\nacute stroke, 99% complete follow-. De-identified data \navailable download https://datashare..ed.ac.uk/handle/10283/128. \nstudy described detail Sandercock, Niewada, Członkowska (2011). example\ndata handbook considers sample 5,000 patients binary\noutcome recurrent ischemic stroke within 14 days randomization. Also\nexample data, ensure subjects missing outcome.26 variables measured, outcome interest, \\(Y\\),\nindicates recurrent ischemic stroke within 14 days randomization (DRSISC\nist); treatment interest, \\(\\), randomized aspirin vs. \naspirin treatment allocation (RXASP ist); adjustment set, \\(W\\),\nconsists variables measured baseline. data, outcome\noccasionally missing, need create variable indicating\nmissingness (\\(\\Delta\\)) analyses tlverse, since \nautomatically detected NA present outcome. observed data\nstructure can denoted \\(n\\) ..d. copies \\(O_i = (W_i, A_i, \\Delta_i, \\Delta Y_i)\\), \\(= 1, \\ldots, n\\), \\(\\Delta\\) denotes binary\nindicator outcome observed.Like , can summarize variables measured IST sample data set\nskimr:(#tab:skim_ist_data)Data summaryVariable type: characterVariable type: numeric","code":"\n# read in data\nist <- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/\",\n    \"data/ist_sample.csv\"\n  )\n)"},{"path":"data.html","id":"NHEFS","chapter":"4 Meet the Data","heading":"4.3 NHANES I Epidemiologic Follow-up Study (NHEFS)","text":"data National Health Nutrition Examination Survey (NHANES)\nData Epidemiologic Follow-Study. coming soon.snapshot data set shown :(#tab:skim_nhefs_data)Data summaryVariable type: numeric","code":"\n# read in data\nnhefs_data <- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/\",\n    \"data/NHEFS.csv\"\n  )\n)"},{"path":"origami.html","id":"origami","chapter":"5 Cross-validation","heading":"5 Cross-validation","text":"Ivana MalenicaBased origami R package\nJeremy Coyle, Nima Hejazi, Ivana Malenica Rachael Phillips.Updated: 2021-04-19","code":""},{"path":"origami.html","id":"learning-objectives-2","chapter":"5 Cross-validation","heading":"5.1 Learning Objectives","text":"Differentiate training, validation test sets.Understand concept loss function, risk cross-validation.Select loss function appropriate functional parameter \nestimated.Understand contrast different cross-validation schemes ..d. data.Understand contrast different cross-validation schemes time dependent\ndata.Setup proper fold structure, build custom fold-based function, \ncross-validate proposed function using origami R package.Setup proper cross-validation structure use Super Learner\nusing origami R package.","code":""},{"path":"origami.html","id":"introduction-1","chapter":"5 Cross-validation","heading":"5.2 Introduction","text":"chapter, start elaborating estimation step outlined \nintroductory chapter, discussed Roadmap Targeted\nLearning. order generate initial estimate target\nparameter – focus following chapter Super\nLearning, first need translate, incorporate, knowledge\ndata generating process estimation procedure, decide \nevaluate estimation performance.performance, error, algorithm used estimation procedure\ndirectly relates generalizability independent data. proper\nassessment performance proposed algorithms extremely important; \nguides choice final learning method, gives us quantitative\nassessment good chosen algorithm . order assess \nperformance algorithm, introduce concept loss function,\nhelps us define risk, also referred expected\nprediction error. goal, specified next chapter, \nestimate true risk proposed statistical learning method. \ngoal(s) consist :Estimating performance different algorithms order choose \nbest one.chosen winner, try estimate true risk proposed\nstatistical learning method.following, propose method using observed data \ncross-validation procedure using origami package (Coyle Hejazi 2018).","code":""},{"path":"origami.html","id":"background","chapter":"5 Cross-validation","heading":"5.3 Background","text":"Ideally, data-rich scenario, split dataset three parts:training set,validation set,test set.training set used fit algorithm(s) interest; evaluate \nperformance fit(s) validation set, can used estimate\nprediction error (e.g., tuning model selection). final error \nchosen algorithm(s) obtained using test set, kept separately,\ndoesn’t see data final evaluation. One might wonder, \ntraining data readily available, use training error evaluate \nproposed algorithm’s performance? Unfortunately, training error \ngood estimate true risk; consistently decreases model complexity,\nresulting possible overfit training data low generalizability.Since data often scarce, separating training, validation test\nset usually possible. absence large data set designated\ntest set, must resort methods estimate true risk efficient\nsample re-use. Re-sampling methods, great generality, involve repeatedly\nsampling training set fitting proposed algorithms new\nsamples. often computationally intensive, re-sampling methods \nparticularly useful model selection estimation true risk. \naddition, might provide insight variability robustness \nalgorithm fit fitting algorithm training data.","code":""},{"path":"origami.html","id":"introducing-cross-validation","chapter":"5 Cross-validation","heading":"5.3.1 Introducing: cross-validation","text":"chapter, focus cross-validation – essential tool \nevaluating given algorithm extends sample target\npopulation sample derived. seen widespread application\nfacets statistics, perhaps notably statistical machine learning.\ncross-validation procedure can used model selection, well \nestimation true risk associated statistical learning method \norder evaluate performance. particular, cross-validation directly\nestimates true risk estimate applied independent sample\njoint distribution predictors outcome. used model\nselection, cross-validation powerful optimality properties. asymptotic\noptimality results state cross-validated selector performs (terms \nrisk) asymptotically well optimal oracle selector based true,\nunknown data generating distribution. details theoretical\nresults, suggest van der Laan, Dudoit, Keles (2004), Dudoit van der Laan (2005) \nVan der Vaart, Dudoit, Laan (2006).great generality, cross-validation works partitioning sample \ncomplementary subsets, applying particular algorithm(s) subset (\ntraining set), evaluating method choice complementary subset\n(validation/test set). procedure repeated across multiple partitions\ndata. variety different partitioning schemes exist, depending \nproblem interest, data size, prevalence outcome, dependence\nstructure. origami package provides suite tools generalize \napplication cross-validation arbitrary data analytic procedures. \nfollowing, describe different types cross-validation schemes readily\navailable origami, introduce general structure origami\npackage, show use applied settings.","code":""},{"path":"origami.html","id":"estimation-roadmap-how-does-it-all-fit-together","chapter":"5 Cross-validation","heading":"5.4 Estimation Roadmap: how does it all fit together?","text":"Similarly defined Roadmap Targeted Learning, \ncan define Estimation Roadmap guide estimation process. \nparticular, developed unified loss-based cross-validation methodology\nestimator construction, selection, performance assessment series \narticles (e.g., see van der Laan, Dudoit, Keles (2004), Dudoit van der Laan (2005),\nVan der Vaart, Dudoit, Laan (2006), van der Laan, Polley, Hubbard (2007)) follow three main steps:loss funtion:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure.\nMap full data loss function observed data loss function, \nexpected value leading efficient estimator risk.loss funtion:\nDefine target parameter minimizer expected loss (risk) \nfull data loss function chosen represent desired performance measure.\nMap full data loss function observed data loss function, \nexpected value leading efficient estimator risk.algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.algorithms:\nConstruct finite collection candidate estimators parameter \ninterest.cross-validation scheme:\nApply appropriate cross-validation select optimal estimator among \ncandidates, assess overall performance resulting estimator.cross-validation scheme:\nApply appropriate cross-validation select optimal estimator among \ncandidates, assess overall performance resulting estimator.Step 1 Estimation Roadmap allows us unify broad range problems\ntraditionally treated separately statistical literature,\nincluding density estimation, prediction polychotomous continuous\noutcomes. example, interested estimating full joint\nconditional density, use negative log-likelihood loss. instead\ninterested conditional mean continuous outcome, one use\nsquared error loss; outcome binary, one resort \nindicator (0-1) loss. unified loss-based framework also reconciles censored\nfull data estimation methods, full data estimators recovered \nspecial cases censored data estimators.","code":""},{"path":"origami.html","id":"example-cross-validation-and-prediction","chapter":"5 Cross-validation","heading":"5.5 Example: cross-validation and prediction","text":"Now introduced Estimation Roadmap, can define objective \nmathematical notation, using prediction example. Let observed\ndata defined \\(X = (W,Y)\\), unit specific data can written \n\\(X_i = (W_i,Y_i)\\), \\(= 1, \\ldots, n\\). \\(n\\) samples, \ndenote \\(Y_i\\) outcome interest (polychotomous continuous), \\(W_i\\)\n\\(p\\)-dimensional set covariates. Let \\(\\psi_0(W)\\) denote target\nparameter interest want estimate; example, interested\nestimating conditional expectation outcome given covariates,\n\\(\\psi_0(W) = E(Y \\mid W)\\). Following Estimation Roadmap, chose \nappropriate loss function, \\(L\\), \\(\\psi_0(W) = \\text{argmin}_{\\psi} E[L(X,\\psi(W))]\\). know \\(\\psi\\) ? order pick\noptimal estimator among candidates, assess overall performance\nresulting estimator, use cross-validation – dividing available data\ntraining set validation set. Observations training set \nused fit (train) estimator, validation set used assess\nrisk (validate) .derive general representation cross-validation, define split\nvector, \\(B_n = (B_n(): = 1, \\ldots, n) \\\\{0,1\\}^n\\). Note split\nvector independent empirical distribution, \\(P_n\\). realization \n\\(B_n\\) defines random split data training validation set \n\n\\[B_n() = 0, \\ \\ \\text{sample training set}\\]\n\\[B_n() = 1, \\ \\ \\text{sample validation set.}\\]\ncan define \\(P_{n,B_n}^0\\) \\(P_{n,B_n}^1\\) empirical\ndistributions training validation sets, respectively. \\(n_0 = \\sum_i 1-B_n()\\) \\(n_1 = \\sum_i B_n()\\) denote number samples \nset. particular distribution split vector \\(B_n\\) defines type \ncross-validation scheme, tailored problem data set hand.","code":""},{"path":"origami.html","id":"cross-validation-schemes-in-origami","chapter":"5 Cross-validation","heading":"5.6 Cross-validation schemes in origami","text":"specified earlier, particular distribution split vector \\(B_n\\)\ndefines type cross-validation method. following, describe\ndifferent types cross-validation schemes available origami package, \nshow use sequel.","code":""},{"path":"origami.html","id":"wash-benefits-study-example","chapter":"5 Cross-validation","heading":"WASH Benefits Study Example","text":"order illustrate different cross-validation schemes, using \nWASH data. Detailed information WASH Benefits Example Dataset can \nfound Chapter 3. particular, interested predicting\nweight--height z-score whz using available covariate data. \nillustration, start treating data independent identically\ndistributed (..d.) random draws. see cross-validation scheme \n, subset data \\(n=30\\). Note row represents \n..d. sample, indexed row number.look first 30 data.","code":"\nlibrary(data.table)\nlibrary(origami)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"origami.html","id":"cross-validation-for-i.i.d.-data","chapter":"5 Cross-validation","heading":"5.6.1 Cross-validation for i.i.d. data","text":"","code":""},{"path":"origami.html","id":"re-substitution","chapter":"5 Cross-validation","heading":"5.6.1.1 Re-substitution","text":"re-substitution method simplest strategy estimating risk\nassociated fitting proposed algorithm set observations. , \nobserved data used training validation set.illustrate usage re-substitution method origami package\n; use function folds_resubstitution(n). order setup\nfolds_resubstitution(n), just need total number samples want \nallocate training validation sets; remember row data \nunique ..d. sample. Notice structure origami output:v: cross-validation foldtraining_set: indexes samples training setvalidation_set: indexes samples training set.structure origami output (fold(s)) persist \ncross-validation schemes present chapter. , show fold\ngenerated re-substitution method:","code":"folds_resubstitution(nrow(washb_data))\n[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n$validation_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"holdout-method","chapter":"5 Cross-validation","heading":"5.6.1.2 Holdout method","text":"holdout method, validation set approach, consists randomly\ndividing available data training set validation set (holdout\nset). model fitted training set, evaluated \nobservations validation set. Typically, data split \n\\(60/40\\), \\(70/30\\) \\(80/20\\) splits.holdout method intuitive, conceptually easy, computationally \ndemanding. However, repeat process randomly splitting data \ntraining validation set, might get different validation loss (e.g.,\nMSE). particular, loss validation sets might highly\nvariable, depending samples included training/validation\nsplit. classification problems, possibility uneven\ndistribution different classes training validation set unless data\nstratified. Finally, note using data train \nevaluate performance proposed algorithm, might result bias.","code":""},{"path":"origami.html","id":"leave-one-out","chapter":"5 Cross-validation","heading":"5.6.1.3 Leave-one-out","text":"leave-one-cross-validation scheme closely related holdout\nmethod. particular, also involves splitting data training \nvalidation set; however, instead partitioning observed data sets \nsimilar size, single observation used validation set. ,\nmajority units employed training (fitting) proposed\nalgorithm. Since one unit (example \\(x_1 = (w_1, y_1)\\)) used \nfitting process, leave-one-cross-validation results possibly less\nbiased estimate true risk; typically, leave-one-approach \noverestimate risk much holdout method. hand, since\nestimate risk based single sample, typically highly\nvariable estimate.can repeat process spiting data training validation set\nsamples part validation set point. example,\nnext iteration cross-validation might \\(x_2 = (w_2,y_2)\\) \nvalidation set rest \\(n-1\\) samples training set. Repeating\napproach \\(n\\) times results , example, \\(n\\) squared errors \\(MSE_1, MSE_2, \\ldots, MSE_n\\). estimate true risk average \n\\(n\\) squared errors. leave-one-cross-validation results less\nbiased (albeit, variable) estimate risk holdout method, \nexpensive implement \\(n\\) large.illustrate usage leave-one-cross-validation origami\npackage ; use function folds_loo(n). order setup\nfolds_loo(n), similarly re-substitution method, just need total\nnumber samples want cross-validate. show first two folds\ngenerated leave-one-cross-validation .","code":"folds <- folds_loo(nrow(washb_data))\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 1\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n[26] 27 28 29 30\n\n$validation_set\n[1] 2\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"v-fold","chapter":"5 Cross-validation","heading":"5.6.1.4 V-fold","text":"alternative leave-one-V-fold cross-validation. \ncross-validation scheme randomly divides data \\(v\\) sets (folds) equal\nsize; fold, number samples validation set .\nV-fold cross-validation, one folds treated validation set,\nwhereas proposed algorithm fit remaining \\(v-1\\) folds \ntraining set. loss, example MSE, computed samples \nvalidation set. proposed algorithm trained performance\nevaluated first fold, repeat process \\(v\\) times; time, \ndifferent group samples treated validation set. Note V-fold\ncross-validation effectively use data train evaluate \nproposed algorithm without overfitting training data. end, \nV-fold cross-validation results \\(v\\) estimates validation error. final\nV-fold CV estimate computed average validation losses.dataset \\(n\\) samples, V-fold cross-validation \\(v=n\\) just\nleave-one-; similarly, set \\(n=1\\), can get holdout method’s\nestimate algorithm’s performance. Despite obvious computational\nadvantages, V-fold cross-validation often gives accurate estimates \ntrue risk. reason comes bias-variance trade-comes\nemploying methods; leave-one-might less biased, \nhigher variance. difference becomes obvious \\(v<<n\\) (\nsmall, increase bias). V-fold cross-validation, end \naveraging output \\(v\\) fits typically less correlated \noutputs leave-one-fits. Since mean many highly correlated\nquantities higher variance, leave-one-estimate risk also\nhigher variance estimate based V-fold cross-validation.Let’s see V-fold cross-validation origami action! next chapter\nstudy Super Learner, actual algorithm fit evaluate\nperformance, uses V-fold default cross-validation scheme. order\nset V-fold CV, need call function folds_vfold(n, V). Arguments\nfolds_vfold(n, V) require total number samples \ncross-validated, number folds want get.\\(V=2\\), get 2 folds \\(n/2\\) number samples training \nvalidation set.","code":"folds <- folds_vfold(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\n$validation_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  5  9 10 13 16 17 18 20 21 25 26 27 29 30\n\n$validation_set\n [1]  2  3  4  6  7  8 11 12 14 15 19 22 23 24 28\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"monte-carlo","chapter":"5 Cross-validation","heading":"5.6.1.5 Monte Carlo","text":"Monte Carlo cross-validation, randomly select fraction data\n(without replacement) form training set; assign rest \nsamples validation set. , data repeatedly randomly\ndivided two sets, training set \\(n_0 = n \\cdot (1-p)\\) observations \nvalidation set \\(n_1 = n \\cdot p\\) observations. process \nrepeated multiple times, generating (random) new training validation\npartitions time.Since partitions independent across folds, sample can appear \nvalidation set multiple times – note stark difference\nMonte Carlo V-fold cross-validation. Monte Carlo\ncross-validation, one able explore many available partitions \nV-fold cross-validation – resulting possibly less variable estimate\nrisk, cost increase bias.illustrate usage Monte Carlo cross-validation origami\npackage using function folds_montecarlo(n, V, pvalidation). order\nsetup folds_montecarlo(n, V, pvalidation), need:total number samples want cross-validate;number folds;proportion observations placed validation set.\\(V=2\\) \\(pvalidation=0.2\\), obtain 2 folds approximately \\(6\\) samples\nvalidation set per fold.","code":"folds <- folds_montecarlo(nrow(washb_data), V = 2, pvalidation = 0.2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1] 19 27 16 29 23 12  1  3 18 11  5  7  8  6  9 22 10 25 20 28 15  2 24 26\n\n$validation_set\n[1]  4 13 14 17 21 30\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 19 15 28 25 29 11 20 17 14  4  9 12 30  8 27 18 16 10 13  6 24  3 26  1\n\n$validation_set\n[1]  2  5  7 21 22 23\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"bootstrap","chapter":"5 Cross-validation","heading":"5.6.1.6 Bootstrap","text":"bootstrap cross-validation also consists randomly selecting samples, \nreplacement, training set. rest samples picked \ntraining set allocated validation set. process repeated\nmultiple times, generating (random) new training validation partitions\ntime. contract Monte Carlo cross-validation, total number \nsamples training validation size across folds constant. also\nsample replacement, hence samples can multiple training\nsets. proportion observations validation sets random\nvariable, expectation \\(\\sim 0.368\\).illustrate usage bootstrap cross-validation origami package\nusing function folds_bootstrap(n, V). order setup\nfolds_bootstrap(n, V), need:total number samples want cross-validate;number folds.\\(V=2\\), obtain \\(2\\) folds different number samples validation\nset across folds.","code":"folds <- folds_bootstrap(nrow(washb_data), V = 2)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  2  5 30  1 29 16 10 11  8 25 28  2 11  2 16 28 15 28  1 27  9 19 20 30 18\n[26] 11 13  2 18 12\n\n$validation_set\n [1]  3  4  6  7 14 17 21 22 23 24 26\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 12 16 10 29 22 15 27  9 27 16 12 28 10 28 26  1 14  6 23 14 21 16  5 20  8\n[26] 23 25  8 27  5\n\n$validation_set\n [1]  2  3  4  7 11 13 17 18 19 24 30\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"cross-validation-for-dependent-data","chapter":"5 Cross-validation","heading":"5.6.2 Cross-validation for dependent data","text":"origami package also supports numerous cross-validation schemes \ntime-series data, single multiple time-series arbitrary time\nnetwork dependence.","code":""},{"path":"origami.html","id":"airpassenger-example","chapter":"5 Cross-validation","heading":"AirPassenger Example","text":"order illustrate different cross-validation schemes time-series, \nusing AirPassenger data; widely used, freely available\ndataset. AirPassenger dataset R provides monthly totals \ninternational airline passengers 1949 1960. dataset already \ntime series class therefore class date manipulation required.Goal: want forecast number airline passengers time \\(h\\)\nhorizon using historical data 1949 1960.","code":"\nlibrary(ggfortify)\n\ndata(AirPassengers)\nAP <- AirPassengers\n\nautoplot(AP) +\n  labs(\n    x = \"Date\",\n    y = \"Passenger numbers (1000's)\",\n    title = \"Air Passengers from 1949 to 1961\"\n  )\n\nt <- length(AP)"},{"path":"origami.html","id":"rolling-origin","chapter":"5 Cross-validation","heading":"5.6.2.1 Rolling origin","text":"Rolling origin cross-validation scheme lends “online” algorithms,\nlarge streams data fit continually, final fit \nconstantly updated data acquired. general, rolling origin\nscheme defines initial training set, iteration size \ntraining set grows \\(m\\) observations reach time \\(t\\) particular\nfold. time points included training set always behind \nvalidation set time points; addition, might gap training\nvalidation times size \\(h\\).illustrate rolling origin cross-validation, show example\n3 folds. , first window size 15 time points, first\ntrain proposed algorithm. evaluate performance 10 time\npoints, gap size 5 training validation time points.\nfollowing fold, train algorithm longer stream data, 25\ntime points, including original 15 started . evaluate \nperformance 10 time points future.\nFIGURE 5.1: Rolling origin CV\nillustrate usage rolling origin cross-validation origami\npackage using function folds_rolling_origin(n, first_window, validation_size, gap, batch). order setup folds_rolling_origin(n, first_window, validation_size, gap, batch), need:total number time points want cross-validatethe size first training setthe size validation setthe gap training validation setthe size update training set per iteration CVOur time-series \\(t=144\\) time points. Setting first_window \\(50\\),\nvalidation_size 10, gap 5 batch 20, get 4 time-series\nfolds; show first two .","code":"folds <- folds_rolling_origin(\n  t,\n  first_window = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-window","chapter":"5 Cross-validation","heading":"5.6.2.2 Rolling window","text":"Instead adding time points training set per iteration, \nrolling window cross-validation scheme “rolls” training sample forward \n\\(m\\) time units. rolling window scheme might considered parametric\nsettings one wishes guard moment parameter drift \ndifficult model explicitly; also efficient computationally\ndemanding settings streaming data, large amounts training\ndata stored. contrast rolling origin CV, training sample \niteration rolling window scheme always .illustrate rolling window cross-validation 3 time-series folds\n. first window size 15 time points, first train \nproposed algorithm. previous illustration, evaluate performance\n10 time points, gap size 5 training validation time\npoints. However, next fold, train algorithm time points\naway origin (, 10 time points). Note size \ntraining set new fold first fold (15 time points).\nsetup keeps training sets comparable time (fold) compared\nrolling origin CV. evaluate performance proposed\nalgorithm 10 time points future.\nFIGURE 5.2: Rolling window CV\nillustrate usage rolling window cross-validation origami\npackage using function folds_rolling_window(n, window_size, validation_size, gap, batch). order setup folds_rolling_window(n, window_size, validation_size, gap, batch), need:total number time points want cross-validatethe size training setsthe size validation setthe gap training validation setthe size update training set per iteration CVSetting window_size \\(50\\), validation_size 10, gap 5 \nbatch 20, also get 4 time-series folds; show first two .","code":"folds <- folds_rolling_window(\n  t,\n  window_size = 50, validation_size = 10, gap = 5, batch = 20\n)\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$validation_set\n [1] 56 57 58 59 60 61 62 63 64 65\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n$validation_set\n [1] 76 77 78 79 80 81 82 83 84 85\n\nattr(,\"class\")\n[1] \"fold\""},{"path":"origami.html","id":"rolling-origin-with-v-fold","chapter":"5 Cross-validation","heading":"5.6.2.3 Rolling origin with V-fold","text":"variant rolling origin scheme accounts sample dependence \nrolling-origin-\\(V\\)-fold cross-validation. contrast canonical rolling\norigin CV, samples training validation set , \nvariant encompasses \\(V\\)-fold CV addition time-series setup. \npredictions evaluated future times time-series units seen\ntraining step, allowing dependence samples time. One\ncan use rolling-origin-\\(v\\)-fold cross-validation origami package\nusing function folds_vfold_rolling_origin_pooled(n, t, id, time, V, first_window, validation_size, gap, batch). figure , show \\(V=2\\)\n\\(V\\)-folds, 2 time-series CV folds.\nFIGURE 5.3: Rolling origin V-fold CV\n","code":""},{"path":"origami.html","id":"rolling-window-with-v-fold","chapter":"5 Cross-validation","heading":"5.6.2.4 Rolling window with v-fold","text":"Analogous previous section, can extend rolling window CV support\nmultiple time-series arbitrary sample dependence. One can use \nrolling-window-\\(V\\)-fold cross-validation origami package using \nfunction folds_vfold_rolling_window_pooled(n, t, id, time, V, window_size, validation_size, gap, batch). figure , show \\(V=2\\) \\(V\\)-folds, \n2 time-series CV folds.\nFIGURE 5.4: Rolling window V-fold CV\n","code":""},{"path":"origami.html","id":"general-workflow-of-origami","chapter":"5 Cross-validation","heading":"5.7 General workflow of origami","text":"dive details, let’s take moment review \nbasic functionality origami R package. main function origami\ncross_validate. start , user must define folds function\noperates fold. passed cross_validate, \nfunction map fold-specific function across folds, combining \nresults reasonable way. see action later sections; \nnow, provide specific details step process .","code":""},{"path":"origami.html","id":"define-folds","chapter":"5 Cross-validation","heading":"5.7.1 (1) Define folds","text":"folds object passed cross_validate list folds; lists can\ngenerated using make_folds function. fold consists list \ntraining index vector, validation index vector, fold_index (\norder list folds). function supports variety \ncross-validation schemes describe following section. make_folds\ncan balance across levels variable (strata_ids), can also keep\nobservations independent unit together (cluster).","code":""},{"path":"origami.html","id":"define-fold-function","chapter":"5 Cross-validation","heading":"5.7.2 (2) Define fold function","text":"cv_fun argument cross_validate function perform \noperation fold. first argument function must fold,\nreceive individual fold object operate . Additional arguments\ncan passed cv_fun using ... argument cross_validate. Within\nfunction, convenience functions training, validation \nfold_index can return various components fold object. training\nvalidation passed object, index sensible way.\ninstance, vector, index vector directly. \ndata.frame matrix, index rows. allows user easily\npartition data training validation sets. fold function must return\nnamed list results containing whatever fold-specific outputs generated.","code":""},{"path":"origami.html","id":"apply-cross_validate","chapter":"5 Cross-validation","heading":"5.7.3 (3) Apply cross_validate","text":"defining folds, cross_validate can used map cv_fun across\nfolds using future_lapply. means can easily parallelized\nspecifying parallelization scheme (.e., plan future\nparallelization framework R\n(Bengtsson 2020)). application cross_validate generates list\nresults. described , call cv_fun returns list \nresults, different elements type result care . main\nloop generates list individual lists results (sort \n“meta-list”). “meta-list” inverted one element\nper result type (list results fold). default,\ncombine_results used combine results type lists sensible\nmanner. results combined determined automatically examining \ndata types results first fold. can modified \nspecifying list arguments .combine_control.","code":""},{"path":"origami.html","id":"cross-validation-in-action","chapter":"5 Cross-validation","heading":"5.8 Cross-validation in action","text":"Let’s see origami action! following chapter learn use\ncross-validation Super Learner, can utilize power \ncross-validation build optimal ensembles algorithms, just use \nsingle statistical learning method.","code":""},{"path":"origami.html","id":"cross-validation-with-linear-regression","chapter":"5 Cross-validation","heading":"5.8.1 Cross-validation with linear regression","text":"First, load relevant R packages, set seed, load full\nWASH data . order illustrate cross-validation origami \nlinear regression, focus predicting weight--height Z-score\nwhz using available covariate data. stated previously, \nassume data independent identically distributed, ignoring cluster\nstructure imposed clinical trial design. sake illustration, \nwork subset data, remove samples missing data \ndataset; learn next chapter deal missingness.’s look data:can see covariates used prediction:Next, fit linear model full data, goal predicting \nweight--height Z-score whz using available covariate data. Let’s\ntry :can assess well model fits data comparing predictions \nlinear model true outcomes observed data set. well\nknown (standard) mean squared error. can extract lm model\nobject like :mean squared error 0.86568. important problem arises\nassess model way - , trained linear\nregression model full data set assessed error full data\nset, using data. , course, generally interested \nwell model explains variation observed data; rather, \ninterested explanation provided model generalizes target\npopulation sample presumably derived. used \navailable data, honestly evaluate well model fits (thus\nexplains) variation population level.resolve issue, cross-validation allows particular procedure (e.g.,\nlinear regression) implemented subsets data, evaluating \nwell procedure fits testing (“validation”) set, thereby providing \nhonest evaluation error.can easily add cross-validation linear regression procedure using\norigami. First, let us define new function perform linear regression \nspecific partition data (called “fold”):cv_lm function rather simple: merely split available data \ntraining validation sets, using eponymous functions provided \norigami, fit linear model training set, evaluate model \ntesting set. simple example origami considers \ncv_fun – functions using cross-validation perform particular routine\ninput data set. defined function, can simply generate \nset partitions using origami’s make_folds function, apply cv_lm\nfunction resultant folds object. , replicate \nre-substitution estimate error – “hand” – using\nfunctions make_folds cv_lm.(nearly) matches estimate error obtained .can honestly evaluate error V-fold cross-validation, \npartitions data \\(v\\) subsets, fitting model \\(v - 1\\) \nsubsets evaluating subset held testing. \nrepeated subset used testing. can easily apply \ncv_lm function using origami’s cross_validate (n.b., default \nperforms 10-fold cross-validation):performed 10-fold cross-validation, quickly notice previous\nestimate model error (resubstitution) bit optimistic. honest\nestimate error larger.","code":"\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# Remove missing data, then pick just the first 500 rows\nwashb_data <- washb_data %>%\n  drop_na() %>%\n  slice(1:500)\n\noutcome <- \"whz\"\ncovars <- colnames(washb_data)[-which(names(washb_data) == outcome)]outcome\n[1] \"whz\"\ncovars\n [1] \"tr\"             \"fracode\"        \"month\"          \"aged\"          \n [5] \"sex\"            \"momage\"         \"momedu\"         \"momheight\"     \n [9] \"hfiacat\"        \"Nlt18\"          \"Ncomp\"          \"watmin\"        \n[13] \"elec\"           \"floor\"          \"walls\"          \"roof\"          \n[17] \"asset_wardrobe\" \"asset_table\"    \"asset_chair\"    \"asset_khat\"    \n[21] \"asset_chouki\"   \"asset_tv\"       \"asset_refrig\"   \"asset_bike\"    \n[25] \"asset_moto\"     \"asset_sewmach\"  \"asset_mobile\"  lm_mod <- lm(whz ~ ., data = washb_data)\nsummary(lm_mod)\n\nCall:\nlm(formula = whz ~ ., data = washb_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8890 -0.6799 -0.0169  0.6595  3.1005 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                     -1.89006    1.72022   -1.10   0.2725   \ntrHandwashing                   -0.25276    0.17032   -1.48   0.1385   \ntrNutrition                     -0.09695    0.15696   -0.62   0.5371   \ntrNutrition + WSH               -0.09587    0.16528   -0.58   0.5622   \ntrSanitation                    -0.27702    0.15846   -1.75   0.0811 . \ntrWSH                           -0.02846    0.15967   -0.18   0.8586   \ntrWater                         -0.07148    0.15813   -0.45   0.6515   \nfracodeN05160                    0.62355    0.30719    2.03   0.0430 * \nfracodeN05265                    0.38762    0.31011    1.25   0.2120   \nfracodeN05359                    0.10187    0.31329    0.33   0.7452   \nfracodeN06229                    0.30933    0.29766    1.04   0.2993   \nfracodeN06453                    0.08066    0.30006    0.27   0.7882   \nfracodeN06458                    0.43707    0.29970    1.46   0.1454   \nfracodeN06473                    0.45406    0.30912    1.47   0.1426   \nfracodeN06479                    0.60994    0.31463    1.94   0.0532 . \nfracodeN06489                    0.25923    0.31901    0.81   0.4169   \nfracodeN06500                    0.07539    0.35794    0.21   0.8333   \nfracodeN06502                    0.36748    0.30504    1.20   0.2290   \nfracodeN06505                    0.20038    0.31560    0.63   0.5258   \nfracodeN06516                    0.55455    0.29807    1.86   0.0635 . \nfracodeN06524                    0.49429    0.31423    1.57   0.1164   \nfracodeN06528                    0.75966    0.31060    2.45   0.0148 * \nfracodeN06531                    0.36856    0.30155    1.22   0.2223   \nfracodeN06862                    0.56932    0.29293    1.94   0.0526 . \nfracodeN08002                    0.36779    0.26846    1.37   0.1714   \nmonth                            0.17161    0.10865    1.58   0.1149   \naged                            -0.00336    0.00112   -3.00   0.0029 **\nsexmale                          0.12352    0.09203    1.34   0.1802   \nmomage                          -0.01379    0.00973   -1.42   0.1570   \nmomeduPrimary (1-5y)            -0.13214    0.15225   -0.87   0.3859   \nmomeduSecondary (>5y)            0.12632    0.16041    0.79   0.4314   \nmomheight                        0.00512    0.00919    0.56   0.5776   \nhfiacatMildly Food Insecure      0.05804    0.19341    0.30   0.7643   \nhfiacatModerately Food Insecure -0.01362    0.12887   -0.11   0.9159   \nhfiacatSeverely Food Insecure   -0.13447    0.25418   -0.53   0.5970   \nNlt18                           -0.02557    0.04060   -0.63   0.5291   \nNcomp                            0.00179    0.00762    0.23   0.8145   \nwatmin                           0.01347    0.00861    1.57   0.1182   \nelec                             0.08906    0.10700    0.83   0.4057   \nfloor                           -0.17763    0.17734   -1.00   0.3171   \nwalls                           -0.03001    0.21445   -0.14   0.8888   \nroof                            -0.03716    0.49214   -0.08   0.9399   \nasset_wardrobe                  -0.05754    0.13736   -0.42   0.6755   \nasset_table                     -0.22079    0.12276   -1.80   0.0728 . \nasset_chair                      0.28012    0.13750    2.04   0.0422 * \nasset_khat                       0.02306    0.11766    0.20   0.8447   \nasset_chouki                    -0.13943    0.14084   -0.99   0.3227   \nasset_tv                         0.17723    0.12972    1.37   0.1726   \nasset_refrig                     0.12613    0.23162    0.54   0.5863   \nasset_bike                      -0.02568    0.10083   -0.25   0.7990   \nasset_moto                      -0.32094    0.19944   -1.61   0.1083   \nasset_sewmach                    0.05090    0.17795    0.29   0.7750   \nasset_mobile                     0.01420    0.14972    0.09   0.9245   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.984 on 447 degrees of freedom\nMultiple R-squared:  0.129, Adjusted R-squared:  0.0277 \nF-statistic: 1.27 on 52 and 447 DF,  p-value: 0.104(err <- mean(resid(lm_mod)^2))\n[1] 0.86568\ncv_lm <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # split up data into training and validation sets\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit linear model on training set and predict on validation set\n  mod <- lm(as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # capture results to be returned as output\n  out <- list(\n    coef = data.frame(t(coef(mod))),\n    SE = (preds - valid_data[, out_var_ind])^2\n  )\n  return(out)\n}# re-substitution estimate\nresub <- make_folds(washb_data, fold_fun = folds_resubstitution)[[1]]\nresub_results <- cv_lm(fold = resub, data = washb_data, reg_form = \"whz ~ .\")\nmean(resub_results$SE, na.rm = TRUE)\n[1] 0.86568# cross-validated estimate\nfolds <- make_folds(washb_data)\ncvlm_results <- cross_validate(\n  cv_fun = cv_lm, folds = folds, data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvlm_results$SE, na.rm = TRUE)\n[1] 1.35"},{"path":"origami.html","id":"cross-validation-with-random-forests","chapter":"5 Cross-validation","heading":"5.8.2 Cross-validation with random forests","text":"examine origami , let us return example analysis using \nWASH data set. , write new cv_fun type object. example, \nuse Breiman’s randomForest (Breiman 2001):, writing cv_rf function cross-validate randomForest, used\nprevious function cv_lm example. now, individual cv_fun must\nwritten hand; however, future releases, wrapper may available \nsupport auto-generating cv_funs used origami., use cross_validate apply new cv_rf function folds\nobject generated make_folds.Using 10-fold cross-validation (default), obtain honest estimate \nprediction error random forests. , gather use \norigami’s cross_validate procedure can generalized arbitrary estimation\ntechniques, given availability appropriate cv_fun function.","code":"\n# make sure to load the package!\nlibrary(randomForest)\n\ncv_rf <- function(fold, data, reg_form) {\n  # get name and index of outcome variable from regression formula\n  out_var <- as.character(unlist(str_split(reg_form, \" \"))[1])\n  out_var_ind <- as.numeric(which(colnames(data) == out_var))\n\n  # define training and validation sets based on input object of class \"folds\"\n  train_data <- training(data)\n  valid_data <- validation(data)\n\n  # fit Random Forest regression on training set and predict on holdout set\n  mod <- randomForest(formula = as.formula(reg_form), data = train_data)\n  preds <- predict(mod, newdata = valid_data)\n  valid_data <- as.data.frame(valid_data)\n\n  # define output object to be returned as list (for flexibility)\n  out <- list(\n    coef = data.frame(mod$coefs),\n    SE = ((preds - valid_data[, out_var_ind])^2)\n  )\n  return(out)\n}# now, let's cross-validate...\nfolds <- make_folds(washb_data)\ncvrf_results <- cross_validate(\n  cv_fun = cv_rf, folds = folds, data = washb_data, reg_form = \"whz ~ .\",\n  use_future = FALSE\n)\nmean(cvrf_results$SE)\n[1] 1.0271"},{"path":"origami.html","id":"cross-validation-with-arima","chapter":"5 Cross-validation","heading":"5.8.3 Cross-validation with arima","text":"Cross-validation can also used forecast model selection time series\nsetting. , partitioning scheme mirrors application \nforecasting model: ’ll train data past observations (either \navailable recent subset), use model fit predict next\nobservations. consider AirPassengers dataset , monthly time\nseries passenger air traffic thousands people.Suppose want pick two forecasting models different arima\nconfigurations. can evaluating forecasting performance.\nFirst, set appropriate cross-validation scheme time-series.default, folds_rolling_origin increase size training set \none time point fold. followed default option, 85\nfolds train! Luckily, can pass batch option \nfolds_rolling_origin tells increase size training set \n10 points iteration. Since want forecast immediate next point,\ngap argument remains default (0).arima model AR component seems better fit data.","code":"data(AirPassengers)\nprint(AirPassengers)\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432folds <- make_folds(AirPassengers,\n  fold_fun = folds_rolling_origin,\n  first_window = 36, validation_size = 24, batch = 10\n)\n\n# How many folds where generated?\nlength(folds)\n[1] 9\n\n# Examine the first 2 folds.\nfolds[[1]]\n$v\n[1] 1\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n$validation_set\n [1] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\nattr(,\"class\")\n[1] \"fold\"\nfolds[[2]]\n$v\n[1] 2\n\n$training_set\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n\n$validation_set\n [1] 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\nattr(,\"class\")\n[1] \"fold\"# make sure to load the package!\nlibrary(forecast)\n\n# function to calculate cross-validated squared error\ncv_forecasts <- function(fold, data) {\n  # Get training and validation data\n  train_data <- training(data)\n  valid_data <- validation(data)\n  valid_size <- length(valid_data)\n\n  train_ts <- ts(log10(train_data), frequency = 12)\n\n  # First arima model\n  arima_fit <- arima(train_ts, c(0, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred <- predict(arima_fit, n.ahead = valid_size)\n  arima_pred <- 10^raw_arima_pred$pred\n  arima_MSE <- mean((arima_pred - valid_data)^2)\n\n  # Second arima model\n  arima_fit2 <- arima(train_ts, c(5, 1, 1),\n    seasonal = list(\n      order = c(0, 1, 1),\n      period = 12\n    )\n  )\n  raw_arima_pred2 <- predict(arima_fit2, n.ahead = valid_size)\n  arima_pred2 <- 10^raw_arima_pred2$pred\n  arima_MSE2 <- mean((arima_pred2 - valid_data)^2)\n\n  out <- list(mse = data.frame(\n    fold = fold_index(),\n    arima = arima_MSE, arima2 = arima_MSE2\n  ))\n  return(out)\n}\n\nmses <- cross_validate(\n  cv_fun = cv_forecasts, folds = folds, data = AirPassengers,\n  use_future = FALSE\n)\nmses$mse\n  fold   arima  arima2\n1    1   68.21  137.28\n2    2  319.68  313.15\n3    3  578.35  713.36\n4    4  428.69  505.31\n5    5  407.33  371.27\n6    6  281.82  250.99\n7    7  827.56  910.12\n8    8 2099.59 2213.15\n9    9  398.37  293.38\ncolMeans(mses$mse[, c(\"arima\", \"arima2\")])\n arima arima2 \n601.07 634.22 "},{"path":"origami.html","id":"exercises","chapter":"5 Cross-validation","heading":"5.9 Exercises","text":"","code":""},{"path":"origami.html","id":"review-of-key-concepts","chapter":"5 Cross-validation","heading":"5.9.1 Review of Key Concepts","text":"Compare contrast V-fold cross-validation resubstitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.Compare contrast V-fold cross-validation resubstitution\ncross-validation. differences two methods?\nsimilar? Describe scenario use one \n.advantages disadvantages \\(v\\)-fold CV relative :\nholdout CV?\nleave-one-CV?\nholdout CV?leave-one-CV?can’t use V-fold cross-validation time-series data?can’t use V-fold cross-validation time-series data?use rolling window origin non-stationary time-series? ?use rolling window origin non-stationary time-series? ?","code":""},{"path":"origami.html","id":"the-ideas-in-action","chapter":"5 Cross-validation","heading":"5.9.2 The Ideas in Action","text":"Let \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\). kind \ncross-validation use rare outcome? can \norigami package?Let \\(Y\\) binary variable \\(P(Y=1 \\mid W) = 0.01\\). kind \ncross-validation use rare outcome? can \norigami package?Consider WASH benefits dataset presented chapter. can \ninclude cluster information cross-validation? can \norigami package?Consider WASH benefits dataset presented chapter. can \ninclude cluster information cross-validation? can \norigami package?","code":""},{"path":"origami.html","id":"advanced-topics","chapter":"5 Cross-validation","heading":"5.9.3 Advanced Topics","text":"Think dataset arbitrary spatial dependence, know\nextent dependence, groups formed dependence clear\nspillover effects. kind cross-validation can use?Think dataset arbitrary spatial dependence, know\nextent dependence, groups formed dependence clear\nspillover effects. kind cross-validation can use?Continuing last problem, kind procedure, cross-validation\nmethod, can use spatial dependence clearly defined \nprevious problem?Continuing last problem, kind procedure, cross-validation\nmethod, can use spatial dependence clearly defined \nprevious problem?Consider classification problem large number predictors. \nstatistician proposes following analysis:\nFirst screen predictors, leaving covariates strong\ncorrelation class labels.\nFit algorithm using subset highly correlated covariates.\nUse cross-validation estimate tuning parameters performance\nproposed algorithm.\ncorrect application cross-validation? ?Consider classification problem large number predictors. \nstatistician proposes following analysis:First screen predictors, leaving covariates strong\ncorrelation class labels.Fit algorithm using subset highly correlated covariates.Use cross-validation estimate tuning parameters performance\nproposed algorithm.correct application cross-validation? ?","code":""},{"path":"tmle3.html","id":"tmle3","chapter":"6 The TMLE Framework","heading":"6 The TMLE Framework","text":"Jeremy CoyleBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learn-tmle","chapter":"6 The TMLE Framework","heading":"6.1 Learning Objectives","text":"end chapter, able toUnderstand use TMLE effect estimation.Use tmle3 estimate Average Treatment Effect (ATE).Understand use tmle3 “Specs” objects.Fit tmle3 custom set target parameters.Use delta method estimate transformations target parameters.","code":""},{"path":"tmle3.html","id":"tmle-intro","chapter":"6 The TMLE Framework","heading":"6.2 Introduction","text":"previous chapter sl3 learned estimate regression\nfunction like \\(\\mathbb{E}[Y \\mid X]\\) data. ’s important first step\nlearning data, can use predictive model estimate\nstatistical causal effects?Going back roadmap targeted learning, suppose ’d like \nestimate effect treatment variable \\(\\) outcome \\(Y\\). discussed,\none potential parameter characterizes effect Average Treatment\nEffect (ATE), defined \\(\\psi_0 = \\mathbb{E}_W[\\mathbb{E}[Y \\mid =1,W] - \\mathbb{E}[Y \\mid =0,W]]\\) interpreted difference mean outcome\ntreatment \\(=1\\) \\(=0\\), averaging distribution \ncovariates \\(W\\). ’ll illustrate several potential estimators \nparameter, motivate use TMLE (targeted maximum likelihood\nestimation; targeted minimum loss-based estimation) framework, using \nfollowing example data:small ticks right indicate mean outcomes (averaging \\(W\\))\n\\(=1\\) \\(=0\\) respectively, difference quantity ’d\nlike estimate.hope motivate application TMLE chapter, refer \ninterested reader two Targeted Learning books associated works \nfull technical details.","code":""},{"path":"tmle3.html","id":"substitution-est","chapter":"6 The TMLE Framework","heading":"6.3 Substitution Estimators","text":"can use sl3 fit Super Learner regression model estimate\noutcome regression function \\(\\mathbb{E}_0[Y \\mid ,W]\\), often refer\n\\(\\overline{Q}_0(,W)\\) whose estimate denote \\(\\overline{Q}_n(,W)\\).\nconstruct estimate ATE \\(\\psi_n\\), need “plug-” \nestimates \\(\\overline{Q}_n(,W)\\), evaluated two intervention contrasts,\ncorresponding ATE “plug-” formula:\n\\(\\psi_n = \\frac{1}{n}\\sum(\\overline{Q}_n(1,W)-\\overline{Q}_n(0,W))\\). kind\nestimator called plug-substitution estimator, since accurate\nestimates \\(\\psi_n\\) parameter \\(\\psi_0\\) may obtained substituting\nestimates \\(\\overline{Q}_n(,W)\\) relevant regression functions\n\\(\\overline{Q}_0(,W)\\) .Applying sl3 estimate outcome regression example, can see\nensemble machine learning predictions fit data quite well:solid lines indicate sl3 estimate regression function, \ndotted lines indicating tmle3 updates (described ).substitution estimators intuitive, naively using approach \nSuper Learner estimate \\(\\overline{Q}_0(,W)\\) several limitations. First,\nSuper Learner selecting learner weights minimize risk across entire\nregression function, instead “targeting” ATE parameter hope \nestimate, leading biased estimation. , sl3 trying well \nfull regression curve left, instead focusing small ticks \nright. ’s , sampling distribution approach \nasymptotically linear, therefore inference possible.can see limitations illustrated estimates generated \nexample data:see Super Learner, estimates true parameter value (indicated \ndashed vertical line) accurately GLM. However, still less\naccurate TMLE, valid inference possible. contrast, TMLE\nachieves less biased estimator valid inference.","code":""},{"path":"tmle3.html","id":"tmle","chapter":"6 The TMLE Framework","heading":"6.4 Targeted Maximum Likelihood Estimation","text":"TMLE takes initial estimate \\(\\overline{Q}_n(,W)\\) well estimate \npropensity score \\(g_n(\\mid W) = \\mathbb{P}(= 1 \\mid W)\\) produces \nupdated estimate \\(\\overline{Q}^{\\star}_n(,W)\\) “targeted” \nparameter interest. TMLE keeps benefits substitution estimators (\none), augments original, potentially erratic estimates correct \nbias also resulting asymptotically linear (thus normally\ndistributed) estimator accommodates inference via asymptotically consistent\nWald-style confidence intervals.","code":""},{"path":"tmle3.html","id":"tmle-updates","chapter":"6 The TMLE Framework","heading":"6.4.1 TMLE Updates","text":"different types TMLEs (, sometimes, multiple set \ntarget parameters) – , give example algorithm TML\nestimation ATE. \\(\\overline{Q}^{\\star}_n(,W)\\) TMLE-augmented\nestimate \\(f(\\overline{Q}^{\\star}_n(,W)) = f(\\overline{Q}_n(,W)) + \\epsilon \\cdot H_n(,W)\\), \\(f(\\cdot)\\) appropriate link function (e.g.,\n\\(\\text{logit}(x) = \\log(x / (1 - x))\\)), estimate \\(\\epsilon_n\\) \ncoefficient \\(\\epsilon\\) “clever covariate” \\(H_n(,W)\\) computed. \nform covariate \\(H_n(,W)\\) differs across target parameters; case\nATE, \\(H_n(,W) = \\frac{}{g_n(\\mid W)} - \\frac{1-}{1-g_n(, W)}\\), \\(g_n(,W) = \\mathbb{P}(=1 \\mid W)\\) estimated propensity\nscore, estimator depends initial fit (sl3) \noutcome regression (\\(\\overline{Q}_n\\)) propensity score (\\(g_n\\)).several robust augmentations used across tlverse,\nincluding use additional layer cross-validation avoid\n-fitting bias (.e., CV-TMLE) well approaches consistently\nestimating several parameters simultaneously (e.g., points survival\ncurve).","code":""},{"path":"tmle3.html","id":"tmle-infer","chapter":"6 The TMLE Framework","heading":"6.4.2 Statistical Inference","text":"Since TMLE yields asymptotically linear estimator, obtaining statistical\ninference convenient. TML estimator corresponding\n(efficient) influence function (often, “EIF”, short) describes \nasymptotic distribution estimator. using estimated EIF, Wald-style\ninference (asymptotically correct confidence intervals) can constructed\nsimply plugging form EIF initial estimates\n\\(\\overline{Q}_n\\) \\(g_n\\), computing sample standard error.following sections describe simple detailed way \nspecifying estimating TMLE tlverse. designing tmle3, \nsought replicate closely possible general estimation framework\nTMLE, theoretical object relevant TMLE encoded \ncorresponding software object/method. First, present simple\napplication tmle3 WASH Benefits example, go describe\nunderlying objects greater detail.","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"6 The TMLE Framework","heading":"6.5 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating average treatment effect.","code":""},{"path":"tmle3.html","id":"load-the-data","chapter":"6 The TMLE Framework","heading":"6.5.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tmle3)\nlibrary(sl3)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"6 The TMLE Framework","heading":"6.5.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\)\n(outcome) data structure. tmle3 needs know variables dataset\ncorrespond roles. use list character vectors tell\n. call “Node List” corresponds nodes Directed\nAcyclic Graph (DAG), way displaying causal relationships variables.","code":"\nnode_list <- list(\n  W = c(\n    \"month\", \"aged\", \"sex\", \"momage\", \"momedu\",\n    \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\", \"asset_wardrobe\",\n    \"asset_table\", \"asset_chair\", \"asset_khat\",\n    \"asset_chouki\", \"asset_tv\", \"asset_refrig\",\n    \"asset_bike\", \"asset_moto\", \"asset_sewmach\",\n    \"asset_mobile\"\n  ),\n  A = \"tr\",\n  Y = \"whz\"\n)"},{"path":"tmle3.html","id":"handle-missingness","chapter":"6 The TMLE Framework","heading":"6.5.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median- (continuous) mode- (discrete)\nimputed, additional covariates indicating imputation generated, just\ndescribed sl3 chapter.Missing treatment variables excluded – observations dropped.Missing outcomes efficiently handled automatic calculation (\nincorporation estimators) inverse probability censoring weights\n(IPCW); also known IPCW-TMLE may thought joint\nintervention remove missingness analogous procedure used \nclassical inverse probability weighted estimators.steps implemented process_missing function tmle3:","code":"\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list"},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"6 The TMLE Framework","heading":"6.5.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set components specification\n(“Spec”) , minimal additional detail, can run fit TMLE.’ll start using one specs, work way \ninternals tmle3.","code":"\nate_spec <- tmle_ATE(\n  treatment_level = \"Nutrition + WSH\",\n  control_level = \"Control\"\n)"},{"path":"tmle3.html","id":"define-the-learners","chapter":"6 The TMLE Framework","heading":"6.5.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":"\n# choose base learners\nlrnr_mean <- make_learner(Lrnr_mean)\nlrnr_rf <- make_learner(Lrnr_ranger)\n\n# define metalearners appropriate to data types\nls_metalearner <- make_learner(Lrnr_nnls)\nmn_metalearner <- make_learner(\n  Lrnr_solnp, metalearner_linear_multinomial,\n  loss_loglik_multinomial\n)\nsl_Y <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = ls_metalearner\n)\nsl_A <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = mn_metalearner\n)\nlearner_list <- list(A = sl_A, Y = sl_Y)"},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"6 The TMLE Framework","heading":"6.5.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)\nprint(tmle_fit)\nA tmle3_Fit that took 1 step(s)\n   type                                    param   init_est tmle_est       se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0031611 0.010044 0.050853\n       lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.089626 0.10971        0.010044         -0.089626           0.10971"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"6 The TMLE Framework","heading":"6.5.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"estimates <- tmle_fit$summary$psi_transformed\nprint(estimates)\n[1] 0.010044"},{"path":"tmle3.html","id":"tmle3-components","chapter":"6 The TMLE Framework","heading":"6.6 tmle3 Components","text":"Now ’ve successfully used spec obtain TML estimate, let’s look\nhood components. spec number functions \ngenerate objects necessary define fit TMLE.","code":""},{"path":"tmle3.html","id":"tmle3_task","chapter":"6 The TMLE Framework","heading":"6.6.1 tmle3_task","text":"First , tmle3_Task, analogous sl3_Task, containing data ’re\nfitting TMLE , well NPSEM generated node_list\ndefined , describing variables relationships.","code":"\ntmle_task <- ate_spec$make_tmle_task(washb_data, node_list)tmle_task$npsem\n$W\ntmle3_Node: W\n    Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight\n    Parents: \n\n$A\ntmle3_Node: A\n    Variables: tr\n    Parents: W\n\n$Y\ntmle3_Node: Y\n    Variables: whz\n    Parents: A, W"},{"path":"tmle3.html","id":"initial-likelihood","chapter":"6 The TMLE Framework","heading":"6.6.2 Initial Likelihood","text":"Next, object representing likelihood, factorized according \nNPSEM described :components likelihood indicate factors estimated: \nmarginal distribution \\(W\\) estimated using NP-MLE, conditional\ndistributions \\(\\) \\(Y\\) estimated using sl3 fits (defined \nlearner_list) .can use tandem tmle_task object obtain likelihood\nestimates observation:","code":"\ninitial_likelihood <- ate_spec$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\nprint(initial_likelihood)\nW: Lf_emp\nA: LF_fit\nY: LF_fitinitial_likelihood$get_likelihoods(tmle_task)\n               W       A        Y\n   1: 0.00021299 0.34702 -0.32696\n   2: 0.00021299 0.37305 -0.88218\n   3: 0.00021299 0.34685 -0.79300\n   4: 0.00021299 0.33625 -0.89157\n   5: 0.00021299 0.34098 -0.63477\n  ---                            \n4691: 0.00021299 0.24334 -0.61095\n4692: 0.00021299 0.24620 -0.21534\n4693: 0.00021299 0.22401 -0.79223\n4694: 0.00021299 0.27641 -0.94319\n4695: 0.00021299 0.20158 -1.08201"},{"path":"tmle3.html","id":"targeted-likelihood-updater","chapter":"6 The TMLE Framework","heading":"6.6.3 Targeted Likelihood (updater)","text":"also need define “Targeted Likelihood” object. special type\nlikelihood able updated using tmle3_Update object. \nobject defines update strategy (e.g., submodel, loss function, CV-TMLE \n).constructing targeted likelihood, can specify different update\noptions. See documentation tmle3_Update details different\noptions. example, can disable CV-TMLE (default tmle3) \nfollows:","code":"\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\ntargeted_likelihood_no_cv <-\n  Targeted_Likelihood$new(initial_likelihood,\n    updater = list(cvtmle = FALSE)\n  )"},{"path":"tmle3.html","id":"parameter-mapping","chapter":"6 The TMLE Framework","heading":"6.6.4 Parameter Mapping","text":"Finally, need define parameters interest. , spec defines \nsingle parameter, ATE. next section, ’ll see add additional\nparameters.","code":"tmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)\nprint(tmle_params)\n[[1]]\nParam_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]"},{"path":"tmle3.html","id":"putting-it-all-together","chapter":"6 The TMLE Framework","heading":"6.6.5 Putting it all together","text":"used spec manually generate components, can now\nmanually fit tmle3:result equivalent fitting using tmle3 function .","code":"tmle_fit_manual <- fit_tmle3(\n  tmle_task, targeted_likelihood, tmle_params,\n  targeted_likelihood$updater\n)\nprint(tmle_fit_manual)\nA tmle3_Fit that took 1 step(s)\n   type                                    param   init_est tmle_est       se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0062324 0.017515 0.050591\n       lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.081641 0.11667        0.017515         -0.081641           0.11667"},{"path":"tmle3.html","id":"fitting-tmle3-with-multiple-parameters","chapter":"6 The TMLE Framework","heading":"6.7 Fitting tmle3 with multiple parameters","text":", fit tmle3 just one parameter. tmle3 also supports fitting\nmultiple parameters simultaneously. illustrate , ’ll use \ntmle_TSM_all spec:spec generates Treatment Specific Mean (TSM) level \nexposure variable. Note must first generate new targeted likelihood,\nold one targeted ATE. However, can recycle initial\nlikelihood fit , saving us super learner step.","code":"tsm_spec <- tmle_TSM_all()\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\nall_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)\nprint(all_tsm_params)\n[[1]]\nParam_TSM: E[Y_{A=Control}]\n\n[[2]]\nParam_TSM: E[Y_{A=Handwashing}]\n\n[[3]]\nParam_TSM: E[Y_{A=Nutrition}]\n\n[[4]]\nParam_TSM: E[Y_{A=Nutrition + WSH}]\n\n[[5]]\nParam_TSM: E[Y_{A=Sanitation}]\n\n[[6]]\nParam_TSM: E[Y_{A=WSH}]\n\n[[7]]\nParam_TSM: E[Y_{A=Water}]"},{"path":"tmle3.html","id":"delta-method","chapter":"6 The TMLE Framework","heading":"6.7.1 Delta Method","text":"can also define parameters based Delta Method Transformations \nparameters. instance, can estimate ATE using delta method two\nTSM parameters:can similarly used estimate derived parameters like Relative\nRisks, Population Attributable Risks","code":"ate_param <- define_param(\n  Param_delta, targeted_likelihood,\n  delta_param_ATE,\n  list(all_tsm_params[[1]], all_tsm_params[[4]])\n)\nprint(ate_param)\nParam_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]"},{"path":"tmle3.html","id":"fit","chapter":"6 The TMLE Framework","heading":"6.7.2 Fit","text":"can now fit TMLE simultaneously TSM parameters, well \ndefined ATE parameter","code":"all_params <- c(all_tsm_params, ate_param)\n\ntmle_fit_multiparam <- fit_tmle3(\n  tmle_task, targeted_likelihood, all_params,\n  targeted_likelihood$updater\n)\n\nprint(tmle_fit_multiparam)\nA tmle3_Fit that took 1 step(s)\n   type                                       param   init_est tmle_est\n1:  TSM                            E[Y_{A=Control}] -0.5953314 -0.61981\n2:  TSM                        E[Y_{A=Handwashing}] -0.6179897 -0.66114\n3:  TSM                          E[Y_{A=Nutrition}] -0.6119870 -0.60338\n4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.6015639 -0.60250\n5:  TSM                         E[Y_{A=Sanitation}] -0.5866311 -0.58147\n6:  TSM                                E[Y_{A=WSH}] -0.5213051 -0.45027\n7:  TSM                              E[Y_{A=Water}] -0.5653576 -0.53554\n8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.0062324  0.01731\n         se     lower    upper psi_transformed lower_transformed\n1: 0.030069 -0.678746 -0.56088        -0.61981         -0.678746\n2: 0.041821 -0.743111 -0.57917        -0.66114         -0.743111\n3: 0.041553 -0.684825 -0.52194        -0.60338         -0.684825\n4: 0.040925 -0.682712 -0.52229        -0.60250         -0.682712\n5: 0.042313 -0.664402 -0.49854        -0.58147         -0.664402\n6: 0.045216 -0.538891 -0.36165        -0.45027         -0.538891\n7: 0.039290 -0.612551 -0.45854        -0.53554         -0.612551\n8: 0.050596 -0.081857  0.11648         0.01731         -0.081857\n   upper_transformed\n1:          -0.56088\n2:          -0.57917\n3:          -0.52194\n4:          -0.52229\n5:          -0.49854\n6:          -0.36165\n7:          -0.45854\n8:           0.11648"},{"path":"tmle3.html","id":"exercises-1","chapter":"6 The TMLE Framework","heading":"6.8 Exercises","text":"","code":""},{"path":"tmle3.html","id":"tmle3-ex1","chapter":"6 The TMLE Framework","heading":"6.8.1 Estimation of the ATE with tmle3","text":"Follow steps estimate average treatment effect using data \nCollaborative Perinatal Project (CPP), available sl3 package. \nsimplify example, define binary intervention variable, parity01 –\nindicator one children current child \nbinary outcome, haz01 – indicator average height \nage.Define variable roles \\((W,,Y)\\) creating list nodes.\nInclude following baseline covariates \\(W\\): apgar1, apgar5,\ngagebrth, mage, meducyrs, sexn. \\(\\) \\(Y\\) specified\n. missingness data (specifically, missingness \ncolumns specified node list) need taking care .\nprocess_missing function can used accomplish , like \nwashb_data example .Define tmle3_Spec object ATE, tmle_ATE().Using base learning libraries defined , specify sl3 base\nlearners estimation \\(\\overline{Q}_0 = \\mathbb{E}_0(Y \\mid ,Y)\\) \n\\(g_0 = \\mathbb{P}(= 1 \\mid W)\\).Define metalearner like .Define one super learner estimating \\(\\overline{Q}_0\\) another \nestimating \\(g_0\\). Use metalearner super learners.Create list two super learners defined step call\nobject learner_list. list names (defining super\nlearner estimation \\(g_0\\)) Y (defining super learner \nestimation \\(\\overline{Q}_0\\)).Fit TMLE tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimation \n\\(g_0\\) \\(\\overline{Q}_0\\), defined Step 6. Note: Like ,\nneed explicitly make copy data (work around\ndata.table optimizations), e.g., (cpp2 <- data.table::copy(cpp)), \nuse cpp2 data going forward.","code":"\n# load the data set\ndata(cpp)\ncpp <- cpp %>%\n  as_tibble() %>%\n  dplyr::filter(!is.na(haz)) %>%\n  mutate(\n    parity01 = as.numeric(parity > 0),\n    haz01 = as.numeric(haz > 0)\n  )\nmetalearner <- make_learner(\n  Lrnr_solnp,\n  loss_function = loss_loglik_binomial,\n  learner_function = metalearner_logistic_binomial\n)"},{"path":"tmle3.html","id":"tmle3-ex2","chapter":"6 The TMLE Framework","heading":"6.8.2 Estimation of Strata-Specific ATEs with tmle3","text":"exercise, work random sample 5,000 patients \nparticipated International Stroke Trial (IST). data described \nChapter 3.2 tlverse handbook. included data \nsummarized description relevant exercise.outcome, \\(Y\\), indicates recurrent ischemic stroke within 14 days \nrandomization (DRSISC); treatment interest, \\(\\), randomized\naspirin vs. aspirin treatment allocation (RXASP ist); \nadjustment set, \\(W\\), consists simply variables measured baseline. \ndata, outcome occasionally missing, need create \nvariable indicating missingness (\\(\\Delta\\)) analyses \ntlverse, since missingness automatically detected NA present\noutcome. Covariates missing values (RATRIAL, RASP3 RHEP24)\nalready imputed. Additional covariates created\n(MISSING_RATRIAL_RASP3 MISSING_RHEP24), indicate whether \ncovariate imputed. missingness identical RATRIAL \nRASP3, one covariate indicating imputation two\ncovariates created.Estimate average effect randomized asprin treatment (RXASP = 1) \nrecurrent ischemic stroke. Even though missingness mechanism \\(Y\\),\n\\(\\Delta\\), need specified node list, still need\naccounted TMLE. words, estimation problem,\n\\(\\Delta\\) relevant factor likelihood. Thus, defining \nlist sl3 learners likelihood factor, sure include list\nlearners estimation \\(\\Delta\\), say sl_Delta, specify \nlearner list, like \nlearner_list <- list(= sl_A, delta_Y = sl_Delta, Y = sl_Y).Recall RCT conducted internationally. Suposse concern\ndose asprin may varied across geographical regions, \naverage across geographical regions may warranted. Calculate \nstrata specific ATEs according geographical region (REGION).","code":"\nist_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/deming2019-workshop/\",\n    \"master/data/ist_sample.csv\"\n  )\n)"},{"path":"tmle3.html","id":"summary","chapter":"6 The TMLE Framework","heading":"6.9 Summary","text":"tmle3 general purpose framework generating TML estimates. easiest\nway use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. However, digging \nhood allows users specify wide range TMLEs. next sections,\n’ll see framework can used estimate advanced parameters \noptimal treatments stochastic shift interventions.","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis","chapter":"7 Causal Mediation Analysis","heading":"7 Causal Mediation Analysis","text":"Nima HejaziBased tmle3mediate R\npackage Nima Hejazi, James\nDuncan, David McCoy.Updated: 2021-04-19","code":""},{"path":"causal-mediation-analysis.html","id":"introduction-to-causal-mediation-analysis","chapter":"7 Causal Mediation Analysis","heading":"7.1 Introduction to Causal Mediation Analysis","text":"treatment often affects outcome indirectly, particular pathway,\neffect intermediate variables (mediators). Causal mediation analysis\nconcerns construction evaluation indirect effects \ncomplementary direct effects. Generally, indirect effect (IE) \ntreatment outcome portion total effect found work\nmediator variables, direct effect often encompasses \ncomponents total effect, including effect treatment\noutcome effect paths explicitly involving \nmediators). Identifying quantifying mechanisms underlying causal effects\nincreasingly desirable endeavor public health, medicine, social\nsciences, mechanistic knowledge improves understanding \ntreatments may effective.study mediation analysis may traced back quite far, field\ncame modern form identification careful study \nnatural direct indirect effects [Robins Greenland (1992);\npearl2001direct]. natural direct effect (NDE) natural indirect\neffect (NIE) based decomposition average treatment effect (ATE)\npresence mediators (VanderWeele 2015); requisite\ntheory construction efficient estimators quantities \nreceiving attention relatively recently (Tchetgen Tchetgen Shpitser 2012).","code":""},{"path":"causal-mediation-analysis.html","id":"data-structure-and-notation","chapter":"7 Causal Mediation Analysis","heading":"7.2 Data Structure and Notation","text":"Consider \\(n\\) observed units \\(O_1, \\ldots, O_n\\), observed data random\nvariable takes form \\(O = (W, , Z, Y)\\), vector observed covariates\n\\(W\\), binary continuous treatment \\(\\), possibly multivariate mediators \\(Z\\),\nbinary continuous outcome \\(Y\\). avoid undue assumptions, assume\n\\(O \\sim \\mathcal{P} \\\\M\\) \\(\\M\\) nonparametric\nstatistical model defined continuous densities \\(O\\) respect \narbitrary dominating measure.formalize definition counterfactual variables using following\nnon-parametric structural equation model (NPSEM):\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Z &= f_Z(W, , U_M) \\\\\n  Y &= f_Y(W, , Z, U_Y).\n  \\tag{7.1}\n\\end{align}\\]\nset equations\nrepresents mechanistic model generating observed data \\(O\\); furthermore,\nNPSEM encodes several fundamental assumptions. Firstly, implicit\ntemporal ordering: \\(W\\) occurs first, depending exogenous factors \\(U_W\\);\n\\(\\) happens next, based \\(W\\) exogenous factors \\(U_A\\); come \nmediators \\(Z\\), depend \\(\\), \\(W\\), another set exogenous factors\n\\(U_Z\\); finally appears outcome \\(Y\\). assume neither access set\nexogenous factors \\(\\{U_W, U_A, U_Z, U_Y\\}\\) knowledge forms \ndeterministic generating functions \\(\\{f_W, f_A, f_Z, f_Y\\}\\). NPSEM\ncorresponds following DAG:likelihood data \\(O\\) admits factorization, wherein, \\(p_0^O\\),\ndensity \\(O\\) respect product measure, density evaluated\nparticular observation \\(o\\) may written\n\\[\\begin{equation}\n  p_0^O(x) = q^O_{0,Y}(y \\mid Z = z = , W = w)\n    q^O_{0,Z}(z \\mid = , W = w)\n    q^O_{0,}(\\mid W = w)\n    q^O_{0,W}(w),\n  \\tag{7.2}\n\\end{equation}\\]\n\\(q_{0, Y}\\) conditional density \\(Y\\) given \\((Z, , W)\\), \\(q_{0, Z}\\)\nconditional density \\(Z\\) given \\((, W)\\), \\(q_{0, }\\) conditional\ndensity \\(\\) given \\(W\\), \\(q_{0, W}\\) density \\(W\\). , \nease notation, let \\(\\bar{Q}_Y(Z, , W) = \\E[Y \\mid Z, , W]\\),\n\\(\\bar{Q}_Z(, W) = \\P[Z \\mid , W]\\) (later re-parametrized \\(e(\\mid Z, W) = \\P(\\mid Z, W)\\)), \\(g(\\mid W) = \\P(\\mid W)\\), \\(q_W\\) marginal\ndistribution \\(W\\).Finally, note explicitly excluded potential confounders \nmediator-outcome relationship affected exposure (.e., variables affected \n\\(\\) affecting \\(Z\\) \\(Y\\)). Mediation analysis presence \nvariables exceptionally challenging (Avin, Shpitser, Pearl 2005); thus, \nefforts develop definitions causal direct indirect effects explicitly\ndisallowed form confounding. discuss matter\n, interested reader may consult recent advances vast literature\ncausal mediation analysis, among Díaz et al. (2020) \nHejazi et al. (2021).","code":"\nlibrary(dagitty)\nlibrary(ggdag)\n\n# make DAG by specifying dependence structure\ndag <- dagitty(\n  \"dag {\n    W -> A\n    W -> Z\n    W -> Y\n    A -> Z\n    A -> Y\n    Z -> Y\n    W -> A -> Y\n    W -> A -> Z -> Y\n  }\"\n)\nexposures(dag) <- c(\"A\")\noutcomes(dag) <- c(\"Y\")\ntidy_dag <- tidy_dagitty(dag)\n\n# visualize DAG\nggdag(tidy_dag) +\n  theme_dag()"},{"path":"causal-mediation-analysis.html","id":"decomposing-the-average-treatment-effect","chapter":"7 Causal Mediation Analysis","heading":"7.3 Decomposing the Average Treatment Effect","text":"natural direct indirect effects arise decomposition ATE:\n\\[\\begin{equation*}\n  \\E[Y(1) - Y(0)] =\n    \\underbrace{\\E[Y(1, Z(0)) - Y(0, Z(0))]}_{NDE} +\n    \\underbrace{\\E[Y(1, Z(1)) - Y(1, Z(0))]}_{NIE}.\n\\end{equation*}\\]\nparticular, natural indirect effect (NIE) measures effect \ntreatment \\(\\\\{0, 1\\}\\) outcome \\(Y\\) mediators \\(Z\\), \nnatural direct effect (NDE) measures effect treatment \noutcome paths. Identification natural direct \nindirect effects requires following non-testable causal assumptions:Exchangeability: \\(Y(, z) \\indep (, Z) \\mid W\\), implies\n\\(\\E\\{Y(, z) \\mid =, W=w, Z=z\\} = \\E\\{Y(, z) \\mid W=w\\}\\). \nspecial case randomization assumption, extended observational\nstudies mediators.Treatment positivity: \\(\\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\xi < g(\\mid w) < 1 - \\xi\\), \\(\\xi > 0\\). mirrors \nassumption required static intervention, discussed previously.Mediator positivity: $z , $\\(\\\\mathcal{}\\), \n\\(w \\\\mathcal{W}\\), \\(\\epsilon < g(\\mid w)\\), \\(\\epsilon > 0\\). \nrequires conditional density mediators bounded away \nzero \\((z, , w)\\) joint support \\(\\mathcal{Z} \\times \\mathcal{} \\times \\mathcal{W}\\).Cross-world counterfactual independence: \\(\\neq '\\), \ncontained \\(\\mathcal{}\\) \\(z \\\\mathcal{Z}\\), \\(Y(', z)\\) independent\n\\(Z()\\), given \\(W\\). , counterfactual outcome treatment\ncontrast \\(' \\\\mathcal{}\\) counterfactual mediator \\(Z() \\mathcal{Z}\\) (different contrast \\(\\\\mathcal{}\\)) \nindependent. Note counterfactual outcome mediator defined\ndiffering contrasts, hence “cross-world” designation.note many attempts made weaken last assumption, \ncross-world counterfactual independence, including work \nPetersen, Sinisi, Laan (2006) Imai, Keele, Yamamoto (2010); however, importantly,\nRobins Richardson (2010) established assumption satisfied \nrandomized experiments. Thus, natural direct indirect effects \nidentifiable randomized experiments, calling question utility.\nDespite significant limitation, turn considering estimation \nstatistical functionals corresponding effects observational\nstudies.","code":""},{"path":"causal-mediation-analysis.html","id":"the-natural-direct-effect","chapter":"7 Causal Mediation Analysis","heading":"7.4 The Natural Direct Effect","text":"NDE defined \n\\[\\begin{equation*}\n  \\Psi_{NDE} = \\E[Y(1, Z(0)) - Y(0, Z(0))]\n  \\overset{\\text{rand.}}{=} \\sum_w \\sum_z\n  [\\underbrace{\\E(Y \\mid = 1, z, w)}_{\\bar{Q}_Y(= 1, z, w)} -\n  \\underbrace{\\E(Y \\mid = 0, z, w)}_{\\bar{Q}_Y(= 0, z, w)}] \\times\n  \\underbrace{p(z \\mid = 0, w)}_{Q_Z(0, w))} \\underbrace{p(w)}_{q_W},\n\\end{equation*}\\]\nlikelihood factors \\(p(z \\mid = 0, w)\\) \\(p(w)\\) (among \nconditional densities) arise factorization joint likelihood:\n\\[\\begin{equation*}\n  p(w, , z, y) = \\underbrace{p(y \\mid w, , z)}_{Q_Y(, W, Z)}\n  \\underbrace{p(z \\mid w, )}_{Q_Z(Z \\mid , W)}\n  \\underbrace{p(\\mid w)}_{g(\\mid W)}\n  \\underbrace{p(w)}_{Q_W}.\n\\end{equation*}\\]process estimating NDE begins constructing \\(\\bar{Q}_{Y, n}\\), \nestimate outcome mechanism \\(\\bar{Q}_Y(Z, , W) = \\E \\{Y \\mid Z, , W\\}\\) (.e., conditional mean \\(Y\\), given \\(Z\\), \\(\\), \\(W\\)). \nestimate conditional expectation hand, predictions \ncounterfactual quantities \\(\\bar{Q}_Y(Z, 1, W)\\) (setting \\(= 1\\)) , likewise,\n\\(\\bar{Q}_Y(Z, 0, W)\\) (setting \\(= 0\\)) can readily obtained. denote \ndifference counterfactual quantities \\(\\bar{Q}_{\\text{diff}}\\), .e.,\n\\(\\bar{Q}_{\\text{diff}} = \\bar{Q}_Y(Z, 1, W) - \\bar{Q}_Y(Z, 0, W)\\).\n\\(\\bar{Q}_{\\text{diff}}\\) represents difference conditional mean \n\\(Y\\) attributable changes \\(\\) keeping \\(Z\\) \\(W\\) natural\n(, observed) values.estimation procedure treats \\(\\bar{Q}_{\\text{diff}}\\) nuisance\nparameter, regressing estimate \\(\\bar{Q}_{\\text{diff}, n}\\) \\(W\\), among\ncontrol observations (.e., \\(= 0\\) observed); goal\nstep remove part marginal impact \\(Z\\) \n\\(\\bar{Q}_{\\text{diff}}\\), since \\(W\\) parent \\(Z\\). Regressing \ndifference \\(W\\) among controls recovers expected\n\\(\\bar{Q}_{\\text{diff}}\\), individuals set control condition\n\\(= 0\\). residual additive effect \\(Z\\) \\(\\bar{Q}_{\\text{diff}}\\) \nremoved TML estimation step using auxiliary (“clever”)\ncovariate, accounts mediators \\(Z\\). auxiliary covariate takes\nform\n\\[\\begin{equation*}\n  C_Y(Q_Z, g)(O) = \\Bigg\\{\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\n  \\frac{Q_Z(Z \\mid 0, W)}{Q_Z(Z \\mid 1, W)} -\n  \\frac{\\mathbb{}(= 0)}{g(0 \\mid W)} \\Bigg\\}.\n\\end{equation*}\\]\nBreaking , \\(\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\\) inverse\npropensity score weight \\(= 1\\) , likewise, \\(\\frac{\\mathbb{}(= 0)} {g(0 \\mid W)}\\) inverse propensity score weight \\(= 0\\). middle\nterm ratio conditional densities mediator control\n(\\(= 0\\)) treatment (\\(= 1\\)) conditions.subtle appearance ratio conditional densities concerning –\nunfortunately, tools estimate quantities sparse statistics\nliterature, problem still complicated (computationally\ntaxing) \\(Z\\) high-dimensional. ratio conditional\ndensities required, convenient re-parametrization may achieved, ,\n\\[\\begin{equation*}\n  \\frac{p(= 0 \\mid Z, W) g(0 \\mid W)}{p(= 1 \\mid Z, W) g(1 \\mid W)}.\n\\end{equation*}\\]\nGoing forward, denote re-parameterized conditional probability\n\\(e(\\mid Z, W) := p(\\mid Z, W)\\). Similar re-parameterizations used\nZheng van der Laan (2012) Tchetgen Tchetgen (2013). particularly useful\nsince reformulation reduces problem one concerning \nestimation conditional means, opening door use wide range \nmachine learning algorithms (e.g., \nsl3).Underneath hood, counterfactual outcome difference\n\\(\\bar{Q}_{\\text{diff}}\\) \\(e(\\mid Z, W)\\), conditional probability \\(\\)\ngiven \\(Z\\) \\(W\\), used constructing auxiliary covariate TML\nestimation. nuisance parameters play important role \nbias-correcting update step TMLE procedure.","code":""},{"path":"causal-mediation-analysis.html","id":"the-natural-indirect-effect","chapter":"7 Causal Mediation Analysis","heading":"7.5 The Natural Indirect Effect","text":"Derivation estimation NIE analogous NDE. NIE\neffect \\(\\) \\(Y\\) mediator(s) \\(Z\\). quantity\n– known natural indirect effect \\(\\E(Y(Z(1), 1) - \\E(Y(Z(0), 1)\\) –\ncorresponds difference conditional expectation \\(Y\\) given \\(= 1\\) \\(Z(1)\\) (values mediator take \\(= 1\\)) \nconditional expectation \\(Y\\) given \\(= 1\\) \\(Z(0)\\) (values mediator\ntake \\(= 0\\)).NDE, re-parameterization trick can used estimate \\(\\E(\\mid Z, W)\\), avoiding estimation possibly multivariate conditional density.\nHowever, case, mediated mean outcome difference, denoted\n\\(\\Psi_Z(Q)\\), instead estimated follows\n\\[\\begin{equation*}\n  \\Psi_{NIE}(Q) = \\E_{QZ}(\\Psi_{NIE, Z}(Q)(1, W) - \\Psi_{NIE, Z}(Q)(0, W))\n\\end{equation*}\\], \\(\\bar{Q}_Y(Z, 1, W)\\) (predicted values \\(Y\\) given \\(Z\\) \\(W\\) \n\\(= 1\\)) regressed \\(W\\), among treated units (\\(= 1\\) \nobserved) obtain conditional mean \\(\\Psi_{NIE, Z}(Q)(1, W)\\). Performing\nprocedure, now regressing \\(\\bar{Q}_Y(Z, 1, W)\\) \\(W\\) among \ncontrol units (\\(= 0\\) observed) yields \\(\\Psi_{NIE,Z}(Q)(0, W)\\). \ndifference two estimates NIE can thought \nadditive marginal effect treatment conditional expectation \\(Y\\)\ngiven \\(W\\), \\(= 1\\), \\(Z\\) effects \\(Z\\). , case NIE,\nestimate \\(\\psi_n\\) slightly different, quantity \\(e(\\mid Z, W)\\) comes play auxiliary covariate.","code":""},{"path":"causal-mediation-analysis.html","id":"the-population-intervention-indirect-effects","chapter":"7 Causal Mediation Analysis","heading":"7.6 The Population Intervention (In)Direct Effects","text":"times, natural direct indirect effects may prove limiting, \neffect definitions based static interventions (.e., setting\n\\(= 0\\) \\(= 1\\)), may unrealistic real-world interventions. \ncases, one may turn instead population intervention direct effect\n(PIDE) population intervention indirect effect (PIIE), based\ndecomposing effect population intervention effect (PIE) \nflexible stochastic interventions (???).particular type stochastic intervention well-suited working binary\ntreatments incremental propensity score intervention (IPSI), first\nproposed Kennedy et al. (2017). interventions \ndeterministically set treatment level observed unit fixed\nquantity (.e., setting \\(= 1\\)), instead alter odds receiving \ntreatment fixed amount (\\(0 \\leq \\delta \\leq \\infty\\)) individual.\nparticular, intervention takes form\n\\[\\begin{equation*}\n  g_{\\delta}(1 \\mid w) = \\frac{\\delta g(1 \\mid w)}{\\delta g(1 \\mid w) + 1\n  - g(1\\mid w)},\n\\end{equation*}\\]\nscalar \\(0 < \\delta < \\infty\\) specifies change odds \nreceiving treatment. described (???), stochastic\nintervention special case exponential tilting, framework unifies\npost-intervention treatment values draws altered distribution.Unlike natural direct indirect effects, conditions required \nidentifiability population intervention direct indirect effects \nlax. importantly, differences involve (1) treatment positivity\nassumption requires counterfactual treatment \nobserved support treatment \\(\\mathcal{}\\), (2) requirement \nindependence cross-world counterfactuals.","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-population-intervention-effect","chapter":"7 Causal Mediation Analysis","heading":"7.7 Decomposing the Population Intervention Effect","text":"may decompose population intervention effect (PIE) terms \npopulation intervention direct effect (PIDE) population\nintervention indirect effect (PIIE):\n\\[\\begin{equation*}\n  \\mathbb{E}\\{Y(A_\\delta)\\} - \\mathbb{E}Y =\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z(A_\\delta))\n      - Y(A_\\delta, Z)\\}}^{\\text{PIIE}} +\n    \\overbrace{\\mathbb{E}\\{Y(A_\\delta, Z) - Y(, Z)\\}}^{\\text{PIDE}}.\n\\end{equation*}\\]decomposition PIE sum population intervention direct\nindirect effects interpretation analogous corresponding\nstandard decomposition average treatment effect. sequel, \ncompute components direct indirect effects using\nappropriate estimators followsFor \\(\\mathbb{E}\\{Y(, Z)\\}\\), sample mean \\(\\frac{1}{n}\\sum_{=1}^n Y_i\\) \nconsistent;\\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\), TML estimator effect \njoint intervention altering treatment mechanism mediation\nmechanism, based proposal (???); ,\\(\\mathbb{E}\\{Y(A_{\\delta}, Z_{A_{\\delta}})\\}\\), efficient estimator \neffect joint intervention altering treatment mediation\nmechanisms, proposed Kennedy et al. (2017) implemented \nnpcausal R package.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-effect-decomposition-term","chapter":"7 Causal Mediation Analysis","heading":"7.8 Estimating the Effect Decomposition Term","text":"described (???), statistical functional identifying \ndecomposition term appears PIDE PIIE\n\\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\), corresponds altering treatment\nmechanism keeping mediation mechanism fixed, \n\\[\\begin{equation*}\n  \\theta_0(\\delta) = \\int m_0(, z, w) g_{0,\\delta}(\\mid w) p_0(z, w)\n    d\\nu(, z, w),\n\\end{equation*}\\]\nTML estimator available. corresponding efficient influence\nfunction (EIF) respect nonparametric model \\(\\mathcal{M}\\) \n\\(D_{\\eta,\\delta}(o) = D^Y_{\\eta,\\delta}(o) + D^A_{\\eta,\\delta}(o) + D^{Z,W}_{\\eta,\\delta}(o) - \\theta(\\delta)\\).TML estimator may computed basd EIF estimating equation may\nincorporate cross-validation (???; ???) \ncircumvent possibly restrictive entropy conditions (e.g., Donsker class). \nresultant estimator \n\\[\\begin{equation*}\n  \\hat{\\theta}(\\delta) = \\frac{1}{n} \\sum_{= 1}^n D_{\\hat{\\eta}_{j()},\n  \\delta}(O_i) = \\frac{1}{n} \\sum_{= 1}^n \\left\\{ D^Y_{\\hat{\\eta}_{j()},\n  \\delta}(O_i) + D^A_{\\hat{\\eta}_{j()}, \\delta}(O_i) +\n  D^{Z,W}_{\\hat{\\eta}_{j()}, \\delta}(O_i) \\right\\},\n\\end{equation*}\\]\nimplemented tmle3mediate (one-step estimator also avaialble,\nmedshift R package). \ndemonstrate use tmle3mediate obtain \\(\\mathbb{E}\\{Y(A_{\\delta}, Z)\\}\\)\nvia TML estimator.","code":""},{"path":"causal-mediation-analysis.html","id":"evaluating-the-direct-and-indirect-effects","chapter":"7 Causal Mediation Analysis","heading":"7.9 Evaluating the Direct and Indirect Effects","text":"now turn estimating natural direct indirect effects, well \npopulation intervention direct effect, using WASH Benefits data,\nintroduced earlier chapters. Let’s first load data:’ll next define baseline covariates \\(W\\), treatment \\(\\), mediators \\(Z\\),\noutcome \\(Y\\) nodes NPSEM via “Node List” object:node_list encodes parents node – example, \\(Z\\) (\nmediators) parents \\(\\) (treatment) \\(W\\) (baseline confounders),\n\\(Y\\) (outcome) parents \\(Z\\), \\(\\), \\(W\\). ’ll also handle \nmissingness data invoking process_missing:’ll now construct ensemble learner using handful popular machine\nlearning algorithms:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mediate)\n\n# download data\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# make intervention node binary\nwashb_data[, tr := as.numeric(tr != \"Control\")]\nnode_list <- list(\n  W = c(\n    \"momage\", \"momedu\", \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\"\n  ),\n  A = \"tr\",\n  Z = c(\"sex\", \"month\", \"aged\"),\n  Y = \"whz\"\n)\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list\n# SL learners used for continuous data (the nuisance parameter M)\nenet_contin_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"gaussian\", nfolds = 5\n)\nlasso_contin_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"gaussian\", nfolds = 5\n)\nfglm_contin_learner <- Lrnr_glm_fast$new(family = gaussian())\nmean_learner <- Lrnr_mean$new()\ncontin_learner_lib <- Stack$new(\n  enet_contin_learner, lasso_contin_learner, fglm_contin_learner, mean_learner\n)\nsl_contin_learner <- Lrnr_sl$new(learners = contin_learner_lib)\n\n# SL learners used for binary data (nuisance parameters G and E in this case)\nenet_binary_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"binomial\", nfolds = 5\n)\nlasso_binary_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"binomial\", nfolds = 5\n)\nfglm_binary_learner <- Lrnr_glm_fast$new(family = binomial())\nbinary_learner_lib <- Stack$new(\n  enet_binary_learner, lasso_binary_learner, fglm_binary_learner, mean_learner\n)\nsl_binary_learner <- Lrnr_sl$new(learners = binary_learner_lib)\n\n# create list for treatment and outcome mechanism regressions\nlearner_list <- list(\n  Y = sl_contin_learner,\n  A = sl_binary_learner\n)"},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-indirect-effect","chapter":"7 Causal Mediation Analysis","heading":"7.10 Estimating the Natural Indirect Effect","text":"demonstrate calculation NIE , starting instantiating “Spec”\nobject encodes exactly learners use nuisance parameters\n\\(e(\\mid Z, W)\\) \\(\\Psi_Z\\). pass Spec object tmle3\nfunction, alongside data, node list (created ), learner list\nindicating machine learning algorithms use estimating nuisance\nparameters based \\(\\) \\(Y\\).Based output, conclude indirect effect treatment\nmediators (sex, month, aged) \n0.00297.","code":"tmle_spec_NIE <- tmle_NIE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NIE <- tmle3(\n  tmle_spec_NIE, washb_data, node_list, learner_list\n)\nwashb_NIE\nA tmle3_Fit that took 1 step(s)\n   type                  param  init_est  tmle_est       se     lower    upper\n1:  NIE NIE[Y_{A=1} - Y_{A=0}] 0.0030972 0.0029694 0.015535 -0.027479 0.033418\n   psi_transformed lower_transformed upper_transformed\n1:       0.0029694         -0.027479          0.033418"},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-direct-effect","chapter":"7 Causal Mediation Analysis","heading":"7.11 Estimating the Natural Direct Effect","text":"analogous procedure applies estimation NDE, replacing \nSpec object NIE tmle_spec_NDE define learners NDE\nnuisance parameters:, can draw conclusion direct effect treatment\n(paths involving mediators (sex, month, aged)) \n0.02893. Note , together, estimates \nnatural direct indirect effects approximately recover average\ntreatment effect, , based estimates NDE NIE, \nATE roughly\n0.0319.","code":"tmle_spec_NDE <- tmle_NDE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NDE <- tmle3(\n  tmle_spec_NDE, washb_data, node_list, learner_list\n)\nwashb_NDE\nA tmle3_Fit that took 1 step(s)\n   type                  param init_est tmle_est      se    lower  upper\n1:  NDE NDE[Y_{A=1} - Y_{A=0}] 0.028931 0.028931 0.31693 -0.59223 0.6501\n   psi_transformed lower_transformed upper_transformed\n1:        0.028931          -0.59223            0.6501"},{"path":"causal-mediation-analysis.html","id":"estimating-the-population-intervention-direct-effect","chapter":"7 Causal Mediation Analysis","heading":"7.12 Estimating the Population Intervention Direct Effect","text":"previously noted, assumptions underlying natural direct indirect\neffects may challenging justify; moreover, effect definitions\ndepend application static intervention treatment,\nsharply limiting flexibility. considering binary treatments,\nincremental propensity score shifts provide alternative class flexible,\nstochastic interventions. ’ll now consider estimating PIDE IPSI\nmodulates odds receiving treatment \\(\\delta = 3\\). \nintervention may interpreted (hypothetically) effect design \nencourages study participants opt receiving treatment, thus\nincreasing relative odds receiving said treatment. exemplify \napproach, postulate motivational intervention triples odds\n(.e., \\(\\delta = 3\\)) receiving treatment individual:Recall , based decomposition outlined previously, PIDE may \ndenoted \\(\\beta_{\\text{PIDE}}(\\delta) = \\theta_0(\\delta) - \\mathbb{E}Y\\). Thus, \nestimator PIDE, \\(\\hat{\\beta}_{\\text{PIDE}}(\\delta)\\) may expressed \ncomposition estimators constituent parameters:\n\\[\\begin{equation*}\n  \\hat{\\beta}_{\\text{PIDE}}({\\delta}) = \\hat{\\theta}(\\delta) -\n  \\frac{1}{n} \\sum_{= 1}^n Y_i.\n\\end{equation*}\\]Based , may construct estimator PIDE using already\nestimated decomposition term empirical (marginal) mean outcome.\nThus, estimate PIDE approximately\n0.00223.\nNote straightforward application delta method \nequivalently performed using functionality exposed tmle3\npackage.","code":"# set the IPSI multiplicative shift\ndelta_ipsi <- 3\n\n# instantiate tmle3 spec for stochastic mediation\ntmle_spec_pie_decomp <- tmle_medshift(\n  delta = delta_ipsi,\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  phi_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE)\n)\n\n# compute the TML estimate\nwashb_pie_decomp <- tmle3(\n  tmle_spec_pie_decomp, washb_data, node_list, learner_list\n)\nwashb_pie_decomp\nA tmle3_Fit that took 510 step(s)\n   type         param init_est tmle_est       se   lower    upper\n1: PIDE E[Y_{A=NULL}] -0.58163 -0.58385 0.016302 -0.6158 -0.55189\n   psi_transformed lower_transformed upper_transformed\n1:        -0.58385           -0.6158          -0.55189"},{"path":"r6.html","id":"r6","chapter":"8 A Primer on the R6 Class System","heading":"8 A Primer on the R6 Class System","text":"central goal Targeted Learning statistical paradigm estimate\nscientifically relevant parameters realistic (usually nonparametric) models.tlverse designed using basic OOP principles R6 OOP framework.\n’ve tried make easy use tlverse packages without worrying\nmuch OOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"r6.html","id":"classes-fields-and-methods","chapter":"8 A Primer on the R6 Class System","heading":"8.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R’s various OOP systems,\nsee http://adv-r..co.nz/OO-essentials.html, Hadley Wickham’s Advanced\nR (Wickham 2014).","code":""},{"path":"r6.html","id":"object-oriented-programming-python-and-r","chapter":"8 A Primer on the R6 Class System","heading":"8.2 Object Oriented Programming: Python and R","text":"OO concepts (classes inherentence) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Anonymous. 2015. “Let’s Think Cognitive Bias.” Nature 526 (7572). Springer Nature. https://doi.org/10.1038/526163a.Avin, Chen, Ilya Shpitser, Judea Pearl. 2005. “Identifiability Path-Specific Effects.” IJCAI International Joint Conference Artificial Intelligence, 357–63.Baker, Monya. 2016. “Reproducibility Crisis? Nature Survey Lifts Lid Researchers View Crisis Rocking Science Think Help.” Nature 533 (7604). Nature Publishing Group: 452–55.Bengtsson, Henrik. 2020. “Unifying Framework Parallel Distributed Processing R Using Futures.” https://arxiv.org/abs/2008.00553.Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1). Springer: 5–32.Buckheit, Jonathan B, David L Donoho. 1995. “Wavelab Reproducible Research.” Wavelets Statistics, 55–81. Springer.Coyle, Jeremy R, Nima S Hejazi. 2018. “Origami: Generalized Framework Cross-Validation R.” Journal Open Source Software 3 (21). Open Journal. https://doi.org/10.21105/joss.00512.Coyle, Jeremy R, Nima S Hejazi, Ivana Malenica, Rachael V Phillips, Benjamin F Arnold, Andrew Mertens, Jade Benjamin-Chung, et al. 2021. “Targeting Learning: Robust Statistics Reproducible Research.” arXiv. https://arxiv.org/abs/2006.07333.Díaz, Iván, Nima S Hejazi, Kara E Rudolph, Mark J van der Laan. 2020. “Non-Parametric Efficient Causal Mediation Intermediate Confounders.” Biometrika. Oxford University Press. https://doi.org/10.1093/biomet/asaa085.Dudoit, Sandrine, Mark J van der Laan. 2005. “Asymptotics Cross-Validated Risk Estimation Estimator Selection Performance Assessment.” Statistical Methodology 2 (2). Elsevier: 131–54.Editorial, Nature. 2015. “Scientists Fool — Can Stop.” Nature 526 (7572). Springer Nature.Hejazi, Nima S, Kara E Rudolph, Mark J van der Laan, Iván Díaz. 2021. “Nonparametric Causal Mediation Analysis Stochastic Interventional ()direct Effects.” https://arxiv.org/abs/2009.06203.Imai, Kosuke, Luke Keele, Teppei Yamamoto. 2010. “Identification, Inference Sensitivity Analysis Causal Mediation Effects.” Statistical Science. JSTOR, 51–71.Kennedy, Edward H, Zongming Ma, Matthew D McHugh, Dylan S Small. 2017. “Nonparametric Methods Doubly Robust Estimation Continuous Treatment Effects.” Journal Royal Statistical Society: Series B (Statistical Methodology) 79 (4). Wiley Online Library: 1229–45.Munafò, Marcus R, Brian Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, John PA Ioannidis. 2017. “Manifesto Reproducible Science.” Nature Human Behaviour 1 (1). Nature Publishing Group: 0021.Nosek, Brian , Charles R Ebersole, Alexander C DeHaven, David T Mellor. 2018. “Preregistration Revolution.” Proceedings National Academy Sciences 115 (11). National Acad Sciences: 2600–2606.Peng, Roger. 2015. “Reproducibility Crisis Science: Statistical Counterattack.” Significance 12 (3). Wiley Online Library: 30–32.Petersen, Maya L, Sandra E Sinisi, Mark J van der Laan. 2006. “Estimation Direct Causal Effects.” Epidemiology. JSTOR, 276–84.Pullenayegum, Eleanor M, Robert W Platt, Melanie Barwick, Brian M Feldman, Martin Offringa, Lehana Thabane. 2016. “Knowledge Translation Biostatistics: Survey Current Practices, Preferences, Barriers Dissemination Uptake New Statistical Methods.” Statistics Medicine 35 (6). Wiley Online Library: 805–18.Robins, James M, Sander Greenland. 1992. “Identifiability Exchangeability Direct Indirect Effects.” Epidemiology. JSTOR, 143–55.Robins, James M, Thomas S Richardson. 2010. “Alternative Graphical Causal Models Identification Direct Effects.” Causality Psychopathology: Finding Determinants Disorders Cures. Oxford: Oxford University Press, 103–58.Sandercock, P, R Collins, C Counsell, B Farrell, R Peto, J Slattery, C Warlow. 1997. “International Stroke Trial (Ist): Randomized Trial Aspirin, Subcutaneous Heparin, , Neither Among 19,435 Patients Acute Ischemic Stroke.” Lancet 349 (9065): 1569–81.Sandercock, Peter AG, Maciej Niewada, Anna Członkowska. 2011. “International Stroke Trial Database.” Trials 12 (1). BioMed Central: 101.Stark, Philip B, Andrea Saltelli. 2018. “Cargo-Cult Statistics Scientific Crisis.” Significance 15 (4). Wiley Online Library: 40–43.Stromberg, Arnold, others. 2004. “Write Statistical Software? Case Robust Statistical Methods.” Journal Statistical Software 10 (5): 1–8.Szucs, Denes, John Ioannidis. 2017. “Null Hypothesis Significance Testing Unsuitable Research: Reassessment.” Frontiers Human Neuroscience 11. Frontiers: 390.Tchetgen Tchetgen, Eric J. 2013. “Inverse Odds Ratio-Weighted Estimation Causal Mediation Analysis.” Statistics Medicine 32 (26). Wiley Online Library: 4567–80.Tchetgen Tchetgen, Eric J, Ilya Shpitser. 2012. “Semiparametric Theory Causal Mediation Analysis: Efficiency Bounds, Multiple Robustness, Sensitivity Analysis.” Annals Statistics 40 (3): 1816–45. https://doi.org/10.1214/12-AOS990.Textor, Johannes, Juliane Hardt, Sven Knüppel. 2011. “DAGitty: Graphical Tool Analyzing Causal Diagrams.” Epidemiology 22 (5). LWW: 745.Tofail, Fahmida, Lia CH Fernald, Kishor K Das, Mahbubur Rahman, Tahmeed Ahmed, Kaniz K Jannat, Leanne Unicomb, et al. 2018. “Effect Water Quality, Sanitation, Hand Washing, Nutritional Interventions Child Development Rural Bangladesh (Wash Benefits Bangladesh): Cluster-Randomised Controlled Trial.” Lancet Child & Adolescent Health 2 (4). Elsevier: 255–68.van der Laan, Mark J, Sandrine Dudoit, Sunduz Keles. 2004. “Asymptotic Optimality Likelihood-Based Cross-Validation.” Statistical Applications Genetics Molecular Biology 3 (1): 1–23.van der Laan, Mark J, Eric C Polley, Alan E Hubbard. 2007. “Super Learner.” Statistical Applications Genetics Molecular Biology 6 (1).van der Laan, Mark J, Sherri Rose. 2011. Targeted Learning: Causal Inference Observational Experimental Data. Springer Science & Business Media.———. 2018. Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies. Springer Science & Business Media.van der Laan, Mark J, Richard JCM Starmans. 2014. “Entering Era Data Science: Targeted Learning Integration Statistics Computational Data Analysis.” Advances Statistics 2014. Hindawi.Van der Vaart, Aad W, Sandrine Dudoit, Mark J van der Laan. 2006. “Oracle Inequalities Multi-Fold Cross Validation.” Statistics & Decisions 24 (3). Oldenbourg Wissenschaftsverlag: 351–71.VanderWeele, Tyler. 2015. Explanation Causal Inference: Methods Mediation Interaction. Oxford University Press.Wickham, Hadley. 2014. Advanced R. Chapman; Hall/CRC.Zheng, Wenjing, Mark J van der Laan. 2012. “Targeted Maximum Likelihood Estimation Natural Direct Effects.” International Journal Biostatistics 8 (1). https://doi.org/10.2202/1557-4679.1361.","code":""}]
