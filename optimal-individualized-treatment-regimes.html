<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse</title>
  <meta name="description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tlverse.org/tlverse-handbook/" />
  
  <meta property="og:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  <meta name="github-repo" content="tlverse/tlverse-handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Optimal Individualized Treatment Regimes | The Hitchhiker’s Guide to the tlverse" />
  
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic handbook for applying the targeted learning methodology in practice using the tlverse software ecosystem." />
  

<meta name="author" content="Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard, Mark van der Laan" />


<meta name="date" content="2020-02-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/logos/favicons/favicon.png" type="image/x-icon" />
<link rel="prev" href="tmle3.html"/>
<link rel="next" href="stochastic-treatment-regimes.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
<link rel="stylesheet" href="css/center.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Hitchhiker's Guide to the tlverse</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i><b>0.1</b> Outline</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-is-not"><i class="fa fa-check"></i>What this book is not</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#jeremy-coyle"><i class="fa fa-check"></i>Jeremy Coyle</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nima-hejazi"><i class="fa fa-check"></i>Nima Hejazi</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ivana-malenica"><i class="fa fa-check"></i>Ivana Malenica</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#rachael-phillips"><i class="fa fa-check"></i>Rachael Phillips</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alan-hubbard"><i class="fa fa-check"></i>Alan Hubbard</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mark-van-der-laan"><i class="fa fa-check"></i>Mark van der Laan</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#learn"><i class="fa fa-check"></i><b>0.2</b> Learning resources</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#setup"><i class="fa fa-check"></i><b>0.3</b> Setup instructions</a><ul>
<li class="chapter" data-level="0.3.1" data-path="index.html"><a href="index.html#r-and-rstudio"><i class="fa fa-check"></i><b>0.3.1</b> R and RStudio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> The Roadmap for Targeted Learning</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#the-roadmap"><i class="fa fa-check"></i><b>1.3</b> The Roadmap</a><ul>
<li><a href="intro.html#data-a-random-variable-with-a-probability-distribution-o-sim-p_0">(1) Data: A random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span></a></li>
<li><a href="intro.html#the-statistical-model-mathcalm-such-that-p_0-in-mathcalm">(2) The statistical model <span class="math inline">\(\mathcal{M}\)</span> such that <span class="math inline">\(P_0 \in \mathcal{M}\)</span></a></li>
<li><a href="intro.html#the-statistical-target-parameter-psi-and-estimand-psip_0">(3) The statistical target parameter <span class="math inline">\(\Psi\)</span> and estimand <span class="math inline">\(\Psi(P_0)\)</span></a></li>
<li><a href="intro.html#the-estimator-hatpsi-and-estimate-hatpsip_n">(4) The estimator <span class="math inline">\(\hat{\Psi}\)</span> and estimate <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
<li><a href="intro.html#a-measure-of-uncertainty-for-the-estimate-hatpsip_n">(5) A measure of uncertainty for the estimate <span class="math inline">\(\hat{\Psi}(P_n)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#summary-of-the-roadmap"><i class="fa fa-check"></i><b>1.4</b> Summary of the Roadmap</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#causal"><i class="fa fa-check"></i><b>1.5</b> Causal Target Parameters</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#the-causal-model"><i class="fa fa-check"></i><b>1.5.1</b> The Causal Model</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#identifiability"><i class="fa fa-check"></i><b>1.5.2</b> Identifiability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tlverse.html"><a href="tlverse.html"><i class="fa fa-check"></i><b>2</b> Welcome to the <code>tlverse</code></a><ul>
<li class="chapter" data-level="2.1" data-path="tlverse.html"><a href="tlverse.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="tlverse.html"><a href="tlverse.html#what-is-the-tlverse"><i class="fa fa-check"></i><b>2.2</b> What is the <code>tlverse</code>?</a></li>
<li class="chapter" data-level="2.3" data-path="tlverse.html"><a href="tlverse.html#tlverse-components"><i class="fa fa-check"></i><b>2.3</b> <code>tlverse</code> components</a></li>
<li class="chapter" data-level="2.4" data-path="tlverse.html"><a href="tlverse.html#installtlverse"><i class="fa fa-check"></i><b>2.4</b> Installation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#wash"><i class="fa fa-check"></i><b>3.1</b> WASH Benefits Example Dataset</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#ist"><i class="fa fa-check"></i><b>3.2</b> International Stroke Trial Example Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#vet"><i class="fa fa-check"></i><b>3.3</b> Veterans’ Administration Lung Cancer Trial Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sl3.html"><a href="sl3.html"><i class="fa fa-check"></i><b>4</b> Super (Machine) Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="sl3.html"><a href="sl3.html#learning-objectives-2"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="sl3.html"><a href="sl3.html#motivation-1"><i class="fa fa-check"></i><b>4.2</b> Motivation</a></li>
<li class="chapter" data-level="4.3" data-path="sl3.html"><a href="sl3.html#introduction-1"><i class="fa fa-check"></i><b>4.3</b> Introduction</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sl3.html"><a href="sl3.html#background"><i class="fa fa-check"></i><b>4.3.1</b> Background</a></li>
<li class="chapter" data-level="4.3.2" data-path="sl3.html"><a href="sl3.html#super-learner-for-prediction"><i class="fa fa-check"></i><b>4.3.2</b> Super Learner for Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sl3.html"><a href="sl3.html#sl3-microwave-dinner-implementation"><i class="fa fa-check"></i><b>4.4</b> <code>sl3</code> “Microwave Dinner” Implementation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sl3.html"><a href="sl3.html#wash-benefits-study-example"><i class="fa fa-check"></i><b>4.4.1</b> WASH Benefits Study Example</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#load-the-necessary-libraries-and-data"><i class="fa fa-check"></i>0. Load the necessary libraries and data</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#define-the-machine-learning-task"><i class="fa fa-check"></i>1. Define the machine learning task</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#make-a-super-learner"><i class="fa fa-check"></i>2. Make a Super Learner</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#train-the-super-learner-on-the-machine-learning-task"><i class="fa fa-check"></i>3. Train the Super Learner on the machine learning task</a></li>
<li class="chapter" data-level="" data-path="sl3.html"><a href="sl3.html#obtain-predicted-values"><i class="fa fa-check"></i>4. Obtain predicted values</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sl3.html"><a href="sl3.html#cross-validated-super-learner"><i class="fa fa-check"></i><b>4.5</b> Cross-validated Super Learner</a></li>
<li class="chapter" data-level="4.6" data-path="sl3.html"><a href="sl3.html#variable-importance-measures-with-sl3"><i class="fa fa-check"></i><b>4.6</b> Variable Importance Measures with <code>sl3</code></a></li>
<li class="chapter" data-level="4.7" data-path="sl3.html"><a href="sl3.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a><ul>
<li class="chapter" data-level="4.7.1" data-path="sl3.html"><a href="sl3.html#sl3ex1"><i class="fa fa-check"></i><b>4.7.1</b> Predicting Myocardial Infarction with <code>sl3</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="sl3.html"><a href="sl3.html#sl3ex2"><i class="fa fa-check"></i><b>4.7.2</b> Predicting Recurrent Ischemic Stroke in an RCT with <code>sl3</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="sl3.html"><a href="sl3.html#concluding-remarks"><i class="fa fa-check"></i><b>4.8</b> Concluding Remarks</a></li>
<li class="chapter" data-level="4.9" data-path="sl3.html"><a href="sl3.html#appendix"><i class="fa fa-check"></i><b>4.9</b> Appendix</a><ul>
<li class="chapter" data-level="4.9.1" data-path="sl3.html"><a href="sl3.html#exercise-1-solution"><i class="fa fa-check"></i><b>4.9.1</b> Exercise 1 Solution</a></li>
<li class="chapter" data-level="4.9.2" data-path="sl3.html"><a href="sl3.html#exercise-2-solution"><i class="fa fa-check"></i><b>4.9.2</b> Exercise 2 Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tmle3.html"><a href="tmle3.html"><i class="fa fa-check"></i><b>5</b> The TMLE Framework</a><ul>
<li class="chapter" data-level="5.1" data-path="tmle3.html"><a href="tmle3.html#learning-objectives-3"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="tmle3.html"><a href="tmle3.html#introduction-2"><i class="fa fa-check"></i><b>5.2</b> Introduction</a><ul>
<li class="chapter" data-level="5.2.1" data-path="tmle3.html"><a href="tmle3.html#substitution-estimators"><i class="fa fa-check"></i><b>5.2.1</b> Substitution Estimators</a></li>
<li class="chapter" data-level="5.2.2" data-path="tmle3.html"><a href="tmle3.html#tmle"><i class="fa fa-check"></i><b>5.2.2</b> TMLE</a></li>
<li class="chapter" data-level="5.2.3" data-path="tmle3.html"><a href="tmle3.html#inference"><i class="fa fa-check"></i><b>5.2.3</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="tmle3.html"><a href="tmle3.html#easy-bake-example-tmle3-for-ate"><i class="fa fa-check"></i><b>5.3</b> Easy-Bake Example: <code>tmle3</code> for ATE</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tmle3.html"><a href="tmle3.html#load-the-data"><i class="fa fa-check"></i><b>5.3.1</b> Load the Data</a></li>
<li class="chapter" data-level="5.3.2" data-path="tmle3.html"><a href="tmle3.html#define-the-variable-roles"><i class="fa fa-check"></i><b>5.3.2</b> Define the variable roles</a></li>
<li class="chapter" data-level="5.3.3" data-path="tmle3.html"><a href="tmle3.html#handle-missingness"><i class="fa fa-check"></i><b>5.3.3</b> Handle Missingness</a></li>
<li class="chapter" data-level="5.3.4" data-path="tmle3.html"><a href="tmle3.html#create-a-spec-object"><i class="fa fa-check"></i><b>5.3.4</b> Create a “Spec” Object</a></li>
<li class="chapter" data-level="5.3.5" data-path="tmle3.html"><a href="tmle3.html#define-the-learners"><i class="fa fa-check"></i><b>5.3.5</b> Define the learners</a></li>
<li class="chapter" data-level="5.3.6" data-path="tmle3.html"><a href="tmle3.html#fit-the-tmle"><i class="fa fa-check"></i><b>5.3.6</b> Fit the TMLE</a></li>
<li class="chapter" data-level="5.3.7" data-path="tmle3.html"><a href="tmle3.html#evaluate-the-estimates"><i class="fa fa-check"></i><b>5.3.7</b> Evaluate the Estimates</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tmle3.html"><a href="tmle3.html#tmle3-components"><i class="fa fa-check"></i><b>5.4</b> <code>tmle3</code> Components</a><ul>
<li class="chapter" data-level="5.4.1" data-path="tmle3.html"><a href="tmle3.html#tmle3_task"><i class="fa fa-check"></i><b>5.4.1</b> <code>tmle3_task</code></a></li>
<li class="chapter" data-level="5.4.2" data-path="tmle3.html"><a href="tmle3.html#initial-likelihood"><i class="fa fa-check"></i><b>5.4.2</b> Initial Likelihood</a></li>
<li class="chapter" data-level="5.4.3" data-path="tmle3.html"><a href="tmle3.html#targeted-likelihood-updater"><i class="fa fa-check"></i><b>5.4.3</b> Targeted Likelihood (updater)</a></li>
<li class="chapter" data-level="5.4.4" data-path="tmle3.html"><a href="tmle3.html#parameter-mapping"><i class="fa fa-check"></i><b>5.4.4</b> Parameter Mapping</a></li>
<li class="chapter" data-level="5.4.5" data-path="tmle3.html"><a href="tmle3.html#putting-it-all-together"><i class="fa fa-check"></i><b>5.4.5</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="tmle3.html"><a href="tmle3.html#fitting-tmle3-with-multiple-parameters"><i class="fa fa-check"></i><b>5.5</b> Fitting <code>tmle3</code> with multiple parameters</a><ul>
<li class="chapter" data-level="5.5.1" data-path="tmle3.html"><a href="tmle3.html#delta-method"><i class="fa fa-check"></i><b>5.5.1</b> Delta Method</a></li>
<li class="chapter" data-level="5.5.2" data-path="tmle3.html"><a href="tmle3.html#fit"><i class="fa fa-check"></i><b>5.5.2</b> Fit</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="tmle3.html"><a href="tmle3.html#exercises-1"><i class="fa fa-check"></i><b>5.6</b> Exercises</a><ul>
<li class="chapter" data-level="5.6.1" data-path="sl3.html"><a href="sl3.html#sl3ex2"><i class="fa fa-check"></i><b>5.6.1</b> Estimation of the ATE with <code>tmle3</code></a></li>
<li class="chapter" data-level="5.6.2" data-path="sl3.html"><a href="sl3.html#sl3ex2"><i class="fa fa-check"></i><b>5.6.2</b> Estimation of Strata-Specific ATEs with <code>tmle3</code></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="tmle3.html"><a href="tmle3.html#summary"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html"><i class="fa fa-check"></i><b>6</b> Optimal Individualized Treatment Regimes</a><ul>
<li class="chapter" data-level="6.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#learning-objectives-4"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#introduction-to-optimal-individualized-interventions"><i class="fa fa-check"></i><b>6.2</b> Introduction to Optimal Individualized Interventions</a></li>
<li class="chapter" data-level="6.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#data-structure-and-notation"><i class="fa fa-check"></i><b>6.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="6.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#defining-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>6.4</b> Defining the Causal Effect of an Optimal Individualized Intervention</a><ul>
<li class="chapter" data-level="6.4.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#binary-treatment"><i class="fa fa-check"></i><b>6.4.1</b> Binary treatment</a></li>
<li class="chapter" data-level="6.4.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#categorical-treatment"><i class="fa fa-check"></i><b>6.4.2</b> Categorical treatment</a></li>
<li class="chapter" data-level="6.4.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#note-on-inference"><i class="fa fa-check"></i><b>6.4.3</b> Note on Inference</a></li>
<li class="chapter" data-level="6.4.4" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#why-cv-tmle"><i class="fa fa-check"></i><b>6.4.4</b> Why CV-TMLE?</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#interpreting-the-causal-effect-of-an-optimal-individualized-intervention"><i class="fa fa-check"></i><b>6.5</b> Interpreting the Causal Effect of an Optimal Individualized Intervention</a></li>
<li class="chapter" data-level="6.6" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-oit-with-binary-treatment"><i class="fa fa-check"></i><b>6.6</b> Evaluating the Causal Effect of an OIT with Binary Treatment</a><ul>
<li class="chapter" data-level="6.6.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data"><i class="fa fa-check"></i><b>6.6.1</b> Simulated Data</a></li>
<li class="chapter" data-level="6.6.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3"><i class="fa fa-check"></i><b>6.6.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="6.6.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects"><i class="fa fa-check"></i><b>6.6.3</b> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment"><i class="fa fa-check"></i><b>6.7</b> Evaluating the Causal Effect of an optimal ITR with Categorical Treatment</a><ul>
<li class="chapter" data-level="6.7.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-1"><i class="fa fa-check"></i><b>6.7.1</b> Simulated Data</a></li>
<li class="chapter" data-level="6.7.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#constructing-optimal-stacked-regressions-with-sl3-1"><i class="fa fa-check"></i><b>6.7.2</b> Constructing Optimal Stacked Regressions with <code>sl3</code></a></li>
<li class="chapter" data-level="6.7.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1"><i class="fa fa-check"></i><b>6.7.3</b> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#extensions-to-causal-effect-of-an-oit"><i class="fa fa-check"></i><b>6.8</b> Extensions to Causal Effect of an OIT</a><ul>
<li class="chapter" data-level="6.8.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simpler-rules"><i class="fa fa-check"></i><b>6.8.1</b> Simpler Rules</a></li>
<li class="chapter" data-level="6.8.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#realistic-optimal-individual-regimes"><i class="fa fa-check"></i><b>6.8.2</b> Realistic Optimal Individual Regimes</a></li>
<li class="chapter" data-level="6.8.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#q-learning"><i class="fa fa-check"></i><b>6.8.3</b> Q-learning</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-analysis-with-oit"><i class="fa fa-check"></i><b>6.9</b> Variable Importance Analysis with OIT</a><ul>
<li class="chapter" data-level="6.9.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#simulated-data-2"><i class="fa fa-check"></i><b>6.9.1</b> Simulated Data</a></li>
<li class="chapter" data-level="6.9.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#variable-importance-using-targeted-estimation-of-the-value-of-the-itr"><i class="fa fa-check"></i><b>6.9.2</b> Variable Importance using Targeted Estimation of the value of the ITR</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#real-world-data-and-tmle3mopttx"><i class="fa fa-check"></i><b>6.10</b> Real World Data and <code>tmle3mopttx</code></a></li>
<li class="chapter" data-level="6.11" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#exercises-2"><i class="fa fa-check"></i><b>6.11</b> Exercises</a><ul>
<li class="chapter" data-level="6.11.1" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#review-of-key-concepts"><i class="fa fa-check"></i><b>6.11.1</b> Review of Key Concepts</a></li>
<li class="chapter" data-level="6.11.2" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#the-ideas-in-action"><i class="fa fa-check"></i><b>6.11.2</b> The Ideas in Action</a></li>
<li class="chapter" data-level="6.11.3" data-path="optimal-individualized-treatment-regimes.html"><a href="optimal-individualized-treatment-regimes.html#advanced-topics"><i class="fa fa-check"></i><b>6.11.3</b> Advanced Topics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html"><i class="fa fa-check"></i><b>7</b> Stochastic Treatment Regimes</a><ul>
<li class="chapter" data-level="7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#learning-objectives-5"><i class="fa fa-check"></i><b>7.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#introduction-to-stochastic-interventions"><i class="fa fa-check"></i><b>7.2</b> Introduction to Stochastic Interventions</a></li>
<li class="chapter" data-level="7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#data-structure-and-notation-1"><i class="fa fa-check"></i><b>7.3</b> Data Structure and Notation</a></li>
<li class="chapter" data-level="7.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>7.4</b> Defining the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="7.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#interpreting-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>7.5</b> Interpreting the Causal Effect of a Stochastic Intervention</a></li>
<li class="chapter" data-level="7.6" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#evaluating-the-causal-effect-of-a-stochastic-intervention"><i class="fa fa-check"></i><b>7.6</b> Evaluating the Causal Effect of a Stochastic Intervention</a><ul>
<li class="chapter" data-level="7.6.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-simulated-data"><i class="fa fa-check"></i><b>7.6.1</b> Example with Simulated Data</a></li>
<li class="chapter" data-level="7.6.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects"><i class="fa fa-check"></i><b>7.6.2</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="7.6.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#statistical-inference-for-targeted-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.6.3</b> Statistical Inference for Targeted Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#extensions-variable-importance-analysis-with-stochastic-interventions"><i class="fa fa-check"></i><b>7.7</b> Extensions: Variable Importance Analysis with Stochastic Interventions</a><ul>
<li class="chapter" data-level="7.7.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#defining-a-grid-of-counterfactual-interventions"><i class="fa fa-check"></i><b>7.7.1</b> Defining a grid of counterfactual interventions</a></li>
<li class="chapter" data-level="7.7.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#initializing-vimshift-through-its-tmle3_spec"><i class="fa fa-check"></i><b>7.7.2</b> Initializing <code>vimshift</code> through its <code>tmle3_Spec</code></a></li>
<li class="chapter" data-level="7.7.3" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#targeted-estimation-of-stochastic-interventions-effects-1"><i class="fa fa-check"></i><b>7.7.3</b> Targeted Estimation of Stochastic Interventions Effects</a></li>
<li class="chapter" data-level="7.7.4" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#inference-with-marginal-structural-models"><i class="fa fa-check"></i><b>7.7.4</b> Inference with Marginal Structural Models</a></li>
<li class="chapter" data-level="7.7.5" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#example-with-the-wash-benefits-data"><i class="fa fa-check"></i><b>7.7.5</b> Example with the WASH Benefits Data</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#exercises-3"><i class="fa fa-check"></i><b>7.8</b> Exercises</a><ul>
<li class="chapter" data-level="7.8.1" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#the-ideas-in-action-1"><i class="fa fa-check"></i><b>7.8.1</b> The Ideas in Action</a></li>
<li class="chapter" data-level="7.8.2" data-path="stochastic-treatment-regimes.html"><a href="stochastic-treatment-regimes.html#review-of-key-concepts-1"><i class="fa fa-check"></i><b>7.8.2</b> Review of Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="r6.html"><a href="r6.html"><i class="fa fa-check"></i><b>8</b> A Primer on the <code>R6</code> Class System</a><ul>
<li class="chapter" data-level="8.1" data-path="r6.html"><a href="r6.html#classes-fields-and-methods"><i class="fa fa-check"></i><b>8.1</b> Classes, Fields, and Methods</a></li>
<li class="chapter" data-level="8.2" data-path="r6.html"><a href="r6.html#object-oriented-programming-python-and-r"><i class="fa fa-check"></i><b>8.2</b> Object Oriented Programming: <code>Python</code> and <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Hitchhiker’s Guide to the <code>tlverse</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimal-individualized-treatment-regimes" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Optimal Individualized Treatment Regimes</h1>
<p><em>Ivana Malenica</em></p>
<p>Based on the <a href="https://github.com/tlverse/tmle3mopttx"><code>tmle3mopttx</code> <code>R</code> package</a>
by <em>Ivana Malenica, Jeremy Coyle, and Mark van der Laan</em>.</p>
<p>Updated: 2020-02-21</p>
<div id="learning-objectives-4" class="section level2">
<h2><span class="header-section-number">6.1</span> Learning Objectives</h2>
<ol style="list-style-type: decimal">
<li>Differentiate dynamic and optimal dynamic treatment regimes from static
interventions.</li>
<li>Understand the benefits and challenges associated with using
optimal individualized treatment regimes in practice.</li>
<li>Contrast the impact of implementing an optimal individualized treatment
in the population with static and dynamic regimes.</li>
<li>Estimate causal effects under optimal individualized treatment regimes with
the <code>tmle3mopttx</code> <code>R</code> package.</li>
<li>Contrast the population impact of implementing optimal individualized
treatment based on sub-optimal rules.</li>
<li>Construct realistic optimal individualized treatments that respect real data
and subject-matter knowledge limitations on interventions.</li>
<li>Understand and implement variable importance analysis defined in
terms of optimal individualized treatment interventions.</li>
</ol>
</div>
<div id="introduction-to-optimal-individualized-interventions" class="section level2">
<h2><span class="header-section-number">6.2</span> Introduction to Optimal Individualized Interventions</h2>
<p>Identifying which intervention will be effective for which patient based on
lifestyle, genetic and environmental factors is a common goal in precision
medicine. To put it in context, Abacavir and Tenofovir are commonly prescribed
as part of the antiretroviral therapy to Human Immunodeficiency Virus (HIV)
patients. However, not all individuals benefit from the two medications equally.
In particular, patients with renal dysfunction might further deteriorate if
prescribed Tenofovir, due to the high nephrotoxicity caused by the medication.
While Tenofovir is still highly effective treatment option for HIV patients, in
order to maximize the patient’s well-being, it would be beneficial to prescribe
Tenofovir only to individuals with healthy kidney function. Along the same
lines, one might seek to improve retention in HIV care. In a randomized clinical
trial, several interventions show efficacy- including appointment reminders
through text messages, small cash incentives for on time clinic visits, and peer
health workers. Ideally, we want to improve effectiveness by assigning each
patient the intervention they are most likely to benefit from, as well as
improve efficiency by not allocating resources to individuals that do not need
them, or would not benefit from it.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="img/image/DynamicA_Illustration.png" alt="Illustration of a Dynamic Treatment Regime in a Clinical Setting" width="60%" />
<p class="caption">
Figure 6.1: Illustration of a Dynamic Treatment Regime in a Clinical Setting
</p>
</div>
<p>One opts to administer the intervention to individuals who will profit from it,
instead of assigning treatment on a population level. But how do we know which
intervention works for which patient? This aim motivates a different type of
intervention, as opposed to the static exposures we might be used to. In
particular, in this chapter we learn about dynamic or individualized interventions
that tailor the treatment decision based on the collected covariates. Formally,
dynamic treatments represent interventions that at each treatment-decision stage
are allowed to respond to the currently available treatment and covariate
history.</p>
<p>In the statistics community such a treatment strategy is termed an
<strong>individualized treatment regime</strong> (ITR), and the (counterfactual) population
mean outcome under an ITR is the value of the ITR <span class="citation">(“On the Application of Probability Theory to Agricultural Experiments. Essay on Principles. Section 9.” <a href="#ref-neyman1990">1990</a>; Robins <a href="#ref-robins1986">1986</a>; Pearl <a href="#ref-pearl2009">2009</a><a href="#ref-pearl2009">b</a>)</span>. Even more, suppose one wishes to maximize the population mean of an
outcome, where for each individual we have access to some set of measured
covariates. This means, for example, that we can learn for which individual
characteristics assigning treatment increases the probability of a beneficial
outcome for each individual. An ITR with the maximal value is referred to as an
optimal ITR or the <strong>optimal individualized treatment</strong>. Consequently, the value
of an optimal ITR is termed the optimal value, or the <strong>mean under the optimal
individualized treatment</strong>.</p>
<p>The problem of estimating the optimal individualized treatment has received much
attention in the statistics literature over the years, especially with the
advancement of precision medicine; see <span class="citation">Murphy (<a href="#ref-murphy2003">2003</a>)</span>, <span class="citation">Robins (<a href="#ref-robins2004">2004</a>)</span>, <span class="citation">Zhang et al. (<a href="#ref-laber2012">2016</a>)</span>,
<span class="citation">Zhao et al. (<a href="#ref-kosorok2012">2012</a>)</span>, <span class="citation">Chakraborty and Moodie (<a href="#ref-moodie2013">2013</a>)</span> and <span class="citation">Robins and Rotnitzky (<a href="#ref-robins2014">2014</a>)</span> to name a few. However, much of the
early work depends on parametric assumptions. As such, even in a randomized
trial, the statistical inference for the optimal individualized treatment relies
on assumptions that are generally believed to be false, and can lead to biased
results.</p>
<p>In this chapter, we consider estimation of the mean outcome under the optimal
individualized treatment where the candidate rules are restricted to depend only
on user-supplied subset of the baseline covariates. The estimation problem is
addressed in a statistical model for the data distribution that is
nonparametric, and at most places restrictions on the probability of a patient
receiving treatment given covariates (as in a randomized trial). As such, we
don’t need to make any assumptions about the relationship of the outcome with
the treatment and covariates, or the relationship between the treatment and
covariates. Further, we provide a Targeted Maximum Likelihood Estimator for the
mean under the optimal individualized treatment that allows us to generate valid
inference for our parameter, without having any parametric assumptions. For a
technical presentation of the algorithm, the interested reader is invited to
further consult <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span> and <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span>.</p>
<hr />
</div>
<div id="data-structure-and-notation" class="section level2">
<h2><span class="header-section-number">6.3</span> Data Structure and Notation</h2>
<p>Suppose we observe <span class="math inline">\(n\)</span> independent and identically distributed observations of
the form <span class="math inline">\(O=(W,A,Y) \sim P_0\)</span>. We denote <span class="math inline">\(A\)</span> as categorical treatment, and <span class="math inline">\(Y\)</span>
as the final outcome. In particular, we define <span class="math inline">\(A \in \mathcal{A}\)</span> where
<span class="math inline">\(\mathcal{A} \equiv \{a_1, \cdots, a_{n_A} \}\)</span> and <span class="math inline">\(n_A = |\mathcal{A}|\)</span>, with
<span class="math inline">\(n_A\)</span> denoting the number of categories (possibly only two, for a binary setup).
Note that we treat <span class="math inline">\(W\)</span> as vector-valued, representing all of our collected
baseline covariates. Therefore, for a single random individual <span class="math inline">\(i\)</span>, we have that
their observed data is <span class="math inline">\(O_i\)</span>: with corresponding baseline covariates <span class="math inline">\(W_i\)</span>,
treatment <span class="math inline">\(A_i\)</span>, and final outcome <span class="math inline">\(Y_i\)</span>. We say that <span class="math inline">\(O \sim P_0\)</span>, or that all
data was drawn from some probability distribution <span class="math inline">\(P_0\)</span>. We emphasize that we
make no assumptions about the distribution of <span class="math inline">\(P_0\)</span>, so that <span class="math inline">\(P_0 \in \mathcal{M}\)</span>, where <span class="math inline">\(\mathcal{M}\)</span> is the fully nonparametric model. As
previously mentioned, this means that we make no assumptions on the relationship
between <span class="math inline">\(Y\)</span> and <span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span>, but might be able to say something about the
relationship of <span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span>, as is the case of a randomized trial. We can assume
a nonparametric structural equation model (NPSEM) to describe generation of <span class="math inline">\(O\)</span>,
as described by <span class="citation">Pearl (<a href="#ref-pearl2009causality">2009</a><a href="#ref-pearl2009causality">a</a>)</span>. Specifically, we have that:
<span class="math display">\[\begin{align*}\label{eqn:npsem}
  W &amp;= f_W(U_W) \\ A &amp;= f_A(W, U_A) \\ Y &amp;= f_Y(A, W, U_Y),
\end{align*}\]</span>
where the collection <span class="math inline">\(f=(f_W,f_A,f_Y)\)</span> denotes unspecified or partially
specified functions. In particular, NPSEM parameterizes <span class="math inline">\(P_0\)</span> in terms of the
distribution of random variables <span class="math inline">\(O\)</span> and <span class="math inline">\(U\)</span>, where <span class="math inline">\(U=(U_W,U_A,U_Y)\)</span> are the
exogenous random variables. We can define counterfactuals <span class="math inline">\(Y_{d(W)}\)</span> defined by
a modified system in which the equation for <span class="math inline">\(A\)</span> is replaced by the rule <span class="math inline">\(d(W)\)</span>,
dependent on covariates <span class="math inline">\(W\)</span>.</p>
<p>The likelihood of the data admits a factorization, implied by the time ordering
of <span class="math inline">\(O\)</span>. We denote the density of <span class="math inline">\(O\)</span> as <span class="math inline">\(p_0\)</span>, corresponding to the distribution
<span class="math inline">\(P_0\)</span> and dominating measure <span class="math inline">\(\mu\)</span>.
<span class="math display">\[\begin{equation*}\label{eqn:likelihood_factorization}
  p_0(O) = p_{Y,0}(Y|A,W) p_{A,0}(A|W) p_{W,0}(W) = q_{Y,0}(Y|A,W) q_{A,0}(A|W)
    q_{W,0}(W),
\end{equation*}\]</span>
where <span class="math inline">\(p_{Y,0}(Y|A,W)\)</span> is the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\((A, W)\)</span> with
respect to some dominating measure <span class="math inline">\(\mu_Y\)</span>, <span class="math inline">\(p_{A,0}\)</span> is the conditional density
of <span class="math inline">\(A\)</span> given <span class="math inline">\(W\)</span> with respect to dominating measure <span class="math inline">\(\mu_A\)</span>, and <span class="math inline">\(p_{W,0}\)</span> is
the density of <span class="math inline">\(W\)</span> with respect to dominating measure <span class="math inline">\(\mu_W\)</span>. Consequently, we
define <span class="math inline">\(P_{Y,0}(Y|A,W)=Q_{Y,0}(Y|A,W)\)</span>, <span class="math inline">\(P_{A,0}(A|W)=g_0(A|W)\)</span> and
<span class="math inline">\(P_{W,0}(W)=Q_{W,0}(W)\)</span> as the corresponding conditional distributions of <span class="math inline">\(Y\)</span>,
<span class="math inline">\(A\)</span> and <span class="math inline">\(W\)</span>. For notational simplicity, we define <span class="math inline">\(\bar{Q}_{Y,0}(A,W) \equiv E_0[Y|A,W]\)</span> as the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\((A,W)\)</span>.</p>
<p>In addition, we denote <span class="math inline">\(V\)</span> as <span class="math inline">\(V \in W\)</span>, defining a subset of the baseline
covariates the optimal individualized rule depends on. Note that <span class="math inline">\(V\)</span> could be
all of <span class="math inline">\(W\)</span>, or an empty set, depending on the subject matter knowledge. In
particular, a researcher might want to consider known effect modifiers available
at the time of treatment decision as possible <span class="math inline">\(V\)</span> covariates. Defining <span class="math inline">\(V\)</span>
allows us to consider possibly sub-optimal rules that are easier to estimate,
and thereby allows for statistical inference for the counterfactual mean outcome
under the sub-optimal rule.</p>
</div>
<div id="defining-the-causal-effect-of-an-optimal-individualized-intervention" class="section level2">
<h2><span class="header-section-number">6.4</span> Defining the Causal Effect of an Optimal Individualized Intervention</h2>
<p>Consider dynamic treatment rules <span class="math inline">\(V \rightarrow d(V) \in \{a_1, \cdots, a_{n_A} \} \times \{1\}\)</span>, for assigning treatment <span class="math inline">\(A\)</span> based on <span class="math inline">\(V \in W\)</span>. As
mentioned in the previous section, causal effects are defined in terms of
hypothetical interventions on the NPSEM (). Our modified system
then takes the following form:
<span class="math display">\[\begin{align*}\label{eqn:npsem_causal}
  W &amp;= f_W(U_W) \\ A &amp;= d(V) \\ Y_{d(V)} &amp;= f_Y(d(V), W, U_Y),
\end{align*}\]</span></p>
<p>where the dynamic treatment regime may be viewed as an intervention in which
<span class="math inline">\(A\)</span> is set equal to a value based on a hypothetical regime <span class="math inline">\(d(V)\)</span>, and
<span class="math inline">\(Y_{d(V)}\)</span> is the corresponding outcome under <span class="math inline">\(d(V)\)</span>. We denote the distribution
of the counterfactual quantities as <span class="math inline">\(P_{0,d(V)}\)</span>.</p>
<p>The goal of any causal analysis motivated by such dynamic, or optimal
individualized intervention, is to estimate a parameter defined as the
counterfactual mean of the outcome with respect to the modified intervention
distribution (either dynamic or optimal dynamic). We are primarily interested in
the value of an individualized rule, <span class="math inline">\(E_0[Y_{d(V)}]\)</span>. The optimal rule is the
rule with the maximal value: <span class="math display">\[d_{opt}(V) \equiv \text{argmax}_{d(V) \in
\mathcal{D}} E_0[Y_{d(V)}]\]</span> where <span class="math inline">\(\mathcal{D}\)</span> represents the set of possible
rules, <span class="math inline">\(d\)</span>, implied by <span class="math inline">\(V\)</span>. We note that, in case the problem at hand requires
minimizing the mean of an outcome, our optimal individualized rule will be the
rule with the minimal value instead. Finally, our target parameter can be
expressed as
<span class="math display">\[\psi_0 := E_0[Y_{d_{opt}(V)}].\]</span></p>
<p>The optimal individualized rule, as well as the value of a rule, are causal
parameters based on the unobserved counterfactuals. In order for the causal
quantities to be estimated from the observed data, they need to be identified
with statistical parameters. This step of the roadmap requires me make few
assumptions:</p>
<ol style="list-style-type: decimal">
<li><em>Consistency</em>: <span class="math inline">\(Y^{d(v_i)}_i = Y_i\)</span> in the event <span class="math inline">\(A_i = d(v_i)\)</span>,
for <span class="math inline">\(i = 1, \ldots, n\)</span>.</li>
<li><em>Stable unit value treatment assumption (SUTVA)</em>: <span class="math inline">\(Y^{d(v_i)}_i\)</span> does
not depend on <span class="math inline">\(d(v_j)\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span> and <span class="math inline">\(j \neq i\)</span>, or lack
of interference.</li>
<li><em>Strong ignorability</em>: <span class="math inline">\(A \perp \!\!\! \perp Y^{d(v)} \mid W\)</span>, for all <span class="math inline">\(a \in \mathcal{A}\)</span>.</li>
<li><em>Positivity (or overlap)</em>: <span class="math inline">\(P_0(\min_{a \in \mathcal{A}} g_0(a|W) &gt; 0)=1\)</span></li>
</ol>
<p>Under the above causal assumptions, we can identify <span class="math inline">\(P_{0,d}\)</span> with observed data
using the G-computation formula:</p>
<p><span class="math display">\[P_{0,d_{opt}}(O) = Q_{Y,0}(Y|A=d_{opt}(V),W)g_0(A=d_{opt}(V)|W)Q_{W,0}(W).\]</span>
The value of an individualized rule can now be expressed as</p>
<p><span class="math display">\[E_0[Y_{d(V)}] = E_{0,W}[\bar{Q}_{Y,0}(A=d(V),W)],\]</span></p>
<p>which, under causal assumptions, can is interpreted as the mean outcome if
(possibly contrary to fact), treatment was assigned according to the rule.
Finally, the statistical counterpart to the causal parameter of interest is
defined as
<span class="math display">\[\psi_0 = E_{0,W}[\bar{Q}_{Y,0}(A=d_{opt}(V),W)].\]</span></p>
<p>Inference for the optimal value has been shown to be difficult at exceptional
laws, defined as probability distributions for which treatment is neither
beneficial nor harmful. Inference is similarly difficult in finite samples if
the treatment effect is very small in all strata, even though valid asymptotic
estimators exist in this setting. With that in mind, we address the estimation
problem under the assumption of non-exceptional laws in effect.</p>
<p>Many methods for learning the optimal rule from data have been developed
<span class="citation">(Murphy <a href="#ref-murphy2003">2003</a>, <span class="citation">@robins2004</span>, <span class="citation">@laber2012</span>, <span class="citation">@kosorok2012</span>, <span class="citation">@moodie2013</span>)</span>. In this
chapter, we focus on the methods discussed in <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> and
<span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>. Note however, that <code>tmle3mopttx</code> also supports the widely
used Q-learning approach, where the optimal individualized rule is based on the
initial estimate of <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> <span class="citation">(Sutton, Barto, and others <a href="#ref-Sutton1998">1998</a>)</span>.</p>
<p>We follow the methodology outlined in <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> and
<span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>, where we learn the optimal ITR using Super Learner
<span class="citation">(van der Laan, Polley, and Hubbard <a href="#ref-vdl2007super">2007</a>)</span>, and estimate its value with cross-validated Targeted Minimum
Loss-based Estimation (CV-TMLE) <span class="citation">(Zheng and van der Laan <a href="#ref-cvtmle2010">2010</a>)</span>. In great generality, we first
need to estimate the true individual treatment regime, <span class="math inline">\(d_0(V)\)</span>, which
corresponds to dynamic treatment rule (<span class="math inline">\(d(V)\)</span>) that takes a subset of covariates
<span class="math inline">\(V \in W\)</span> and assigns treatment to each individual based on their observed
covariates <span class="math inline">\(v\)</span>. With the estimate of the true optimal ITR in hand, we can
estimate its corresponding value.</p>
<div id="binary-treatment" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Binary treatment</h3>
<p>How do we estimate the optimal individualized treatment regime? In the case of a
binary treatment, a key quantity for optimal ITR is the blip function. One can
show that any optimal ITR assigns treatment to individuals falling in strata in
which the stratum specific average treatment effect, the blip function, is
positive and does not assign treatment to individuals for which this quantity is
negative. Therefore for a binary treatment, under causal assumptions, we define
the blip function as:
<span class="math display">\[\bar{Q}_0(V) \equiv E_0[Y_1-Y_0|V] \equiv E_0[\bar{Q}_{Y,0}(1,W) -
\bar{Q}_{Y,0}(0,W) | V],\]</span>
or the average treatment effect within a stratum of <span class="math inline">\(V\)</span>. The note that the
optimal individualized rule can now be derived as <span class="math inline">\(d_{opt}(V) = I(\bar{Q}_{0}(V) &gt; 0)\)</span>.</p>
<p>The package <code>tmle3mopttx</code> relies on using the Super Learner to estimate the blip
function, as it easily extends to more general categorical treatment. With that
in mind, the loss function utilized for learning the optimal individualized rule
corresponds to conditional mean type losses. It is however worth mentioning that
<span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> present three different approaches for learning the optimal
rule. Namely, they focus on:</p>
<ol style="list-style-type: decimal">
<li><p>Super Learning the Blip Function,</p></li>
<li><p>Super Learning the Weighted Classification Problem,</p></li>
<li><p>Joint Super Learner of the Blip and Weighted Classification Problem.</p></li>
</ol>
<p>We refer the interested reader to <span class="citation">Luedtke and van der Laan (<a href="#ref-luedtke2016super">2016</a>)</span> for further reference on
advantages of each approach.</p>
<p>Relying on the Targeted Maximum Likelihood (TML) estimator and the Super Learner
estimate of the blip function, we follow the below steps in order to obtain
value of the ITR:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> and <span class="math inline">\(g_0(A|W)\)</span> using <code>sl3</code>. We denote such
estimates as <span class="math inline">\(\bar{Q}_{Y,n}(A,W)\)</span> and <span class="math inline">\(g_n(A|W)\)</span>.</li>
<li>Apply the doubly robust Augmented-Inverse Probability Weighted (A-IPW)
transform to our outcome, where we define:
<span class="math display">\[D_{\bar{Q}_Y,g,a}(O) \equiv \frac{I(A=a)}{g(A|W)} (Y-\bar{Q}_Y(A,W)) +
  \bar{Q}_Y(A=a,W)\]</span></li>
</ol>
<p>Note that under the randomization and positivity assumptions we have that
<span class="math inline">\(E[D_{\bar{Q}_Y,g,a}(O) | V] = E[Y_a |V]\)</span>. We emphasize the double robust nature
of the A-IPW transform- consistency of <span class="math inline">\(E[Y_a |V]\)</span> will depend on correct
estimation of either <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> or <span class="math inline">\(g_0(A|W)\)</span>. As such, in a
randomized trial, we are guaranteed a consistent estimate of <span class="math inline">\(E[Y_a |V]\)</span> even if
we get <span class="math inline">\(\bar{Q}_{Y,0}(A,W)\)</span> wrong!</p>
<p>Using this transform, we can define the following contrast:
<span class="math inline">\(D_{\bar{Q}_Y,g}(O) = D_{\bar{Q}_Y,g,a=1}(O) - D_{\bar{Q}_Y,g,a=0}(O)\)</span></p>
<p>We estimate the blip function, <span class="math inline">\(\bar{Q}_{0,a}(V)\)</span>, by regressing
<span class="math inline">\(D_{\bar{Q}_Y,g}(O)\)</span> on <span class="math inline">\(V\)</span> using the specified <code>sl3</code> library of learners and an
appropriate loss function.</p>
<ol start="3" style="list-style-type: decimal">
<li>Our estimated rule is <span class="math inline">\(d(V) = \text{argmax}_{a \in \mathcal{A}} \bar{Q}_{0,a}(V)\)</span>.</li>
<li>We obtain inference for the mean outcome under the estimated optimal rule
using CV-TMLE.</li>
</ol>
</div>
<div id="categorical-treatment" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Categorical treatment</h3>
<p>In line with the approach considered for binary treatment, we extend the blip
function to allow for categorical treatment. We denote such blip function
extensions as <em>pseudo-blips</em>, which are our new estimation targets in a
categorical setting. We define pseudo-blips as vector-valued entities where the
output for a given <span class="math inline">\(V\)</span> is a vector of length equal to the number of treatment
categories, <span class="math inline">\(n_A\)</span>. As such, we define it as:
<span class="math display">\[\bar{Q}_0^{pblip}(V) = \{\bar{Q}_{0,a}^{pblip}(V): a \in \mathcal{A} \}\]</span></p>
<p>We implement three different pseudo-blips in <code>tmle3mopttx</code>.</p>
<ol style="list-style-type: decimal">
<li><p><em>Blip1</em> corresponds to choosing a reference category of treatment, and
defining the blip for all other categories relative to the specified
reference. Hence we have that:
<span class="math display">\[\bar{Q}_{0,a}^{pblip-ref}(V) \equiv E_0(Y_a-Y_0|V)\]</span> where <span class="math inline">\(Y_0\)</span> is the
specified reference category with <span class="math inline">\(A=0\)</span>. Note that, for the case of binary
treatment, this strategy reduces to the approach described for the binary
setup.</p></li>
<li><p><em>Blip2</em> approach corresponds to defining the blip relative to the average of
all categories. As such, we can define <span class="math inline">\(\bar{Q}_{0,a}^{pblip-avg}(V)\)</span> as:
<span class="math display">\[\bar{Q}_{0,a}^{pblip-avg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in
  \mathcal{A}} Y_a|V)\]</span>
In the case where subject-matter knowledge regarding which reference category
to use is not available, blip2 might be a viable option.</p></li>
<li><p><em>Blip3</em> reflects an extension of Blip2, where the average is now a weighted
average:
<span class="math display">\[\bar{Q}_{0,a}^{pblip-wavg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in
  \mathcal{A}} Y_{a} P(A=a|V) |V)\]</span></p></li>
</ol>
<p>Just like in the binary case, pseudo-blips are estimated by regressing contrasts
composed using the A-IPW transform on <span class="math inline">\(V\)</span>.</p>
</div>
<div id="note-on-inference" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Note on Inference</h3>
<p>In a randomized trial, statistical inference relies on the second-order
difference between the estimator of the optimal individualized treatment and the
optimal individualized treatment itself to be asymptotically negligible. This is
a reasonable condition if we consider rules that depend on small number of
covariates, or if we are willing to make smoothness assumptions. Alternatively,
we can consider TMLEs and statistical inference for data-adaptive target
parameters defined in terms of an estimate of the optimal individualized
treatment. In particular, instead of trying to estimate the mean under the true
optimal individualized treatment, we aim to estimate the mean under the
estimated optimal individualized treatment. As such, we develop cross-validated
TMLE approach that provides asymptotic inference under minimal conditions for
the mean under the estimate of the optimal individualized treatment. In
particular, considering the data adaptive parameter allows us to avoid
consistency and rate condition for the fitted optimal rule, as required for
asymptotic linearity of the TMLE of the mean under the actual, true optimal
rule. Practically, the estimated (data-adaptive) rule should be preferred, as
this possibly sub-optimal rule is the one implemented in the population.</p>
</div>
<div id="why-cv-tmle" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Why CV-TMLE?</h3>
<p>As discussed in <span class="citation">van der Laan and Luedtke (<a href="#ref-vanderLaanLuedtke15">2015</a>)</span>, CV-TMLE is necessary as the
non-cross-validated TMLE is biased upward for the mean outcome under the rule,
and therefore overly optimistic. More generally however, using CV-TMLE allows us
more freedom in estimation and therefore greater data adaptivity, without
sacrificing inference.</p>
</div>
</div>
<div id="interpreting-the-causal-effect-of-an-optimal-individualized-intervention" class="section level2">
<h2><span class="header-section-number">6.5</span> Interpreting the Causal Effect of an Optimal Individualized Intervention</h2>
<p>In summary, the mean outcome under the optimal individualized treatment is a
counterfactual quantity of interest representing what the mean outcome would
have been if everybody, contrary to the fact, received treatment that optimized
their outcome. The optimal individualized treatment regime is a rule that
optimizes the mean outcome under the dynamic treatment, where the candidate
rules are restricted to only respond to a user-supplied subset of the baseline
and intermediate covariates. In essence, our target parameter answers the key
aim of precision medicine: allocating the available treatment by tailoring it to
the individual characteristics of the patient, with the goal of optimizing the
final outcome.</p>
</div>
<div id="evaluating-the-causal-effect-of-an-oit-with-binary-treatment" class="section level2">
<h2><span class="header-section-number">6.6</span> Evaluating the Causal Effect of an OIT with Binary Treatment</h2>
<p>Finally, we demonstrate how to evaluate the mean outcome under the optimal
individualized treatment using <code>tmle3mopptx</code>. To start, let’s load the packages
we’ll use and set a seed:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1"><span class="kw">library</span>(here)</a>
<a class="sourceLine" id="cb82-2" data-line-number="2"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb82-3" data-line-number="3"><span class="kw">library</span>(sl3)</a>
<a class="sourceLine" id="cb82-4" data-line-number="4"><span class="kw">library</span>(tmle3)</a>
<a class="sourceLine" id="cb82-5" data-line-number="5"><span class="kw">library</span>(tmle3mopttx)</a>
<a class="sourceLine" id="cb82-6" data-line-number="6"><span class="kw">library</span>(devtools)</a>
<a class="sourceLine" id="cb82-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">111</span>)</a></code></pre></div>
<div id="simulated-data" class="section level3">
<h3><span class="header-section-number">6.6.1</span> Simulated Data</h3>
<p>First, we load the simulated data. We will start with the more general setup
where the treatment is a binary variable; later in the chapter we will consider
another data-generating distribution where <span class="math inline">\(A\)</span> is categorical. In this example,
our data generating distribution is of the following form:
<span class="math display">\[\begin{align*}
  W &amp;\sim \mathcal{N}(\bf{0},I_{3 \times 3})\\
  P(A=1|W) &amp;= \frac{1}{1+\exp^{(-0.8*W_1)}}\\
  P(Y=1|A,W) &amp;= 0.5\text{logit}^{-1}[-5I(A=1)(W_1-0.5)+5I(A=0)(W_1-0.5)] +
     0.5\text{logit}^{-1}(W_2W_3)
\end{align*}\]</span></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;data_bin&quot;</span>)</a></code></pre></div>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. Note that the
mean under the true optimal rule is <span class="math inline">\(\psi=0.578\)</span> for this data generating
distribution.</p>
<p>To formally express this fact using the <code>tlverse</code> grammar introduced by the
<code>tmle3</code> package, we create a single data object and specify the functional
relationships between the nodes in the <em>directed acyclic graph</em> (DAG) via
<em>nonparametric structural equation models</em> (NPSEMs), reflected in the node list
that we set up:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="co"># organize data and nodes for tmle3</span></a>
<a class="sourceLine" id="cb84-2" data-line-number="2">data &lt;-<span class="st"> </span>data_bin</a>
<a class="sourceLine" id="cb84-3" data-line-number="3">node_list &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb84-4" data-line-number="4">  <span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>),</a>
<a class="sourceLine" id="cb84-5" data-line-number="5">  <span class="dt">A =</span> <span class="st">&quot;A&quot;</span>,</a>
<a class="sourceLine" id="cb84-6" data-line-number="6">  <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span></a>
<a class="sourceLine" id="cb84-7" data-line-number="7">)</a></code></pre></div>
<p>We now have an observed data structure (<code>data</code>) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.</p>
</div>
<div id="constructing-optimal-stacked-regressions-with-sl3" class="section level3">
<h3><span class="header-section-number">6.6.2</span> Constructing Optimal Stacked Regressions with <code>sl3</code></h3>
<p>To easily incorporate ensemble machine learning into the estimation procedure,
we rely on the facilities provided in the <a href="https://tlverse.org/sl3"><code>sl3</code> R
package</a>. Using the framework provided by the <a href="https://tlverse.org/sl3"><code>sl3</code>
package</a>, the nuisance parameters of the TML estimator
may be fit with ensemble learning, using the cross-validation framework of the
Super Learner algorithm of <span class="citation">van der Laan, Polley, and Hubbard (<a href="#ref-vdl2007super">2007</a>)</span>.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># Define sl3 library and metalearners:</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2">lrn_xgboost_<span class="dv">50</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb85-3" data-line-number="3">lrn_xgboost_<span class="dv">100</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb85-4" data-line-number="4">lrn_xgboost_<span class="dv">500</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb85-5" data-line-number="5">lrn_mean &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb85-6" data-line-number="6">lrn_glm &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb85-7" data-line-number="7"></a>
<a class="sourceLine" id="cb85-8" data-line-number="8">## Define the Q learner:</a>
<a class="sourceLine" id="cb85-9" data-line-number="9">Q_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb85-10" data-line-number="10">  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>,</a>
<a class="sourceLine" id="cb85-11" data-line-number="11">                  lrn_xgboost_<span class="dv">500</span>, lrn_mean, lrn_glm),</a>
<a class="sourceLine" id="cb85-12" data-line-number="12">  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb85-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb85-14" data-line-number="14"></a>
<a class="sourceLine" id="cb85-15" data-line-number="15">## Define the g learner:</a>
<a class="sourceLine" id="cb85-16" data-line-number="16">g_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb85-17" data-line-number="17">  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_xgboost_<span class="dv">100</span>, lrn_glm),</a>
<a class="sourceLine" id="cb85-18" data-line-number="18">  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb85-19" data-line-number="19">)</a>
<a class="sourceLine" id="cb85-20" data-line-number="20"></a>
<a class="sourceLine" id="cb85-21" data-line-number="21">## Define the B learner:</a>
<a class="sourceLine" id="cb85-22" data-line-number="22">b_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb85-23" data-line-number="23">  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>,</a>
<a class="sourceLine" id="cb85-24" data-line-number="24">                  lrn_xgboost_<span class="dv">500</span>,lrn_mean, lrn_glm),</a>
<a class="sourceLine" id="cb85-25" data-line-number="25">  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb85-26" data-line-number="26">)</a></code></pre></div>
<p>As seen above, we generate three different ensemble learners that must be fit,
corresponding to the learners for the outcome regression (Q), propensity score
(g), and the blip function (B). We make the above explicit with respect to
standard notation by bundling the ensemble learners into a list object below:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="co"># specify outcome and treatment regressions and create learner list</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2">learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</a></code></pre></div>
<p>The <code>learner_list</code> object above specifies the role that each of the ensemble
learners we’ve generated is to play in computing initial estimators. Recall that
we need initial estimators of relevant parts of the likelihood in order to
building a TMLE for the parameter of interest. In particular, <code>learner_list</code>
makes explicit the fact that our <code>Y</code> is used in fitting the outcome regression,
while <code>A</code> is used in fitting the treatment mechanism regression, and finally <code>B</code>
is used in fitting the blip function.</p>
</div>
<div id="targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects" class="section level3">
<h3><span class="header-section-number">6.6.3</span> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</h3>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest simply by calling <code>tmle3_mopttx_blip_revere</code>. We specify the argument
<code>V = c(&quot;W1&quot;, &quot;W2&quot;, &quot;W3&quot;)</code> when initializing the <code>tmle3_Spec</code> object in order to
communicate that we’re interested in learning a rule dependent on <code>V</code>
covariates. Note that we don’t have to specify <code>V</code>- this will result in a rule
that is not based on any collected covariates. We also need to specify the type
of pseudo-blip we will use in this estimation problem, the list of learners used
to estimate the blip function, whether we want to maximize or minimize the final
outcome, and few other more advanced features including searching for a less
complex rule and realistic interventions.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb87-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(</a>
<a class="sourceLine" id="cb87-3" data-line-number="3">  <span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip1&quot;</span>,</a>
<a class="sourceLine" id="cb87-4" data-line-number="4">  <span class="dt">learners =</span> learner_list,</a>
<a class="sourceLine" id="cb87-5" data-line-number="5">  <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>, <span class="dt">realistic =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb87-6" data-line-number="6">)</a></code></pre></div>
<p>As seen above, the <code>tmle3_mopttx_blip_revere</code> specification object
(like all <code>tmle3_Spec</code> objects) does <em>not</em> store the data for our
specific analysis of interest. Later,
we’ll see that passing a data object directly to the <code>tmle3</code> wrapper function,
alongside the instantiated <code>tmle_spec</code>, will serve to construct a <code>tmle3_Task</code>
object internally.</p>
<p>We elaborate more on the initialization specifications. In initializing the
specification for the TMLE of our parameter of interest, we have specified the
set of covariates the rule depends on (<code>V</code>), the type of pseudo-blip to use
(<code>type</code>), and the learners used for estimating the relevant parts of the
likelihood and the blip function. In addition, we need to specify whether we
want to maximize the mean outcome under the rule (<code>maximize</code>), and whether we
want to estimate the rule under all the covariates <span class="math inline">\(V\)</span> provided by the user
(<code>complex</code>). If <code>FALSE</code>, <code>tmle3mopttx</code> will instead consider all the possible
rules under a smaller set of covariates including the static rules, and optimize
the mean outcome over all the subsets of <span class="math inline">\(V\)</span>. As such, while the user might have
provided a full set of collected covariates as input for <span class="math inline">\(V\)</span>, it is possible
that the true rule only depends on a subset of the set provided by the user. In
that case, our returned mean under the optimal individualized rule will be based
on the smaller subset. In addition, we provide an option to search for realistic
optimal individualized interventions via the <code>realistic</code> specification. If
<code>TRUE</code>, only treatments supported by the data will be considered, therefore
alleviating concerns regarding practical positivity issues. We explore all the
important extensions of <code>tmle3mopttx</code> in later sections.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</a>
<a class="sourceLine" id="cb88-3" data-line-number="3">fit</a></code></pre></div>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est         se     lower     upper
1:  TSM E[Y_{A=NULL}] 0.4289592 0.5701264 0.02749039 0.5162462 0.6240065
   psi_transformed lower_transformed upper_transformed
1:       0.5701264         0.5162462         0.6240065</code></pre>
<p>We can see that the estimate of <span class="math inline">\(psi_0\)</span> is <span class="math inline">\(0.56\)</span>, and that the confidence
interval covers our true mean under the true optimal individualized treatment.</p>
</div>
</div>
<div id="evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment" class="section level2">
<h2><span class="header-section-number">6.7</span> Evaluating the Causal Effect of an optimal ITR with Categorical Treatment</h2>
<p>In this section, we consider how to evaluate the mean outcome under the optimal
individualized treatment when <span class="math inline">\(A\)</span> has more than two categories. While the
procedure is analogous to the previously described binary treatment, we now need
to pay attention to the type of blip we define in the estimation stage, as well
as how we construct our learners.</p>
<div id="simulated-data-1" class="section level3">
<h3><span class="header-section-number">6.7.1</span> Simulated Data</h3>
<p>First, we load the simulated data. Here, our data generating distribution was
of the following form:
<span class="math display">\[\begin{align*}
  W &amp;\sim \mathcal{N}(\bf{0},I_{4 \times 4})\\
  P(A=a|W) &amp;= \frac{1}{1+\exp^{(-0.8*W_1)}}\\
  P(Y=1|A,W) = 0.5\text{logit}^{-1}[15I(A=1)(W_1-0.5) - 3I(A=2)(2W_1+0.5) +
    3I(A=3)(3W_1-0.5)] +\text{logit}^{-1}(W_2W_1)
\end{align*}\]</span></p>
<p>We can just load the data available as part of the package as follows:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;data_cat_realistic&quot;</span>)</a></code></pre></div>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. Note that the
mean under the true optimal rule is <span class="math inline">\(\psi=0.658\)</span>, which is the quantity we aim
to estimate.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="co"># organize data and nodes for tmle3</span></a>
<a class="sourceLine" id="cb91-2" data-line-number="2">data &lt;-<span class="st"> </span>data_cat_realistic</a>
<a class="sourceLine" id="cb91-3" data-line-number="3">node_list &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb91-4" data-line-number="4">  <span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W4&quot;</span>),</a>
<a class="sourceLine" id="cb91-5" data-line-number="5">  <span class="dt">A =</span> <span class="st">&quot;A&quot;</span>,</a>
<a class="sourceLine" id="cb91-6" data-line-number="6">  <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span></a>
<a class="sourceLine" id="cb91-7" data-line-number="7">)</a></code></pre></div>
</div>
<div id="constructing-optimal-stacked-regressions-with-sl3-1" class="section level3">
<h3><span class="header-section-number">6.7.2</span> Constructing Optimal Stacked Regressions with <code>sl3</code></h3>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="co"># Initialize few of the learners:</span></a>
<a class="sourceLine" id="cb92-2" data-line-number="2">lrn_xgboost_<span class="dv">50</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb92-3" data-line-number="3">lrn_xgboost_<span class="dv">100</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb92-4" data-line-number="4">lrn_xgboost_<span class="dv">500</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb92-5" data-line-number="5">lrn_mean &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb92-6" data-line-number="6">lrn_glm &lt;-<span class="st"> </span>Lrnr_glm_fast<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb92-7" data-line-number="7"></a>
<a class="sourceLine" id="cb92-8" data-line-number="8">## Define the Q learner, which is just a regular learner:</a>
<a class="sourceLine" id="cb92-9" data-line-number="9">Q_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb92-10" data-line-number="10">  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>, lrn_xgboost_<span class="dv">500</span>, lrn_mean,</a>
<a class="sourceLine" id="cb92-11" data-line-number="11">                  lrn_glm),</a>
<a class="sourceLine" id="cb92-12" data-line-number="12">  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb92-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb92-14" data-line-number="14"></a>
<a class="sourceLine" id="cb92-15" data-line-number="15"><span class="co"># Define the g learner, which is a multinomial learner:</span></a>
<a class="sourceLine" id="cb92-16" data-line-number="16"><span class="co">#specify the appropriate loss of the multinomial learner:</span></a>
<a class="sourceLine" id="cb92-17" data-line-number="17">mn_metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_solnp,</a>
<a class="sourceLine" id="cb92-18" data-line-number="18">                               <span class="dt">loss_function =</span> loss_loglik_multinomial,</a>
<a class="sourceLine" id="cb92-19" data-line-number="19">                               <span class="dt">learner_function =</span></a>
<a class="sourceLine" id="cb92-20" data-line-number="20">                                 metalearner_linear_multinomial)</a>
<a class="sourceLine" id="cb92-21" data-line-number="21">g_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl,</a>
<a class="sourceLine" id="cb92-22" data-line-number="22">                          <span class="kw">list</span>(lrn_xgboost_<span class="dv">100</span>, lrn_xgboost_<span class="dv">500</span>, lrn_mean),</a>
<a class="sourceLine" id="cb92-23" data-line-number="23">                          mn_metalearner)</a>
<a class="sourceLine" id="cb92-24" data-line-number="24"></a>
<a class="sourceLine" id="cb92-25" data-line-number="25"><span class="co"># Define the Blip learner, which is a multivariate learner:</span></a>
<a class="sourceLine" id="cb92-26" data-line-number="26">learners &lt;-<span class="st"> </span><span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>, lrn_xgboost_<span class="dv">500</span>, lrn_mean,</a>
<a class="sourceLine" id="cb92-27" data-line-number="27">                 lrn_glm)</a>
<a class="sourceLine" id="cb92-28" data-line-number="28">b_learner &lt;-<span class="st"> </span><span class="kw">create_mv_learners</span>(<span class="dt">learners =</span> learners)</a></code></pre></div>
<p>As seen above, we generate three different ensemble learners that must be fit,
corresponding to the learners for the outcome regression, propensity score, and
the blip function. Note that we need to estimate <span class="math inline">\(g_0(A|W)\)</span> for a categorical
<span class="math inline">\(A\)</span> – therefore, we use the multinomial Super Learner option available within
the <code>sl3</code> package with learners that can address multi-class classification
problems. In order to see which learners can be used to estimate <span class="math inline">\(g_0(A|W)\)</span> in
<code>sl3</code>, we run the following:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="co"># See which learners support multi-class classification:</span></a>
<a class="sourceLine" id="cb93-2" data-line-number="2"><span class="kw">sl3_list_learners</span>(<span class="kw">c</span>(<span class="st">&quot;categorical&quot;</span>))</a></code></pre></div>
<pre><code> [1] &quot;Lrnr_bartMachine&quot;           &quot;Lrnr_caret&quot;                
 [3] &quot;Lrnr_dbarts&quot;                &quot;Lrnr_gam&quot;                  
 [5] &quot;Lrnr_glmnet&quot;                &quot;Lrnr_grf&quot;                  
 [7] &quot;Lrnr_h2o_glm&quot;               &quot;Lrnr_h2o_grid&quot;             
 [9] &quot;Lrnr_independent_binomial&quot;  &quot;Lrnr_mean&quot;                 
[11] &quot;Lrnr_multivariate&quot;          &quot;Lrnr_optim&quot;                
[13] &quot;Lrnr_polspline&quot;             &quot;Lrnr_pooled_hazards&quot;       
[15] &quot;Lrnr_randomForest&quot;          &quot;Lrnr_ranger&quot;               
[17] &quot;Lrnr_rpart&quot;                 &quot;Lrnr_screener_corP&quot;        
[19] &quot;Lrnr_screener_corRank&quot;      &quot;Lrnr_screener_randomForest&quot;
[21] &quot;Lrnr_solnp&quot;                 &quot;Lrnr_svm&quot;                  
[23] &quot;Lrnr_xgboost&quot;              </code></pre>
<p>Note that since the corresponding blip will be vector valued, we will have a
column for each additional level of treatment. As such, we need to create
multivariate learners with the helper function <code>create_mv_learners</code> that takes a
list of initialized learners as input.</p>
<p>We make the above explicit with respect to standard notation by bundling the
ensemble learners into a list object below:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="co"># specify outcome and treatment regressions and create learner list</span></a>
<a class="sourceLine" id="cb95-2" data-line-number="2">learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</a></code></pre></div>
</div>
<div id="targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1" class="section level3">
<h3><span class="header-section-number">6.7.3</span> Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects</h3>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(</a>
<a class="sourceLine" id="cb96-3" data-line-number="3">  <span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W4&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>,</a>
<a class="sourceLine" id="cb96-4" data-line-number="4">  <span class="dt">learners =</span> learner_list, <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb96-5" data-line-number="5">  <span class="dt">realistic =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb96-6" data-line-number="6">)</a></code></pre></div>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb97-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</a>
<a class="sourceLine" id="cb97-3" data-line-number="3"></a>
<a class="sourceLine" id="cb97-4" data-line-number="4">fit</a></code></pre></div>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est         se     lower     upper
1:  TSM E[Y_{A=NULL}] 0.5415567 0.6031287 0.07562618 0.4549041 0.7513533
   psi_transformed lower_transformed upper_transformed
1:       0.6031287         0.4549041         0.7513533</code></pre>
<p>We can see that the estimate of <span class="math inline">\(psi_0\)</span> is <span class="math inline">\(0.60\)</span>, and that the confidence
interval covers our true mean under the true optimal individualized treatment.</p>
</div>
</div>
<div id="extensions-to-causal-effect-of-an-oit" class="section level2">
<h2><span class="header-section-number">6.8</span> Extensions to Causal Effect of an OIT</h2>
<p>In this section, we consider two extensions to the procedure described for
estimating the value of the OIT. First one considers a setting where the user
might be interested in a grid of possible sub-optimal rules, corresponding to
potentially limited knowledge of potential effect modifiers. The second
extension concerns implementation of a realistic optimal individual
interventions where certain regimes might be preferred, but due to practical or
global positivity restraints are not realistic to implement.</p>
<div id="simpler-rules" class="section level3">
<h3><span class="header-section-number">6.8.1</span> Simpler Rules</h3>
<p>In order to not only consider the most ambitious fully <span class="math inline">\(V\)</span>-optimal rule, we
define <span class="math inline">\(S\)</span>-optimal rules as the optimal rule that considers all possible subsets
of <span class="math inline">\(V\)</span> covariates, with card(<span class="math inline">\(S\)</span>) <span class="math inline">\(\leq\)</span> card(<span class="math inline">\(V\)</span>) and <span class="math inline">\(\emptyset \in S\)</span>. This
allows us to consider sub-optimal rules that are easier to estimate and
potentially provide more realistic rules- as such, we allow for statistical
inference for the counterfactual mean outcome under the sub-optimal rule.
Within the <code>tmle3mopttx</code> paradigm, we just need to change the <code>complex</code>
parameter to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb99-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(</a>
<a class="sourceLine" id="cb99-3" data-line-number="3">  <span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W4&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W1&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>,</a>
<a class="sourceLine" id="cb99-4" data-line-number="4">  <span class="dt">learners =</span> learner_list,</a>
<a class="sourceLine" id="cb99-5" data-line-number="5">  <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">FALSE</span>, <span class="dt">realistic =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb99-6" data-line-number="6">)</a></code></pre></div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb100-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</a>
<a class="sourceLine" id="cb100-3" data-line-number="3"></a>
<a class="sourceLine" id="cb100-4" data-line-number="4">fit</a></code></pre></div>
<pre><code>A tmle3_Fit that took 1 step(s)
   type       param  init_est  tmle_est         se     lower     upper
1:  TSM E[Y_{A=W1}] 0.5502096 0.6057064 0.06227979 0.4836402 0.7277725
   psi_transformed lower_transformed upper_transformed
1:       0.6057064         0.4836402         0.7277725</code></pre>
<p>Therefore even though the user specified all baseline covariates as the basis
for rule estimation, a simpler rule based on only <span class="math inline">\(W_2\)</span> and <span class="math inline">\(W_1\)</span> is sufficient
to maximize the mean under the optimal individualized treatment.</p>
</div>
<div id="realistic-optimal-individual-regimes" class="section level3">
<h3><span class="header-section-number">6.8.2</span> Realistic Optimal Individual Regimes</h3>
<p>In addition to considering less complex rules, <code>tmle3mopttx</code> also provides an
option to estimate the mean under the realistic, or implementable, optimal
individualized treatment. It is often the case that assigning particular regime
might have the ability to fully maximize (or minimize) the desired outcome, but
due to global or practical positivity constrains, such treatment can never be
implemented in real life (or is highly unlikely). As such, specifying
<code>realistic</code> to <code>TRUE</code>, we consider possibly suboptimal treatments that optimize
the outcome in question while being supported by the data.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb102-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(</a>
<a class="sourceLine" id="cb102-3" data-line-number="3">  <span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;W4&quot;</span>, <span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W2&quot;</span>, <span class="st">&quot;W1&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>,</a>
<a class="sourceLine" id="cb102-4" data-line-number="4">  <span class="dt">learners =</span> learner_list,</a>
<a class="sourceLine" id="cb102-5" data-line-number="5">  <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>, <span class="dt">realistic =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb102-6" data-line-number="6">)</a></code></pre></div>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2">fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, data, node_list, learner_list)</a>
<a class="sourceLine" id="cb103-3" data-line-number="3">fit</a></code></pre></div>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param  init_est  tmle_est         se     lower     upper
1:  TSM E[Y_{A=NULL}] 0.5383577 0.6502342 0.02164617 0.6078085 0.6926599
   psi_transformed lower_transformed upper_transformed
1:       0.6502342         0.6078085         0.6926599</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1"><span class="co"># How many individuals got assigned each treatment?</span></a>
<a class="sourceLine" id="cb105-2" data-line-number="2"><span class="kw">table</span>(tmle_spec<span class="op">$</span>return_rule)</a></code></pre></div>
<pre><code>
  2   3 
506 494 </code></pre>
</div>
<div id="q-learning" class="section level3">
<h3><span class="header-section-number">6.8.3</span> Q-learning</h3>
<p>Alternatively, we could estimate the mean under the optimal individualized
treatment using Q-learning. The optimal rule can be learned through fitting the
likelihood, and consequently estimating the optimal rule under this fit of the
likelihood <span class="citation">(Sutton, Barto, and others <a href="#ref-Sutton1998">1998</a>, <span class="citation">@murphy2003</span>)</span>.</p>
<p>Below we outline how to use <code>tmle3mopttx</code> package in order to estimate the mean
under the ITR using Q-learning. As demonstrated in the previous sections, we
first need to initialize a specification for the TMLE of our parameter of
interest. As opposed to the previous section however, we will now use
<code>tmle3_mopttx_Q</code> instead of <code>tmle3_mopttx_blip_revere</code> in order to indicate that
we want to use Q-learning instead of TMLE.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb107-2" data-line-number="2">tmle_spec_Q &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_Q</span>(<span class="dt">maximize =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb107-3" data-line-number="3"></a>
<a class="sourceLine" id="cb107-4" data-line-number="4"><span class="co"># Define data:</span></a>
<a class="sourceLine" id="cb107-5" data-line-number="5">tmle_task &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_tmle_task</span>(data, node_list)</a>
<a class="sourceLine" id="cb107-6" data-line-number="6"></a>
<a class="sourceLine" id="cb107-7" data-line-number="7"><span class="co"># Define likelihood:</span></a>
<a class="sourceLine" id="cb107-8" data-line-number="8">initial_likelihood &lt;-<span class="st"> </span>tmle_spec_Q<span class="op">$</span><span class="kw">make_initial_likelihood</span>(tmle_task,</a>
<a class="sourceLine" id="cb107-9" data-line-number="9">                                                          learner_list)</a>
<a class="sourceLine" id="cb107-10" data-line-number="10"></a>
<a class="sourceLine" id="cb107-11" data-line-number="11"><span class="co"># Estimate the parameter:</span></a>
<a class="sourceLine" id="cb107-12" data-line-number="12"><span class="kw">Q_learning</span>(tmle_spec_Q, initial_likelihood, tmle_task)[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>[1] 0.4777343</code></pre>
</div>
</div>
<div id="variable-importance-analysis-with-oit" class="section level2">
<h2><span class="header-section-number">6.9</span> Variable Importance Analysis with OIT</h2>
<p>Suppose one wishes to assess the importance of each observed covariate, in
terms of maximizing (or minimizing) the population mean of an outcome under an
optimal individualized treatment regime. In particular, a covariate that
maximizes (or minimizes) the population mean outcome the most under an optimal
individualized treatment out of all other considered covariates under optimal
assignment might be considered <em>more important</em> for the outcome. To put it in
context, perhaps optimal allocation of treatment 1, denoted <span class="math inline">\(A_1\)</span>, results in a
larger mean outcome than optimal allocation of another treatment (<span class="math inline">\(A_2\)</span>).
Therefore, we would label <span class="math inline">\(A_1\)</span> as having a higher variable importance with
regard to maximizing (minimizing) the mean outcome under the optimal
individualized treatment.</p>
<div id="simulated-data-2" class="section level3">
<h3><span class="header-section-number">6.9.1</span> Simulated Data</h3>
<p>In order to run <code>tmle3mopttx</code> variable importance measure, we need to consider
covariates to be categorical variables. For illustration purpose, we bin
baseline covariates corresponding to the data-generating distribution described
in section 5.7.1:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1"><span class="co"># bin baseline covariates to 3 categories:</span></a>
<a class="sourceLine" id="cb109-2" data-line-number="2">data<span class="op">$</span>W1 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(data<span class="op">$</span>W1 <span class="op">&lt;</span><span class="st"> </span><span class="kw">quantile</span>(data<span class="op">$</span>W1)[<span class="dv">2</span>], <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb109-3" data-line-number="3">                  <span class="kw">ifelse</span>(data<span class="op">$</span>W1 <span class="op">&lt;</span><span class="st"> </span><span class="kw">quantile</span>(data<span class="op">$</span>W1)[<span class="dv">3</span>], <span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb109-4" data-line-number="4"></a>
<a class="sourceLine" id="cb109-5" data-line-number="5">node_list &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb109-6" data-line-number="6">  <span class="dt">W =</span> <span class="kw">c</span>(<span class="st">&quot;W3&quot;</span>, <span class="st">&quot;W4&quot;</span>, <span class="st">&quot;W2&quot;</span>),</a>
<a class="sourceLine" id="cb109-7" data-line-number="7">  <span class="dt">A =</span> <span class="kw">c</span>(<span class="st">&quot;W1&quot;</span>, <span class="st">&quot;A&quot;</span>),</a>
<a class="sourceLine" id="cb109-8" data-line-number="8">  <span class="dt">Y =</span> <span class="st">&quot;Y&quot;</span></a>
<a class="sourceLine" id="cb109-9" data-line-number="9">)</a></code></pre></div>
<p>Note that our node list now includes <span class="math inline">\(W_1\)</span> as treatments as well! Don’t worry,
we will still properly adjust for all baseline covariates.</p>
</div>
<div id="variable-importance-using-targeted-estimation-of-the-value-of-the-itr" class="section level3">
<h3><span class="header-section-number">6.9.2</span> Variable Importance using Targeted Estimation of the value of the ITR</h3>
<p>In the previous sections we have seen how to obtain a contrast between the mean
under the optimal individualized rule and the mean under the observed outcome
for a single covariate- we are now ready to run the variable importance analysis
for all of our specified covariates. In order to run the variable importance
analysis, we first need to initialize a specification for the TMLE of our
parameter of interest as we have done before. In addition, we need to specify
the data and the corresponding list of nodes, as well as the appropriate
learners for the outcome regression, propensity score, and the blip function.
Finally, we need to specify whether we should adjust for all the other
covariates we are assessing variable importance for. We will adjust for all <span class="math inline">\(W\)</span>s
in our analysis, and if <code>adjust_for_other_A=TRUE</code>, also for all <span class="math inline">\(A\)</span> covariates
that are not treated as exposure in the variable importance loop.</p>
<p>To start, we will initialize a specification for the TMLE of our parameter of
interest (called a <code>tmle3_Spec</code> in the <code>tlverse</code> nomenclature) simply by calling
<code>tmle3_mopttx_vim</code>. First, we indicate the method used for learning the optimal
individualized treatment by specifying the <code>method</code> argument of
<code>tmle3_mopttx_vim</code>. If <code>method=&quot;Q&quot;</code>, then we will be using Q-learning for rule
estimation, and we do not need to specify <code>V</code>, <code>type</code> and <code>learners</code> arguments
in the spec, since they are not important for Q-learning. However, if
<code>method=&quot;SL&quot;</code>, which corresponds to learning the optimal individualized
treatment using the above outlined methodology, then we need to specify the type
of pseudo-blip we will use in this estimation problem, whether we want to
maximize or minimize the outcome, complex and realistic rules. Finally, for
<code>method=&quot;SL&quot;</code> we also need to communicate that we’re interested in learning a
rule dependent on <code>V</code> covariates by specifying the <code>V</code> argument. For both
<code>method=&quot;Q&quot;</code> and <code>method=&quot;SL&quot;</code>, we need to indicate whether we want to maximize
or minimize the mean under the optimal individualized rule. Finally, we also
need to specify whether the final comparison of the mean under the optimal
individualized rule and the mean under the observed outcome should be on the
multiplicative scale (risk ratio) or linear (similar to average treatment
effect).</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb110-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_vim</span>(</a>
<a class="sourceLine" id="cb110-3" data-line-number="3">  <span class="dt">V=</span><span class="kw">c</span>(<span class="st">&quot;W2&quot;</span>),</a>
<a class="sourceLine" id="cb110-4" data-line-number="4">  <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>,</a>
<a class="sourceLine" id="cb110-5" data-line-number="5">  <span class="dt">learners =</span> learner_list,</a>
<a class="sourceLine" id="cb110-6" data-line-number="6">  <span class="dt">contrast =</span> <span class="st">&quot;multiplicative&quot;</span>,</a>
<a class="sourceLine" id="cb110-7" data-line-number="7">  <span class="dt">maximize =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb110-8" data-line-number="8">  <span class="dt">method =</span> <span class="st">&quot;SL&quot;</span>,</a>
<a class="sourceLine" id="cb110-9" data-line-number="9">  <span class="dt">complex =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb110-10" data-line-number="10">  <span class="dt">realistic =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb110-11" data-line-number="11">)</a></code></pre></div>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb111-2" data-line-number="2">vim_results &lt;-<span class="st"> </span><span class="kw">tmle3_vim</span>(tmle_spec, data, node_list, learner_list,</a>
<a class="sourceLine" id="cb111-3" data-line-number="3">  <span class="dt">adjust_for_other_A =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb111-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb111-5" data-line-number="5"></a>
<a class="sourceLine" id="cb111-6" data-line-number="6"><span class="kw">print</span>(vim_results)</a></code></pre></div>
<pre><code>   type                  param     init_est    tmle_est         se       lower
1:   RR RR(E[Y_{A=NULL}]/E[Y])  0.003367191  0.08225021 0.03298895  0.01759305
2:   RR RR(E[Y_{A=NULL}]/E[Y]) -0.021181883 -0.06817364 0.05052442 -0.16719969
        upper psi_transformed lower_transformed upper_transformed  A
1: 0.14690737       1.0857274         1.0177487          1.158247  A
2: 0.03085241       0.9340983         0.8460306          1.031333 W1
             W    Z_stat       p_nz p_nz_corrected
1: W3,W4,W2,W1  2.493265 0.00632871     0.01265742
2:  W3,W4,W2,A -1.349321 0.08861701     0.08861701</code></pre>
<p>The final result of <code>tmle3_vim</code> with the <code>tmle3mopttx</code> spec is an ordered list
of mean outcomes under the optimal individualized treatment for all categorical
covariates in our dataset.</p>
</div>
</div>
<div id="real-world-data-and-tmle3mopttx" class="section level2">
<h2><span class="header-section-number">6.10</span> Real World Data and <code>tmle3mopttx</code></h2>
<p>Finally, we cement everything we learned so far with a real data application.
As in the previous sections, we will be using the WASH Benefits data,
corresponding to the Effect of water quality, sanitation, hand washing, and
nutritional interventions on child development in rural Bangladesh trial.
The main aim of the cluster-randomized controlled trial was to assess the
impact of six intervention groups, including:</p>
<ol style="list-style-type: decimal">
<li><p>chlorinated drinking water</p></li>
<li><p>improved sanitation</p></li>
<li><p>handwashing with soap</p></li>
<li><p>combined water, sanitation, and handwashing</p></li>
<li><p>improved nutrition through counselling and provision of lipid-based nutrient
supplements</p></li>
<li><p>combined water, sanitation, handwashing, and nutrition.</p></li>
</ol>
<p>We aim to estimate the optimal ITR and the corresponding value under the optimal
ITR for the main intervention in WASH Benefits data.</p>
<p>To start, let’s load the data, convert all columns to be of class <code>numeric</code>,
and take a quick look at it:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1">washb_data &lt;-<span class="st"> </span><span class="kw">fread</span>(<span class="st">&quot;https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv&quot;</span>,</a>
<a class="sourceLine" id="cb113-2" data-line-number="2">                    <span class="dt">stringsAsFactors =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb113-3" data-line-number="3">washb_data &lt;-<span class="st"> </span>washb_data[<span class="op">!</span><span class="kw">is.na</span>(momage), <span class="kw">lapply</span>(.SD, as.numeric)]</a>
<a class="sourceLine" id="cb113-4" data-line-number="4"><span class="kw">head</span>(washb_data, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>     whz tr fracode month aged sex momage momedu momheight hfiacat Nlt18 Ncomp
1:  0.00  1       4     9  268   2     30      2    146.40       1     3    11
2: -1.16  1       4     9  286   2     25      2    148.75       3     2     4
3: -1.05  1      20     9  264   2     25      2    152.15       1     1    10
   watmin elec floor walls roof asset_wardrobe asset_table asset_chair
1:      0    1     0     1    1              0           1           1
2:      0    1     0     1    1              0           1           0
3:      0    0     0     1    1              0           0           1
   asset_khat asset_chouki asset_tv asset_refrig asset_bike asset_moto
1:          1            0        1            0          0          0
2:          1            1        0            0          0          0
3:          0            1        0            0          0          0
   asset_sewmach asset_mobile
1:             0            1
2:             0            1
3:             0            1</code></pre>
<p>As before, we specify the NPSEM via the <code>node_list</code> object. Our outcome of
interest is the weight-for-height Z-score which we seek to maximize, whereas our
treatment is the six intervention groups aimed at improving living conditions.
All the other collected baseline covariates correspond to <span class="math inline">\(W\)</span>.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1">node_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">W =</span> <span class="kw">names</span>(washb_data)[<span class="op">!</span>(<span class="kw">names</span>(washb_data) <span class="op">%in%</span></a>
<a class="sourceLine" id="cb115-2" data-line-number="2"><span class="st">                                          </span><span class="kw">c</span>(<span class="st">&quot;whz&quot;</span>, <span class="st">&quot;tr&quot;</span>))],</a>
<a class="sourceLine" id="cb115-3" data-line-number="3">                  <span class="dt">A =</span> <span class="st">&quot;tr&quot;</span>, <span class="dt">Y =</span> <span class="st">&quot;whz&quot;</span>)</a></code></pre></div>
<p>We pick few potential effect modifiers, including mother’s education, current
living conditions (floor), and possession of material items including the
refrigerator. We concentrate of these covariates as they might be indicative of
the socio-economic status of individuals involved in the trial.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="kw">table</span>(washb_data<span class="op">$</span>momedu)</a></code></pre></div>
<pre><code>
   1    2    3 
 733 1441 2503 </code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">table</span>(washb_data<span class="op">$</span>floor)</a></code></pre></div>
<pre><code>
   0    1 
4177  500 </code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">table</span>(washb_data<span class="op">$</span>asset_refrig)</a></code></pre></div>
<pre><code>
   0    1 
4305  372 </code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw">summary</span>(washb_data<span class="op">$</span>whz)</a></code></pre></div>
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-4.6700 -1.2800 -0.6000 -0.5859  0.0800  4.9700 </code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="co"># Initialize few of the learners:</span></a>
<a class="sourceLine" id="cb124-2" data-line-number="2">lrn_xgboost_<span class="dv">50</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb124-3" data-line-number="3">lrn_xgboost_<span class="dv">100</span> &lt;-<span class="st"> </span>Lrnr_xgboost<span class="op">$</span><span class="kw">new</span>(<span class="dt">nrounds =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb124-4" data-line-number="4">lrn_mean &lt;-<span class="st"> </span>Lrnr_mean<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb124-5" data-line-number="5"></a>
<a class="sourceLine" id="cb124-6" data-line-number="6">## Define the Q learner, which is just a regular learner:</a>
<a class="sourceLine" id="cb124-7" data-line-number="7">Q_learner &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(</a>
<a class="sourceLine" id="cb124-8" data-line-number="8">  <span class="dt">learners =</span> <span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>, lrn_mean),</a>
<a class="sourceLine" id="cb124-9" data-line-number="9">  <span class="dt">metalearner =</span> Lrnr_nnls<span class="op">$</span><span class="kw">new</span>()</a>
<a class="sourceLine" id="cb124-10" data-line-number="10">)</a>
<a class="sourceLine" id="cb124-11" data-line-number="11"></a>
<a class="sourceLine" id="cb124-12" data-line-number="12"><span class="co"># Define the g learner, which is a multinomial learner:</span></a>
<a class="sourceLine" id="cb124-13" data-line-number="13"><span class="co">#specify the appropriate loss of the multinomial learner:</span></a>
<a class="sourceLine" id="cb124-14" data-line-number="14">mn_metalearner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_solnp,</a>
<a class="sourceLine" id="cb124-15" data-line-number="15">                               <span class="dt">loss_function =</span> loss_loglik_multinomial,</a>
<a class="sourceLine" id="cb124-16" data-line-number="16">                               <span class="dt">learner_function =</span></a>
<a class="sourceLine" id="cb124-17" data-line-number="17">                                 metalearner_linear_multinomial)</a>
<a class="sourceLine" id="cb124-18" data-line-number="18">g_learner &lt;-<span class="st"> </span><span class="kw">make_learner</span>(Lrnr_sl,</a>
<a class="sourceLine" id="cb124-19" data-line-number="19">                          <span class="kw">list</span>(lrn_xgboost_<span class="dv">100</span>, lrn_mean),</a>
<a class="sourceLine" id="cb124-20" data-line-number="20">                          mn_metalearner)</a>
<a class="sourceLine" id="cb124-21" data-line-number="21"></a>
<a class="sourceLine" id="cb124-22" data-line-number="22"><span class="co"># Define the Blip learner, which is a multivariate learner:</span></a>
<a class="sourceLine" id="cb124-23" data-line-number="23">learners &lt;-<span class="st"> </span><span class="kw">list</span>(lrn_xgboost_<span class="dv">50</span>, lrn_xgboost_<span class="dv">100</span>, lrn_mean)</a>
<a class="sourceLine" id="cb124-24" data-line-number="24">b_learner &lt;-<span class="st"> </span><span class="kw">create_mv_learners</span>(<span class="dt">learners =</span> learners)</a>
<a class="sourceLine" id="cb124-25" data-line-number="25"></a>
<a class="sourceLine" id="cb124-26" data-line-number="26">learner_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Y =</span> Q_learner, <span class="dt">A =</span> g_learner, <span class="dt">B =</span> b_learner)</a></code></pre></div>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="co"># initialize a tmle specification</span></a>
<a class="sourceLine" id="cb125-2" data-line-number="2">tmle_spec &lt;-<span class="st"> </span><span class="kw">tmle3_mopttx_blip_revere</span>(</a>
<a class="sourceLine" id="cb125-3" data-line-number="3">  <span class="dt">V =</span> <span class="kw">c</span>(<span class="st">&quot;momedu&quot;</span>, <span class="st">&quot;floor&quot;</span>, <span class="st">&quot;asset_refrig&quot;</span>), <span class="dt">type =</span> <span class="st">&quot;blip2&quot;</span>,</a>
<a class="sourceLine" id="cb125-4" data-line-number="4">  <span class="dt">learners =</span> learner_list, <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">complex =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb125-5" data-line-number="5">  <span class="dt">realistic =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb125-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb125-7" data-line-number="7"></a>
<a class="sourceLine" id="cb125-8" data-line-number="8"><span class="co"># fit the TML estimator</span></a>
<a class="sourceLine" id="cb125-9" data-line-number="9">fit &lt;-<span class="st"> </span><span class="kw">tmle3</span>(tmle_spec, <span class="dt">data=</span>washb_data, node_list, learner_list)</a></code></pre></div>
<pre><code>Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.

Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1">fit</a></code></pre></div>
<pre><code>A tmle3_Fit that took 1 step(s)
   type         param   init_est   tmle_est         se      lower     upper
1:  TSM E[Y_{A=NULL}] -0.5546495 -0.4749502 0.04445805 -0.5620864 -0.387814
   psi_transformed lower_transformed upper_transformed
1:      -0.4749502        -0.5620864         -0.387814</code></pre>
<hr />
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">6.11</span> Exercises</h2>
<div id="review-of-key-concepts" class="section level3">
<h3><span class="header-section-number">6.11.1</span> Review of Key Concepts</h3>
<ol style="list-style-type: decimal">
<li><p>What is the difference between dynamic and optimal individualized regimes?</p></li>
<li><p>What’s the intuition behind using different blip types? Why did we switch
from <code>blip1</code> to <code>blip2</code> when considering categorical treatment? What are some
of the advantages of each?</p></li>
<li><p>Look back at the results generated in section 5.7.1, and compare then to the
mean under the optimal individualized treatment in section 5.6. Why do you
think the estimate if higher under the less complex rule? How does the set of
covariates picked by <code>tmle3mopttx</code> compare to the baseline covariates the
true rule depends on?</p></li>
<li><p>Compare the distribution of treatments assigned under the true optimal
individualized treatment (section 5.6) and realistic optimal individualized
treatment (section 5.7.2). Referring back to the data-generating
distribution, why do you think the distribution of allocated treatment
changed?</p></li>
<li><p>Using the same simulation, perform a variable importance analysis using
Q-learning. How do the results change and why?</p></li>
</ol>
</div>
<div id="the-ideas-in-action" class="section level3">
<h3><span class="header-section-number">6.11.2</span> The Ideas in Action</h3>
<ol style="list-style-type: decimal">
<li><p>Using the WASH benefits data, extract the optimal ITR for exact individual.
Which intervention is the most dominant? Why do you think that is?</p></li>
<li><p>Consider simpler rules for the WASH benefits data example. What set of rules
are picked?</p></li>
<li><p>Using the WASH benefits data, estimate the realistic optimal ITR and the
corresponding value of the realistic ITR. Did the results change?</p></li>
<li><p>Change the treatment to Mother’s education (momedu), and estimate the
value under the ITR in this setting. What do the results indicate? Can
we intervene on such a variable?</p></li>
</ol>
</div>
<div id="advanced-topics" class="section level3">
<h3><span class="header-section-number">6.11.3</span> Advanced Topics</h3>
<ol style="list-style-type: decimal">
<li><p>How can we extend the current approach to include exceptional laws?</p></li>
<li><p>How can we extend the current approach to incorporate resource constraints?</p></li>
<li><p>How can we extend the current approach to continuous interventions?</p></li>
</ol>
<!--
## Appendix

### Exercise solutions
-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-moodie2013">
<p>Chakraborty, Bibhas, and Erica EM Moodie. 2013. <em>Statistical Methods for Dynamic Treatment Regimes: Reinforcement Learning, Causal Inference, and Personalized Medicine (Statistics for Biology and Health)</em>. Springer.</p>
</div>
<div id="ref-luedtke2016super">
<p>Luedtke, A., and M. J van der Laan. 2016. “Super-Learning of an Optimal Dynamic Treatment Rule.” <em>International Journal of Biostatistics</em> 12 (1): 305–32.</p>
</div>
<div id="ref-murphy2003">
<p>Murphy, Susan A. 2003. “Optimal Dynamic Treatment Regimes.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 65 (2). Wiley Online Library: 331–55.</p>
</div>
<div id="ref-neyman1990">
<p>“On the Application of Probability Theory to Agricultural Experiments. Essay on Principles. Section 9.” 1990. <em>Statistical Science</em> 5 (4). Institute of Mathematical Statistics: 465–72. <a href="http://www.jstor.org/stable/2245382" class="uri">http://www.jstor.org/stable/2245382</a>.</p>
</div>
<div id="ref-pearl2009causality">
<p>Pearl, Judea. 2009a. <em>Causality: Models, Reasoning, and Inference</em>. Cambridge University Press.</p>
</div>
<div id="ref-pearl2009">
<p>Pearl, Judea. 2009b. <em>Causality: Models, Reasoning and Inference</em>. 2nd ed. New York, NY, USA: Cambridge University Press.</p>
</div>
<div id="ref-robins1986">
<p>Robins, James. 1986. “A New Approach to Causal Inference in Mortality Studies with a Sustained Exposure Period—Application to Control of the Healthy Worker Survivor Effect.” <em>Mathematical Modelling</em> 7 (9): 1393–1512. <a href="https://doi.org/https://doi.org/10.1016/0270-0255(86)90088-6" class="uri">https://doi.org/https://doi.org/10.1016/0270-0255(86)90088-6</a>.</p>
</div>
<div id="ref-robins2004">
<p>Robins, James M. 2004. “Optimal Structural Nested Models for Optimal Sequential Decisions.” In <em>Proceedings of the Second Seattle Symposium in Biostatistics: Analysis of Correlated Data</em>, edited by D. Y. Lin and P. J. Heagerty, 189–326. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4419-9076-1_11" class="uri">https://doi.org/10.1007/978-1-4419-9076-1_11</a>.</p>
</div>
<div id="ref-robins2014">
<p>Robins, James, and Andrea Rotnitzky. 2014. “Discussion of ‘Dynamic Treatment Regimes: Technical Challenges and Applications’.” <em>Electron. J. Statist.</em> 8 (1). The Institute of Mathematical Statistics; the Bernoulli Society: 1273–89. <a href="https://doi.org/10.1214/14-EJS908" class="uri">https://doi.org/10.1214/14-EJS908</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, Richard S, Andrew G Barto, and others. 1998. <em>Introduction to Reinforcement Learning</em>. Vol. 135. MIT press Cambridge.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
<div id="ref-vanderLaanLuedtke15">
<p>van der Laan, M. J, and A. Luedtke. 2015. “Targeted Learning of the Mean Outcome Under an Optimal Dynamic Treatment Rule.” <em>Journal of Causal Inference</em> 3 (1): 61–95.</p>
</div>
<div id="ref-laber2012">
<p>Zhang, Baqun, Anastasios A Tsiatis, Marie Davidian, Min Zhang, and Eric Laber. 2016. “Estimating Optimal Treatment Regimes from a Classification Perspective.” <em>Stat</em> 5 (1): 278–78. <a href="https://doi.org/10.1002/sta4.124" class="uri">https://doi.org/10.1002/sta4.124</a>.</p>
</div>
<div id="ref-kosorok2012">
<p>Zhao, Yingqi, Donglin Zeng, A John Rush, and Michael R Kosorok. 2012. “Estimating Individualized Treatment Rules Using Outcome Weighted Learning.” <em>Journal of the American Statistical Association</em> 107 (499). Taylor &amp; Francis: 1106–18. <a href="https://doi.org/10.1080/01621459.2012.695674" class="uri">https://doi.org/10.1080/01621459.2012.695674</a>.</p>
</div>
<div id="ref-cvtmle2010">
<p>Zheng, W., and M. J van der Laan. 2010. “Asymptotic Theory for Cross-validated Targeted Maximum Likelihood Estimation.” <em>U.C. Berkeley Division of Biostatistics Working Paper Series.</em></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tmle3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stochastic-treatment-regimes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tlverse/tlverse-handbook/edit/master/07-tmle3mopttx.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
