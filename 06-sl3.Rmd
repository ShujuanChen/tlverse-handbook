# Super (Machine) Learning {#sl3}

## Introduction
### Roadmap Review
### We still want to fit the data to estimate Q
### sl3 makes that process easier


## Learning Objectives{-}

By the end of this chapter you will be able to:

1. Select an objective function that (i) aligns with the intention of the 
   analysis and (ii) is optimized by the target parameter.
2. Assemble a diverse library of learners to be considered in the Super Learner 
   ensemble. In particular, you should be able to:
   
     a. Customize a learner by modifying it's tuning parameters.
     b. Create several different versions of the same learner at once by 
        specifying a grid of tuning parameters.
     c. Curate covariate screening pipelines in order to pass a screener's 
        output, a subset of covariates, as input for another learner that will 
        use the subset of covariates selected by the screener to model the data.
        
3. Specify the learner for ensembling (the metalearner) such that it corresponds 
   to your objective function. 
4. Fit the Super Learner ensemble with nested cross-validation to obtain an 
   estimate of the performance of the ensemble itself on out-of-sample data.
5. Obtain `sl3` variable importance metrics.
6. Interpret the fit for discrete and continuous Super Learners' from the 
   cross-validated risk table and the coefficients.
7. Justify the base library of machine learning algorithms and the ensembling 
   learner in terms of the prediction problem, statistical model $\M$, data 
   sparsity, and the dimensionality of the covariates. 
   
   

## Setup

```{r sl3_setup}
library(sl3)
library(ggplot2)
```

## Schematic Example

We can define a `sl3` task as follows:
```{r sl3_schematic_task}
data(schematic, package="tlverse")
task <- make_sl3_Task(schematic, 
                      covariates=c("A","W"),
                      outcome="Y")
```

This is a way of organizing data and metadata together. In essence, it's telling 
the `tlverse` about the *Data* step of the roadmap.

Next, we can make some guesses about the *Model* step of the roadmap. This is 
similar to what we did in the [#origami] chapter:

```{r sl3_schematic_learners}
gl <- make_learner(Lrnr_glm, 
                   formula = "~ A + W")
gl_interaction <- make_learner(Lrnr_glm, 
                               formula = "~ A*W")
gl_poly2 <- make_learner(Lrnr_glm, 
                         formula = "~ A*(W + I(W^2))")
gl_poly4 <- make_learner(Lrnr_glm, 
                         formula = "~ A*(W + I(W^2) + I(W^3) + I (W^4))")
```

We can train one of these on our task and generate predictions:
```{r sl3_schematic_train_pred}
gl_fit <- gl$train(task)
preds <- gl_fit$predict(task)
head(preds)
```
We can also ``ensemble'' these together using a special learner called `Lrnr_sl`. 
We'll explain more in a minute what this means.

```{r sl3_schematic_sl}
learners <- list(linear = gl, 
                 interaction = gl_interaction, 
                 poly2 = gl_poly2,
                 poly4 = gl_poly4)
sl <- make_learner(Lrnr_sl, learners)
sl_fit <- sl$train(task)
```

`Lrnr_sl` applies cross-validation to all the learners, so we can get 
CV risk estimates, similar to those we generated by hand with origami.

```{r sl3_schematic_cv_risk}
sl_fit$cv_risk(loss_squared_error)
```

<!-- TODO make plot, would be helpful if we had a convience function here -->

## washb example
## advanced usage
